============================= DIRECTORY OVERVIEW =============================
Root: C:\Projects\sense-pi-local-recording-live-main
Timestamp: 02/12/2025 23:35:58,80

----------------------------- TREE (with files) -----------------------------
Folder PATH listing
Volume serial number is 46B7-CF63
C:\PROJECTS\SENSE-PI-LOCAL-RECORDING-LIVE-MAIN
¦   .gitignore
¦   combine.bat
¦   combined.py
¦   debug_log_channels.py
¦   debug_pc_ingest_worker.py
¦   debug_pc_recorder_stream.py
¦   debug_pi_via_ssh.py
¦   decimation.py
¦   envelope_plot.py
¦   live_plot.py
¦   main.py
¦   pi_recorder.py
¦   profile_benchmark.py
¦   pyproject.toml
¦   README.md
¦   requirements-pi.txt
¦   requirements.txt
¦   run_live_plot.py
¦   ssh_client.py
¦   
+---codex-prompts
¦       desktop.ini
¦       
+---data
¦   ¦   .gitkeep
¦   ¦   
¦   +---processed
¦   ¦       .gitkeep
¦   ¦       
¦   +---raw
¦           .gitkeep
¦           
+---docs
¦       AI_AGENT_NOTES.md
¦       json_protocol.md
¦       LEARNING_PATH.md
¦       
+---logs
¦       .gitkeep
¦       
+---raspberrypi_scripts
¦       debug_log_sample_rate.py
¦       install_pi_deps.sh
¦       mpu6050_multi_logger.py
¦       pi_config.yaml
¦       pi_logger_common.py
¦       README_rpi.md
¦       run_all_sensors.sh
¦       
+---src
¦   +---sensepi
¦       ¦   perf_system.py
¦       ¦   __init__.py
¦       ¦   
¦       +---analysis
¦       ¦       features.py
¦       ¦       fft.py
¦       ¦       filters.py
¦       ¦       rate.py
¦       ¦       __init__.py
¦       ¦       
¦       +---config
¦       ¦       app_config.py
¦       ¦       constants.py
¦       ¦       hosts.yaml
¦       ¦       log_paths.py
¦       ¦       pi_logger_config.py
¦       ¦       runtime.py
¦       ¦       sampling.py
¦       ¦       sensors.yaml
¦       ¦       __init__.py
¦       ¦       
¦       +---core
¦       ¦       live_stream.py
¦       ¦       models.py
¦       ¦       pipeline.py
¦       ¦       pipeline_wiring.py
¦       ¦       recorder_session.py
¦       ¦       ringbuffer.py
¦       ¦       stream_reader.py
¦       ¦       timeseries_buffer.py
¦       ¦       __init__.py
¦       ¦       
¦       +---data
¦       ¦       stream_buffer.py
¦       ¦       __init__.py
¦       ¦       
¦       +---dataio
¦       ¦       csv_writer.py
¦       ¦       file_paths.py
¦       ¦       log_loader.py
¦       ¦       __init__.py
¦       ¦       
¦       +---gui
¦       ¦   ¦   application.py
¦       ¦   ¦   benchmark.py
¦       ¦   ¦   main_window.py
¦       ¦   ¦   perf_metrics.py
¦       ¦   ¦   pg_signal_plot_widget.py
¦       ¦   ¦   __init__.py
¦       ¦   ¦   
¦       ¦   +---tabs
¦       ¦   ¦       tab_fft.py
¦       ¦   ¦       tab_logs.py
¦       ¦   ¦       tab_offline.py
¦       ¦   ¦       tab_recorder.py
¦       ¦   ¦       tab_settings.py
¦       ¦   ¦       tab_signals.py
¦       ¦   ¦       __init__.py
¦       ¦   ¦       
¦       ¦   +---widgets
¦       ¦           acquisition_settings.py
¦       ¦           collapsible.py
¦       ¦           __init__.py
¦       ¦           
¦       +---remote
¦       ¦       pi_recorder.py
¦       ¦       sensor_ingest_worker.py
¦       ¦       ssh_client.py
¦       ¦       __init__.py
¦       ¦       
¦       +---sensors
¦       ¦       mpu6050.py
¦       ¦       __init__.py
¦       ¦       
¦       +---tools
¦               debug.py
¦               local_plot_runner.py
¦               plotter.py
¦               __init__.py
¦               
+---tests
        conftest.py
        test_rate_controller.py
        test_stream_reader.py
        

------------------------- DETAILED DIRECTORY LISTING -------------------------
 Volume in drive C has no label.
 Volume Serial Number is 46B7-CF63

 Directory of C:\Projects\sense-pi-local-recording-live-main

02/12/2025  23:35    <DIR>          .
02/12/2025  22:35    <DIR>          ..
30/11/2025  16:51               417 .gitignore
02/12/2025  22:35    <DIR>          codex-prompts
30/11/2025  16:51             2.329 combine.bat
02/12/2025  23:35             3.791 combined.py
30/11/2025  16:51    <DIR>          data
30/11/2025  16:51             5.419 debug_log_channels.py
30/11/2025  16:51            11.822 debug_pc_ingest_worker.py
30/11/2025  16:51            10.294 debug_pc_recorder_stream.py
30/11/2025  16:51             4.522 debug_pi_via_ssh.py
30/11/2025  16:51             6.996 decimation.py
30/11/2025  16:51    <DIR>          docs
30/11/2025  16:51             4.652 envelope_plot.py
30/11/2025  16:51            10.030 live_plot.py
30/11/2025  16:51    <DIR>          logs
30/11/2025  16:51               901 main.py
30/11/2025  16:51               297 pi_recorder.py
30/11/2025  16:51             3.882 profile_benchmark.py
30/11/2025  16:51               717 pyproject.toml
30/11/2025  16:51    <DIR>          raspberrypi_scripts
30/11/2025  16:51            11.755 README.md
30/11/2025  16:51                39 requirements-pi.txt
30/11/2025  16:51               111 requirements.txt
30/11/2025  16:51             3.341 run_live_plot.py
30/11/2025  16:51    <DIR>          src
30/11/2025  16:51               309 ssh_client.py
30/11/2025  16:51    <DIR>          tests
              19 File(s)         81.624 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\codex-prompts

02/12/2025  22:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             1.424 desktop.ini
               1 File(s)          1.424 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
30/11/2025  16:51    <DIR>          processed
30/11/2025  16:51    <DIR>          raw
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data\processed

30/11/2025  16:51    <DIR>          .
30/11/2025  16:51    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data\raw

30/11/2025  16:51    <DIR>          .
30/11/2025  16:51    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\docs

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             4.972 AI_AGENT_NOTES.md
30/11/2025  16:51               934 json_protocol.md
30/11/2025  16:51             7.831 LEARNING_PATH.md
               3 File(s)         13.737 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\logs

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\raspberrypi_scripts

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             8.330 debug_log_sample_rate.py
30/11/2025  16:51               360 install_pi_deps.sh
30/11/2025  16:51            41.160 mpu6050_multi_logger.py
30/11/2025  16:51               393 pi_config.yaml
30/11/2025  16:51               367 pi_logger_common.py
30/11/2025  16:51               967 README_rpi.md
30/11/2025  16:51               207 run_all_sensors.sh
               7 File(s)         51.784 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
02/12/2025  23:35    <DIR>          sensepi
               0 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi

02/12/2025  23:35    <DIR>          .
30/11/2025  16:51    <DIR>          ..
02/12/2025  23:28    <DIR>          analysis
02/12/2025  23:35    <DIR>          config
02/12/2025  23:35    <DIR>          core
02/12/2025  23:28    <DIR>          data
02/12/2025  23:35    <DIR>          dataio
02/12/2025  23:28    <DIR>          gui
30/11/2025  16:51               577 perf_system.py
02/12/2025  23:35    <DIR>          remote
02/12/2025  23:35    <DIR>          sensors
02/12/2025  23:35    <DIR>          tools
30/11/2025  16:51                44 __init__.py
               2 File(s)            621 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\analysis

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               258 features.py
30/11/2025  16:51               376 fft.py
30/11/2025  16:51               491 filters.py
30/11/2025  16:51             2.068 rate.py
30/11/2025  16:51               362 __init__.py
               5 File(s)          3.555 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\config

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51            16.950 app_config.py
30/11/2025  16:51               161 constants.py
30/11/2025  16:51               415 hosts.yaml
30/11/2025  16:51             3.793 log_paths.py
30/11/2025  16:51             3.862 pi_logger_config.py
30/11/2025  16:51             3.623 runtime.py
30/11/2025  16:51             4.930 sampling.py
30/11/2025  16:51               258 sensors.yaml
30/11/2025  16:51               556 __init__.py
               9 File(s)         34.548 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\core

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               922 live_stream.py
30/11/2025  16:51               455 models.py
30/11/2025  16:51             9.002 pipeline.py
30/11/2025  16:51             3.704 pipeline_wiring.py
30/11/2025  16:51             1.596 recorder_session.py
30/11/2025  16:51             1.829 ringbuffer.py
30/11/2025  16:51             7.471 stream_reader.py
30/11/2025  16:51             3.411 timeseries_buffer.py
30/11/2025  16:51               931 __init__.py
               9 File(s)         29.321 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\data

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             8.474 stream_buffer.py
30/11/2025  16:51               835 __init__.py
               2 File(s)          9.309 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\dataio

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               453 csv_writer.py
30/11/2025  16:51               368 file_paths.py
30/11/2025  16:51               838 log_loader.py
30/11/2025  16:51               347 __init__.py
               4 File(s)          2.006 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             5.375 application.py
30/11/2025  16:51             5.744 benchmark.py
30/11/2025  16:51             7.104 main_window.py
30/11/2025  16:51             1.818 perf_metrics.py
30/11/2025  16:51               362 pg_signal_plot_widget.py
02/12/2025  23:28    <DIR>          tabs
02/12/2025  23:28    <DIR>          widgets
30/11/2025  16:51               351 __init__.py
               6 File(s)         20.754 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui\tabs

02/12/2025  23:28    <DIR>          .
02/12/2025  23:28    <DIR>          ..
30/11/2025  16:51            27.661 tab_fft.py
30/11/2025  16:51             4.637 tab_logs.py
30/11/2025  16:51            14.136 tab_offline.py
30/11/2025  16:51            29.948 tab_recorder.py
30/11/2025  16:51            23.099 tab_settings.py
30/11/2025  16:51           104.952 tab_signals.py
30/11/2025  16:51               192 __init__.py
               7 File(s)        204.625 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui\widgets

02/12/2025  23:28    <DIR>          .
02/12/2025  23:28    <DIR>          ..
30/11/2025  16:51             8.779 acquisition_settings.py
30/11/2025  16:51             1.608 collapsible.py
30/11/2025  16:51               239 __init__.py
               3 File(s)         10.626 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\remote

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             3.964 pi_recorder.py
30/11/2025  16:51             4.760 sensor_ingest_worker.py
30/11/2025  16:51             7.139 ssh_client.py
30/11/2025  16:51               404 __init__.py
               4 File(s)         16.267 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\sensors

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             4.077 mpu6050.py
30/11/2025  16:51               279 __init__.py
               2 File(s)          4.356 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\tools

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             1.084 debug.py
30/11/2025  16:51             2.192 local_plot_runner.py
30/11/2025  16:51            17.804 plotter.py
30/11/2025  16:51               303 __init__.py
               4 File(s)         21.383 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\tests

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               168 conftest.py
30/11/2025  16:51               325 test_rate_controller.py
30/11/2025  16:51             1.989 test_stream_reader.py
               3 File(s)          2.482 bytes

     Total Files Listed:
              94 File(s)        508.422 bytes
              65 Dir(s)  755.937.816.576 bytes free

============================================================================

============================= .gitignore
# File: .gitignore (ext: .gitignore
# Dir : 
# Size: 417 bytes
# Time: 30/11/2025 16:51
============================= .gitignore
# Virtualenv
.venv/
venv/
env/

# Python
__pycache__/
*.pyc
*.pyo
*.pyd

# Logs & data (keep folders, ignore contents)
logs/*
!logs/.gitkeep

data/*
!data/.gitkeep
data/raw/*
!data/raw/.gitkeep
data/processed/*
!data/processed/.gitkeep

# OS / editor junk
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp

# AI / scratch
.ai/

# Build / packaging
*.egg-info/
.dist/
.eggs/
build/
dist/

------------------------------ END OF FILE ------------------------------

============================= codex-prompts\desktop.ini
# File: desktop.ini (ext: .ini
# Dir : codex-prompts\
# Size: 1424 bytes
# Time: 30/11/2025 16:51
============================= codex-prompts\desktop.ini
[LocalizedFileNames]
1_01_offline_sync_and_open_latest.md=@1_01_offline_sync_and_open_latest.md,0
3_04-module-boundaries-doc-and-shims.md=@3_04-module-boundaries-doc-and-shims.md,0
3_03-learning-path-doc.md=@3_03-learning-path-doc.md,0
3_02-signals-tab-refactor.md=@3_02-signals-tab-refactor.md,0
3_01-gui-tab-reorg.md=@3_01-gui-tab-reorg.md,0
2_05_update_readme_log_conventions.md=@2_05_update_readme_log_conventions.md,0
2_04_offline_ui_sync_and_grouping.md=@2_04_offline_ui_sync_and_grouping.md,0
2_03_pc_offline_sync_paths.md=@2_03_pc_offline_sync_paths.md,0
2_02_pi_logger_use_helpers.md=@2_02_pi_logger_use_helpers.md,0
2_01_log_paths_and_naming_helpers.md=@2_01_log_paths_and_naming_helpers.md,0
4_05_gui_tabs_docstrings.md=@4_05_gui_tabs_docstrings.md,0
4_04_stream_buffer_and_ingest_docstrings.md=@4_04_stream_buffer_and_ingest_docstrings.md,0
4_03_buffers_and_stream_reader_docstrings.md=@4_03_buffers_and_stream_reader_docstrings.md,0
4_02_core_pipeline_docstrings.md=@4_02_core_pipeline_docstrings.md,0
4_01_pi_logger_docstrings.md=@4_01_pi_logger_docstrings.md,0
1_05_readme_download_logs_workflow.md=@1_05_readme_download_logs_workflow.md,0
1_04_offline_tab_copy_and_empty_state.md=@1_04_offline_tab_copy_and_empty_state.md,0
1_03_main_window_recording_stopped_offline_hint.md=@1_03_main_window_recording_stopped_offline_hint.md,0
1_02_signals_tab_mode_hint_offline.md=@1_02_signals_tab_mode_hint_offline.md,0

------------------------------ END OF FILE ------------------------------

============================= combine.bat
# File: combine.bat (ext: .bat
# Dir : 
# Size: 2329 bytes
# Time: 30/11/2025 16:51
============================= combine.bat
@echo off
setlocal EnableExtensions EnableDelayedExpansion

REM ====== Configuration ======
set "OUTPUT_FILE=combined.py"

REM Use the current directory as the root (run this from your main folder)
set "ROOT=%CD%"
set "OUT_FULL=%ROOT%\%OUTPUT_FILE%"

REM Delete output file if it exists
del "%OUTPUT_FILE%" 2>nul

REM ================== Directory Details (Tree + Detailed DIR) ==================
>>"%OUTPUT_FILE%" echo ============================= DIRECTORY OVERVIEW =============================
>>"%OUTPUT_FILE%" echo Root: %ROOT%
>>"%OUTPUT_FILE%" echo Timestamp: %DATE% %TIME%
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ----------------------------- TREE (with files) -----------------------------
tree "%ROOT%" /F >>"%OUTPUT_FILE%"
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ------------------------- DETAILED DIRECTORY LISTING -------------------------
dir "%ROOT%" /S /A >>"%OUTPUT_FILE%"
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ============================================================================
>>"%OUTPUT_FILE%" echo(

REM ================== Concatenate ALL files (any extension) ======================
for /f "delims=" %%f in ('
  dir /b /s /a:-d "%ROOT%\*" ^| sort
') do (
  REM Skip the output file itself
  if /i not "%%~f"=="%OUT_FULL%" (
    REM Build nice relative labels
    set "ABS=%%~f"
    set "REL=!ABS:%ROOT%\=!"          REM e.g. sub\pkg\file.ext
    set "DIRABS=%%~dpf"
    set "DIRREL=!DIRABS:%ROOT%\=!"     REM e.g. sub\pkg\
    set "SIZE=%%~zf"
    set "TIME=%%~tf"
    set "EXT=%%~xf"

    echo Adding !REL!...

    REM Safe header lines (use echo() so parentheses are harmless)
    >>"%OUTPUT_FILE%" echo(============================= !REL!
    >>"%OUTPUT_FILE%" echo(# File: %%~nxf (ext: !EXT!)
    >>"%OUTPUT_FILE%" echo(# Dir : !DIRREL!
    >>"%OUTPUT_FILE%" echo(# Size: !SIZE! bytes
    >>"%OUTPUT_FILE%" echo(# Time: !TIME!
    >>"%OUTPUT_FILE%" echo(============================= !REL!

    REM Append file contents (binary files will be dumped raw)
    type "%%~f">>"%OUTPUT_FILE%"

    REM Separator line after content
    >>"%OUTPUT_FILE%" echo(
    >>"%OUTPUT_FILE%" echo ------------------------------ END OF FILE ------------------------------
    >>"%OUTPUT_FILE%" echo(
  )
)

echo All files from "%ROOT%" and subfolders combined into "%OUTPUT_FILE%".
pause

------------------------------ END OF FILE ------------------------------

# Dir : 
# Size: 2329 bytes
# Time: 30/11/2025 16:51
============================= combine.bat
============================= DIRECTORY OVERVIEW =============================
Root: C:\Projects\sense-pi-local-recording-live-main
Timestamp: 02/12/2025 23:35:58,80

----------------------------- TREE (with files) -----------------------------
Folder PATH listing
Volume serial number is 46B7-CF63
C:\PROJECTS\SENSE-PI-LOCAL-RECORDING-LIVE-MAIN
¦   .gitignore
¦   combine.bat
¦   combined.py
¦   debug_log_channels.py
¦   debug_pc_ingest_worker.py
¦   debug_pc_recorder_stream.py
¦   debug_pi_via_ssh.py
¦   decimation.py
¦   envelope_plot.py
¦   live_plot.py
¦   main.py
¦   pi_recorder.py
¦   profile_benchmark.py
¦   pyproject.toml
¦   README.md
¦   requirements-pi.txt
¦   requirements.txt
¦   run_live_plot.py
¦   ssh_client.py
¦   
+---codex-prompts
¦       desktop.ini
¦       
+---data
¦   ¦   .gitkeep
¦   ¦   
¦   +---processed
¦   ¦       .gitkeep
¦   ¦       
¦   +---raw
¦           .gitkeep
¦           
+---docs
¦       AI_AGENT_NOTES.md
¦       json_protocol.md
¦       LEARNING_PATH.md
¦       
+---logs
¦       .gitkeep
¦       
+---raspberrypi_scripts
¦       debug_log_sample_rate.py
¦       install_pi_deps.sh
¦       mpu6050_multi_logger.py
¦       pi_config.yaml
¦       pi_logger_common.py
¦       README_rpi.md
¦       run_all_sensors.sh
¦       
+---src
¦   +---sensepi
¦       ¦   perf_system.py
¦       ¦   __init__.py
¦       ¦   
¦       +---analysis
¦       ¦       features.py
¦       ¦       fft.py
¦       ¦       filters.py
¦       ¦       rate.py
¦       ¦       __init__.py
¦       ¦       
¦       +---config
¦       ¦       app_config.py
¦       ¦       constants.py
¦       ¦       hosts.yaml
¦       ¦       log_paths.py
¦       ¦       pi_logger_config.py
¦       ¦       runtime.py
¦       ¦       sampling.py
¦       ¦       sensors.yaml
¦       ¦       __init__.py
¦       ¦       
¦       +---core
¦       ¦       live_stream.py
¦       ¦       models.py
¦       ¦       pipeline.py
¦       ¦       pipeline_wiring.py
¦       ¦       recorder_session.py
¦       ¦       ringbuffer.py
¦       ¦       stream_reader.py
¦       ¦       timeseries_buffer.py
¦       ¦       __init__.py
¦       ¦       
¦       +---data
¦       ¦       stream_buffer.py
¦       ¦       __init__.py
¦       ¦       
¦       +---dataio
¦       ¦       csv_writer.py
¦       ¦       file_paths.py
¦       ¦       log_loader.py
¦       ¦       __init__.py
¦       ¦       
¦       +---gui
¦       ¦   ¦   application.py
¦       ¦   ¦   benchmark.py
¦       ¦   ¦   main_window.py
¦       ¦   ¦   perf_metrics.py
¦       ¦   ¦   pg_signal_plot_widget.py
¦       ¦   ¦   __init__.py
¦       ¦   ¦   
¦       ¦   +---tabs
¦       ¦   ¦       tab_fft.py
¦       ¦   ¦       tab_logs.py
¦       ¦   ¦       tab_offline.py
¦       ¦   ¦       tab_recorder.py
¦       ¦   ¦       tab_settings.py
¦       ¦   ¦       tab_signals.py
¦       ¦   ¦       __init__.py
¦       ¦   ¦       
¦       ¦   +---widgets
¦       ¦           acquisition_settings.py
¦       ¦           collapsible.py
¦       ¦           __init__.py
¦       ¦           
¦       +---remote
¦       ¦       pi_recorder.py
¦       ¦       sensor_ingest_worker.py
¦       ¦       ssh_client.py
¦       ¦       __init__.py
¦       ¦       
¦       +---sensors
¦       ¦       mpu6050.py
¦       ¦       __init__.py
¦       ¦       
¦       +---tools
¦               debug.py
¦               local_plot_runner.py
¦               plotter.py
¦               __init__.py
¦               
+---tests
        conftest.py
        test_rate_controller.py
        test_stream_reader.py
        

------------------------- DETAILED DIRECTORY LISTING -------------------------
 Volume in drive C has no label.
 Volume Serial Number is 46B7-CF63

 Directory of C:\Projects\sense-pi-local-recording-live-main

02/12/2025  23:35    <DIR>          .
02/12/2025  22:35    <DIR>          ..
30/11/2025  16:51               417 .gitignore
02/12/2025  22:35    <DIR>          codex-prompts
30/11/2025  16:51             2.329 combine.bat
02/12/2025  23:35             3.791 combined.py
30/11/2025  16:51    <DIR>          data
30/11/2025  16:51             5.419 debug_log_channels.py
30/11/2025  16:51            11.822 debug_pc_ingest_worker.py
30/11/2025  16:51            10.294 debug_pc_recorder_stream.py
30/11/2025  16:51             4.522 debug_pi_via_ssh.py
30/11/2025  16:51             6.996 decimation.py
30/11/2025  16:51    <DIR>          docs
30/11/2025  16:51             4.652 envelope_plot.py
30/11/2025  16:51            10.030 live_plot.py
30/11/2025  16:51    <DIR>          logs
30/11/2025  16:51               901 main.py
30/11/2025  16:51               297 pi_recorder.py
30/11/2025  16:51             3.882 profile_benchmark.py
30/11/2025  16:51               717 pyproject.toml
30/11/2025  16:51    <DIR>          raspberrypi_scripts
30/11/2025  16:51            11.755 README.md
30/11/2025  16:51                39 requirements-pi.txt
30/11/2025  16:51               111 requirements.txt
30/11/2025  16:51             3.341 run_live_plot.py
30/11/2025  16:51    <DIR>          src
30/11/2025  16:51               309 ssh_client.py
30/11/2025  16:51    <DIR>          tests
              19 File(s)         81.624 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\codex-prompts

02/12/2025  22:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             1.424 desktop.ini
               1 File(s)          1.424 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
30/11/2025  16:51    <DIR>          processed
30/11/2025  16:51    <DIR>          raw
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data\processed

30/11/2025  16:51    <DIR>          .
30/11/2025  16:51    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\data\raw

30/11/2025  16:51    <DIR>          .
30/11/2025  16:51    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\docs

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             4.972 AI_AGENT_NOTES.md
30/11/2025  16:51               934 json_protocol.md
30/11/2025  16:51             7.831 LEARNING_PATH.md
               3 File(s)         13.737 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\logs

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51                 0 .gitkeep
               1 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\raspberrypi_scripts

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             8.330 debug_log_sample_rate.py
30/11/2025  16:51               360 install_pi_deps.sh
30/11/2025  16:51            41.160 mpu6050_multi_logger.py
30/11/2025  16:51               393 pi_config.yaml
30/11/2025  16:51               367 pi_logger_common.py
30/11/2025  16:51               967 README_rpi.md
30/11/2025  16:51               207 run_all_sensors.sh
               7 File(s)         51.784 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
02/12/2025  23:35    <DIR>          sensepi
               0 File(s)              0 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi

02/12/2025  23:35    <DIR>          .
30/11/2025  16:51    <DIR>          ..
02/12/2025  23:28    <DIR>          analysis
02/12/2025  23:35    <DIR>          config
02/12/2025  23:35    <DIR>          core
02/12/2025  23:28    <DIR>          data
02/12/2025  23:35    <DIR>          dataio
02/12/2025  23:28    <DIR>          gui
30/11/2025  16:51               577 perf_system.py
02/12/2025  23:35    <DIR>          remote
02/12/2025  23:35    <DIR>          sensors
02/12/2025  23:35    <DIR>          tools
30/11/2025  16:51                44 __init__.py
               2 File(s)            621 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\analysis

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               258 features.py
30/11/2025  16:51               376 fft.py
30/11/2025  16:51               491 filters.py
30/11/2025  16:51             2.068 rate.py
30/11/2025  16:51               362 __init__.py
               5 File(s)          3.555 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\config

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51            16.950 app_config.py
30/11/2025  16:51               161 constants.py
30/11/2025  16:51               415 hosts.yaml
30/11/2025  16:51             3.793 log_paths.py
30/11/2025  16:51             3.862 pi_logger_config.py
30/11/2025  16:51             3.623 runtime.py
30/11/2025  16:51             4.930 sampling.py
30/11/2025  16:51               258 sensors.yaml
30/11/2025  16:51               556 __init__.py
               9 File(s)         34.548 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\core

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               922 live_stream.py
30/11/2025  16:51               455 models.py
30/11/2025  16:51             9.002 pipeline.py
30/11/2025  16:51             3.704 pipeline_wiring.py
30/11/2025  16:51             1.596 recorder_session.py
30/11/2025  16:51             1.829 ringbuffer.py
30/11/2025  16:51             7.471 stream_reader.py
30/11/2025  16:51             3.411 timeseries_buffer.py
30/11/2025  16:51               931 __init__.py
               9 File(s)         29.321 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\data

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             8.474 stream_buffer.py
30/11/2025  16:51               835 __init__.py
               2 File(s)          9.309 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\dataio

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               453 csv_writer.py
30/11/2025  16:51               368 file_paths.py
30/11/2025  16:51               838 log_loader.py
30/11/2025  16:51               347 __init__.py
               4 File(s)          2.006 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui

02/12/2025  23:28    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             5.375 application.py
30/11/2025  16:51             5.744 benchmark.py
30/11/2025  16:51             7.104 main_window.py
30/11/2025  16:51             1.818 perf_metrics.py
30/11/2025  16:51               362 pg_signal_plot_widget.py
02/12/2025  23:28    <DIR>          tabs
02/12/2025  23:28    <DIR>          widgets
30/11/2025  16:51               351 __init__.py
               6 File(s)         20.754 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui\tabs

02/12/2025  23:28    <DIR>          .
02/12/2025  23:28    <DIR>          ..
30/11/2025  16:51            27.661 tab_fft.py
30/11/2025  16:51             4.637 tab_logs.py
30/11/2025  16:51            14.136 tab_offline.py
30/11/2025  16:51            29.948 tab_recorder.py
30/11/2025  16:51            23.099 tab_settings.py
30/11/2025  16:51           104.952 tab_signals.py
30/11/2025  16:51               192 __init__.py
               7 File(s)        204.625 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\gui\widgets

02/12/2025  23:28    <DIR>          .
02/12/2025  23:28    <DIR>          ..
30/11/2025  16:51             8.779 acquisition_settings.py
30/11/2025  16:51             1.608 collapsible.py
30/11/2025  16:51               239 __init__.py
               3 File(s)         10.626 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\remote

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             3.964 pi_recorder.py
30/11/2025  16:51             4.760 sensor_ingest_worker.py
30/11/2025  16:51             7.139 ssh_client.py
30/11/2025  16:51               404 __init__.py
               4 File(s)         16.267 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\sensors

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             4.077 mpu6050.py
30/11/2025  16:51               279 __init__.py
               2 File(s)          4.356 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\src\sensepi\tools

02/12/2025  23:35    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51             1.084 debug.py
30/11/2025  16:51             2.192 local_plot_runner.py
30/11/2025  16:51            17.804 plotter.py
30/11/2025  16:51               303 __init__.py
               4 File(s)         21.383 bytes

 Directory of C:\Projects\sense-pi-local-recording-live-main\tests

30/11/2025  16:51    <DIR>          .
02/12/2025  23:35    <DIR>          ..
30/11/2025  16:51               168 conftest.py
30/11/2025  16:51               325 test_rate_controller.py
30/11/2025  16:51             1.989 test_stream_reader.py
               3 File(s)          2.482 bytes

     Total Files Listed:
              94 File(s)        508.422 bytes
              65 Dir(s)  755.937.816.576 bytes free

============================================================================

============================= .gitignore
# File: .gitignore (ext: .gitignore
# Dir : 
# Size: 417 bytes
# Time: 30/11/2025 16:51
============================= .gitignore
# Virtualenv
.venv/
venv/
env/

# Python
__pycache__/
*.pyc
*.pyo
*.pyd

# Logs & data (keep folders, ignore contents)
logs/*
!logs/.gitkeep

data/*
!data/.gitkeep
data/raw/*
!data/raw/.gitkeep
data/processed/*
!data/processed/.gitkeep

# OS / editor junk
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp

# AI / scratch
.ai/

# Build / packaging
*.egg-info/
.dist/
.eggs/
build/
dist/

------------------------------ END OF FILE ------------------------------

============================= codex-prompts\desktop.ini
# File: desktop.ini (ext: .ini
# Dir : codex-prompts\
# Size: 1424 bytes
# Time: 30/11/2025 16:51
============================= codex-prompts\desktop.ini
[LocalizedFileNames]
1_01_offline_sync_and_open_latest.md=@1_01_offline_sync_and_open_latest.md,0
3_04-module-boundaries-doc-and-shims.md=@3_04-module-boundaries-doc-and-shims.md,0
3_03-learning-path-doc.md=@3_03-learning-path-doc.md,0
3_02-signals-tab-refactor.md=@3_02-signals-tab-refactor.md,0
3_01-gui-tab-reorg.md=@3_01-gui-tab-reorg.md,0
2_05_update_readme_log_conventions.md=@2_05_update_readme_log_conventions.md,0
2_04_offline_ui_sync_and_grouping.md=@2_04_offline_ui_sync_and_grouping.md,0
2_03_pc_offline_sync_paths.md=@2_03_pc_offline_sync_paths.md,0
2_02_pi_logger_use_helpers.md=@2_02_pi_logger_use_helpers.md,0
2_01_log_paths_and_naming_helpers.md=@2_01_log_paths_and_naming_helpers.md,0
4_05_gui_tabs_docstrings.md=@4_05_gui_tabs_docstrings.md,0
4_04_stream_buffer_and_ingest_docstrings.md=@4_04_stream_buffer_and_ingest_docstrings.md,0
4_03_buffers_and_stream_reader_docstrings.md=@4_03_buffers_and_stream_reader_docstrings.md,0
4_02_core_pipeline_docstrings.md=@4_02_core_pipeline_docstrings.md,0
4_01_pi_logger_docstrings.md=@4_01_pi_logger_docstrings.md,0
1_05_readme_download_logs_workflow.md=@1_05_readme_download_logs_workflow.md,0
1_04_offline_tab_copy_and_empty_state.md=@1_04_offline_tab_copy_and_empty_state.md,0
1_03_main_window_recording_stopped_offline_hint.md=@1_03_main_window_recording_stopped_offline_hint.md,0
1_02_signals_tab_mode_hint_offline.md=@1_02_signals_tab_mode_hint_offline.md,0

------------------------------ END OF FILE ------------------------------

============================= combine.bat
# File: combine.bat (ext: .bat
# Dir : 
# Size: 2329 bytes
# Time: 30/11/2025 16:51
============================= combine.bat
@echo off
setlocal EnableExtensions EnableDelayedExpansion

REM ====== Configuration ======
set "OUTPUT_FILE=combined.py"

REM Use the current directory as the root (run this from your main folder)
set "ROOT=%CD%"
set "OUT_FULL=%ROOT%\%OUTPUT_FILE%"

REM Delete output file if it exists
del "%OUTPUT_FILE%" 2>nul

REM ================== Directory Details (Tree + Detailed DIR) ==================
>>"%OUTPUT_FILE%" echo ============================= DIRECTORY OVERVIEW =============================
>>"%OUTPUT_FILE%" echo Root: %ROOT%
>>"%OUTPUT_FILE%" echo Timestamp: %DATE% %TIME%
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ----------------------------- TREE (with files) -----------------------------
tree "%ROOT%" /F >>"%OUTPUT_FILE%"
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ------------------------- DETAILED DIRECTORY LISTING -------------------------
dir "%ROOT%" /S /A >>"%OUTPUT_FILE%"
>>"%OUTPUT_FILE%" echo.
>>"%OUTPUT_FILE%" echo ============================================================================
>>"%OUTPUT_FILE%" echo(

REM ================== Concatenate ALL files (any extension) ======================
for /f "delims=" %%f in ('
  dir /b /s /a:-d "%ROOT%\*" ^| sort
') do (
  REM Skip the output file itself
  if /i not "%%~f"=="%OUT_FULL%" (
    REM Build nice relative labels
    set "ABS=%%~f"
    set "REL=!ABS:%ROOT%\=!"          REM e.g. sub\pkg\file.ext
    set "DIRABS=%%~dpf"
    set "DIRREL=!DIRABS:%ROOT%\=!"     REM e.g. sub\pkg\
    set "SIZE=%%~zf"
    set "TIME=%%~tf"
    set "EXT=%%~xf"

    echo Adding !REL!...

    REM Safe header lines (use echo() so parentheses are harmless)
    >>"%OUTPUT_FILE%" echo(============================= !REL!
    >>"%OUTPUT_FILE%" echo(# File: %%~nxf (ext: !EXT!)
    >>"%OUTPUT_FILE%" echo(# Dir : !DIRREL!
    >>"%OUTPUT_FILE%" echo(# Size: !SIZE! bytes
    >>"%OUTPUT_FILE%" echo(# Time: !TIME!
    >>"%OUTPUT_FILE%" echo(============================= !REL!

    REM Append file contents (binary files will be dumped raw)
    type "%%~f">>"%OUTPUT_FILE%"

    REM Separator line after content
    >>"%OUTPUT_FILE%" echo(
    >>"%OUTPUT_FILE%" echo ------------------------------ END OF FILE ------------------------------
    >>"%OUTPUT_FILE%" echo(
  )
)

echo All files from "%ROOT%" and subfolders combined into "%OUTPUT_FILE%".
pause

------------------------------ END OF FILE ------------------------------

# Dir : 
# Size: 2329 bytes
# Time: 30/11/2025 16:51
============================= combine.bat
============================= DIRECTORY OVERVIEW =============================
Root: C:\Projects\sense-pi-local-recording-live-main
Timestamp: 02/12/2025 23:35:58,80

----------------------------- TREE (with files) -----------------------------
Folder PATH listing
Volume serial number is 46B7-CF63
C:\PROJECTS\SENSE-PI-LOCAL-RECORDING-LIVE-MAIN
¦   .gitignore
¦   combine.bat
¦   combined.py
¦   debug_log_channels.py
¦   debug_pc_ingest_worker.py
¦   debug_pc_rec
------------------------------ END OF FILE ------------------------------

============================= data\.gitkeep
# File: .gitkeep (ext: .gitkeep
# Dir : data\
# Size: 0 bytes
# Time: 30/11/2025 16:51
============================= data\.gitkeep

------------------------------ END OF FILE ------------------------------

============================= data\processed\.gitkeep
# File: .gitkeep (ext: .gitkeep
# Dir : data\processed\
# Size: 0 bytes
# Time: 30/11/2025 16:51
============================= data\processed\.gitkeep

------------------------------ END OF FILE ------------------------------

============================= data\raw\.gitkeep
# File: .gitkeep (ext: .gitkeep
# Dir : data\raw\
# Size: 0 bytes
# Time: 30/11/2025 16:51
============================= data\raw\.gitkeep

------------------------------ END OF FILE ------------------------------

============================= debug_log_channels.py
# File: debug_log_channels.py (ext: .py
# Dir : 
# Size: 5419 bytes
# Time: 30/11/2025 16:51
============================= debug_log_channels.py
#!/usr/bin/env python
# debug_log_channels.py
"""
Inspect which MPU6050 channels are present and active in a log file.

Each sensor can have up to six numeric channels:

    ax, ay, az, gx, gy, gz

In many deployments we intentionally only use three channels per sensor
(ax, ay, gz) to match the GUI's 3-of-6 "default3" view and 9-plot layout.
This script reads a CSV or JSONL log produced by mpu6050_multi_logger.py
and reports, per sensor_id, which of the six channels ever carry a
non-zero, non-NaN value. It is a quick sanity check of 3-of-6 vs all-6 usage.
"""

from __future__ import annotations

import argparse
import csv
import json
import math
from pathlib import Path
from typing import Any, Dict, Iterable, Mapping

ALL_CHANNELS = ("ax", "ay", "az", "gx", "gy", "gz")


def _load_rows_csv(path: Path) -> Iterable[Dict[str, Any]]:
    """Yield rows from a CSV log as dictionaries."""
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            yield dict(row)


def _load_rows_jsonl(path: Path) -> Iterable[Dict[str, Any]]:
    """Yield rows from a JSONL log as dictionaries."""
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            text = line.strip()
            if not text:
                continue
            try:
                obj = json.loads(text)
            except json.JSONDecodeError:
                continue
            if isinstance(obj, dict):
                yield obj


def _parse_sensor_id(row: Mapping[str, Any]) -> Any:
    """Parse sensor_id from a row, returning int when possible."""
    sid_val = row.get("sensor_id")
    if sid_val is None or sid_val == "":
        return None
    try:
        return int(sid_val)
    except (TypeError, ValueError):
        # Fall back to the raw value; we'll stringify it later
        return sid_val


def _value_is_active(value: Any) -> bool:
    """
    Return True when a channel value should be considered "active".

    - Missing / empty / non-numeric values -> inactive
    - 0.0 and NaN -> inactive
    - Any other numeric value -> active
    """
    if value is None:
        return False
    if isinstance(value, str):
        value = value.strip()
        if not value:
            return False
        try:
            value = float(value)
        except ValueError:
            return False

    if isinstance(value, (int, float)):
        v = float(value)
        if v == 0.0 or math.isnan(v):
            return False
        return True

    return False


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Report which of the 6 possible MPU6050 channels (ax, ay, az, gx, gy, gz)\n"
            "are present and active per sensor in a log file."
        )
    )
    parser.add_argument(
        "path",
        help="Path to a CSV or JSONL log produced by mpu6050_multi_logger.py.",
    )
    args = parser.parse_args()

    path = Path(args.path)
    if not path.is_file():
        raise SystemExit(f"Path is not a file: {path}")

    suffix = path.suffix.lower()
    if suffix.endswith(".csv"):
        rows_iter = _load_rows_csv(path)
    elif suffix.endswith(".jsonl"):
        rows_iter = _load_rows_jsonl(path)
    else:
        raise SystemExit(f"Unsupported log extension: {path.suffix!r}")

    present_cols = set()
    # sensor_id -> channel_name -> active_flag
    coverage: Dict[Any, Dict[str, bool]] = {}

    for row in rows_iter:
        present_cols.update(row.keys())
        sid = _parse_sensor_id(row)
        if sid not in coverage:
            coverage[sid] = {ch: False for ch in ALL_CHANNELS}
        chan_flags = coverage[sid]
        for ch in ALL_CHANNELS:
            if ch in row and not chan_flags[ch] and _value_is_active(row[ch]):
                chan_flags[ch] = True

    print("=== Channel coverage ===")
    print(f"File: {path}")
    if present_cols:
        print(f"Present columns: {', '.join(sorted(present_cols))}")
    else:
        print("Present columns: (none)")

    used_channels = sorted(ch for ch in ALL_CHANNELS if ch in present_cols)
    print(
        f"Max channels per sensor is {len(ALL_CHANNELS)} "
        f"({', '.join(ALL_CHANNELS)}); this log uses {len(used_channels)}."
    )

    if not coverage:
        print("No rows found in log; nothing to report.")
        return

    # Sort sensor_ids in a stable way (None last)
    def _sort_key(sid: Any) -> tuple:
        return (sid is None, str(sid))

    for sid in sorted(coverage.keys(), key=_sort_key):
        if sid is None:
            print("\nSensor (unknown):")
        else:
            print(f"\nSensor {sid}:")

        chan_flags = coverage[sid]
        active = [ch for ch in ALL_CHANNELS if chan_flags.get(ch)]
        inactive = [ch for ch in ALL_CHANNELS if ch not in active]

        if active:
            print(f"  active: {', '.join(active)}")
        else:
            print("  active: (none)")

        if inactive:
            print(f"  inactive: {', '.join(inactive)}")
        else:
            print("  inactive: (none)")

        if len(active) > 3:
            sid_label = sid if sid is not None else "(unknown)"
            print(
                f"  NOTE: sensor {sid_label} uses "
                f"{len(active)}/{len(ALL_CHANNELS)} channels "
                f"({', '.join(active)})."
            )


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= debug_pc_ingest_worker.py
# File: debug_pc_ingest_worker.py (ext: .py
# Dir : 
# Size: 11822 bytes
# Time: 30/11/2025 16:51
============================= debug_pc_ingest_worker.py
#!/usr/bin/env python
# debug_pc_ingest_worker.py
"""
Debug SensorIngestWorker + PiRecorder in a minimal Qt event loop.

Run from project root:

    python debug_pc_ingest_worker.py
    python debug_pc_ingest_worker.py --seconds 5 --host-name Pi06

This uses hosts.yaml via HostInventory to pick the Pi, starts
PiRecorder.stream_mpu6050(...), and then feeds that stream into a
SensorIngestWorker running in a QThread. It counts how many samples
arrive per sensor_id.

Each MPU6050 sensor can provide up to 6 channels (ax, ay, az, gx, gy, gz).
This script does not inspect the channel values; it only counts how many
MpuSample instances arrive per sensor_id via SensorIngestWorker.
"""

from __future__ import annotations

import argparse
from collections import Counter
from collections.abc import Iterator
from dataclasses import dataclass
import json
from pathlib import Path
import sys
import time
from typing import Any, Dict, Optional

from PySide6.QtCore import QCoreApplication, QObject, QThread, QTimer, Slot  # type: ignore

# --- Make src/ importable ----------------------------------------------------
ROOT = Path(__file__).resolve().parent
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

from sensepi.config.app_config import HostInventory  # type: ignore
from sensepi.remote.pi_recorder import PiRecorder  # type: ignore
from sensepi.remote.sensor_ingest_worker import SensorIngestWorker  # type: ignore
from sensepi.core.live_stream import select_parser  # type: ignore
from sensepi.sensors.mpu6050 import MpuSample  # type: ignore


@dataclass
class PiStreamConfig:
    pi_device_sample_rate_hz: Optional[float] = None
    pi_stream_decimation: Optional[int] = None
    pi_stream_rate_hz: Optional[float] = None
    sensor_ids: list[int] | None = None

    @classmethod
    def from_meta_json(cls, obj: dict) -> "PiStreamConfig":
        return cls(
            pi_device_sample_rate_hz=float(obj.get("pi_device_sample_rate_hz"))
            if obj.get("pi_device_sample_rate_hz") is not None
            else None,
            pi_stream_decimation=int(obj.get("pi_stream_decimation"))
            if obj.get("pi_stream_decimation") is not None
            else None,
            pi_stream_rate_hz=float(obj.get("pi_stream_rate_hz"))
            if obj.get("pi_stream_rate_hz") is not None
            else None,
            sensor_ids=[int(s) for s in obj.get("sensor_ids", [])],
        )


def extract_pi_meta_and_wrap_stream(
    raw_stream: Iterator[str],
) -> tuple[PiStreamConfig | None, Iterator[str]]:
    """
    Consume any initial JSON meta header lines and return a cleaned
    stream iterator that yields only sample lines.
    """

    buffer: list[str] = []
    pi_cfg: PiStreamConfig | None = None

    # Try to read at most a few lines as potential meta headers
    for _ in range(3):
        try:
            line = next(raw_stream)
        except StopIteration:
            break
        if not line:
            continue
        # Try to parse JSON; ignore failures
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            buffer.append(line)
            break

        if isinstance(obj, dict) and obj.get("meta") == "mpu6050_stream_config":
            pi_cfg = PiStreamConfig.from_meta_json(obj)
            # do NOT put this line back into buffer
            continue
        else:
            buffer.append(line)
            break

    def _iter() -> Iterator[str]:
        for b in buffer:
            yield b
        for line in raw_stream:
            yield line

    return pi_cfg, _iter()


def pick_host(inv: HostInventory, name: str | None) -> Dict[str, Any]:
    """
    Pick a host entry from hosts.yaml by name (or the first one by default).
    """
    hosts = inv.list_hosts()
    if not hosts:
        raise SystemExit("No Pi hosts defined in hosts.yaml")

    if name:
        for h in hosts:
            if h.get("name") == name:
                return h
        raise SystemExit(
            f"Host {name!r} not found in hosts.yaml. "
            f"Available: {[h.get('name') for h in hosts]}"
        )

    return hosts[0]


def build_recorder(inv: HostInventory, host_dict: Dict[str, Any]) -> PiRecorder:
    """
    Construct a PiRecorder for the given host mapping from hosts.yaml.

    Uses HostInventory.to_remote_host(...) and HostInventory.scripts_dir_for(...)
    so behaviour matches the GUI's RecorderTab.
    """
    remote_host = inv.to_remote_host(host_dict)
    base_path = inv.scripts_dir_for(host_dict)
    print(
        f"Using host {host_dict.get('name', remote_host.host)} "
        f"at {remote_host.host}:{remote_host.port}, "
        f"base_path={base_path}"
    )
    return PiRecorder(remote_host, base_path=base_path)


class IngestDebug(QObject):
    """
    Small helper that owns a SensorIngestWorker in a QThread and keeps
    per-sensor sample counts for a fixed duration.
    """

    def __init__(
        self,
        recorder: PiRecorder,
        stream,
        seconds: float,
        pi_cfg: PiStreamConfig | None = None,
        parent: QObject | None = None,
    ) -> None:
        super().__init__(parent)
        self._recorder = recorder
        self._stream = stream
        self._seconds = float(seconds)
        self._pi_cfg = pi_cfg
        if self._seconds < 0:
            self._seconds = 0.0

        self._thread = QThread(self)
        parser = select_parser("mpu6050")

        def _stream_factory():
            # Mirror RecorderTab: return the same iterator; the worker will
            # consume it until stopped or the remote process exits.
            return self._stream

        self._worker = SensorIngestWorker(
            recorder=self._recorder,
            stream_factory=_stream_factory,
            parser=parser,
            batch_size=50,
            max_latency_ms=100,
            stream_label="mpu6050",
        )
        self._worker.moveToThread(self._thread)
        self._thread.started.connect(self._worker.start)

        self._worker.samples_batch.connect(self.on_batch)
        self._worker.error.connect(self.on_error)
        self._worker.finished.connect(self.on_finished)
        self._worker.finished.connect(self._worker.deleteLater)
        self._worker.finished.connect(self._thread.quit)
        self._thread.finished.connect(self._thread.deleteLater)

        app = QCoreApplication.instance()
        if app is not None:
            self._worker.finished.connect(app.quit)

        self._counts: Counter[int] = Counter()
        self._total = 0
        self._t_start: float | None = None

        # Hard stop after N seconds (even if the stream keeps running)
        if self._seconds > 0:
            QTimer.singleShot(int(self._seconds * 1000), self.stop)

    @Slot()
    def start(self) -> None:
        print("[INGEST] Starting QThread + worker...")
        self._t_start = time.time()
        self._thread.start()

    @Slot()
    def stop(self) -> None:
        print("[INGEST] stop() requested")
        try:
            self._worker.stop()
        except Exception as exc:  # pragma: no cover
            print(f"[INGEST] stop() raised: {exc!r}")

    @Slot(list)
    def on_batch(self, samples: list) -> None:
        n = len(samples)
        self._total += n
        for s in samples:
            if isinstance(s, MpuSample) and s.sensor_id is not None:
                self._counts[int(s.sensor_id)] += 1
        print(f"[INGEST] got batch of {n} samples (total={self._total})")

    @Slot(str)
    def on_error(self, msg: str) -> None:
        print(f"[INGEST ERROR] {msg}")

    @Slot()
    def on_finished(self) -> None:
        print("[INGEST] finished")
        print(f"Total samples seen: {self._total}")

        elapsed: float | None = None
        if self._t_start is not None:
            elapsed = max(0.0, time.time() - self._t_start)
        elif self._seconds > 0:
            elapsed = self._seconds

        print("\n=== Ingest summary (Pi vs PC) ===")

        # Pi config
        if self._pi_cfg and (
            self._pi_cfg.pi_device_sample_rate_hz is not None
            or self._pi_cfg.pi_stream_rate_hz is not None
        ):
            print("Pi config:")
            if self._pi_cfg.pi_device_sample_rate_hz is not None:
                print(
                    "  pi_device_sample_rate_hz = "
                    f"{self._pi_cfg.pi_device_sample_rate_hz:.1f}"
                )
            if self._pi_cfg.pi_stream_decimation is not None:
                print(
                    "  pi_stream_decimation     = "
                    f"{self._pi_cfg.pi_stream_decimation}"
                )
            if self._pi_cfg.pi_stream_rate_hz is not None:
                print(
                    "  pi_stream_rate_hz        = "
                    f"{self._pi_cfg.pi_stream_rate_hz:.1f}"
                )
        else:
            print("Pi config: (unknown in this run)")

        # PC ingest
        print("\nPC ingest:")
        pc_rates: list[float] = []
        for sid in sorted(self._counts.keys()):
            count = self._counts[sid]
            line = f"  sensor_id={sid}: {count} samples"
            if elapsed and elapsed > 0:
                approx_rate = count / elapsed
                pc_rates.append(approx_rate)
                line += f" â†’ pc_effective_rate_hz â‰ˆ {approx_rate:.1f}"
            print(line)

        if pc_rates and self._pi_cfg and self._pi_cfg.pi_stream_rate_hz:
            avg_pc_rate = sum(pc_rates) / len(pc_rates)
            loss_pct = 100.0 * (1.0 - (avg_pc_rate / self._pi_cfg.pi_stream_rate_hz))
            print("\nComparison:")
            print(
                "  expected_pc_rate_hz (from Pi) "
                f"â‰ˆ {self._pi_cfg.pi_stream_rate_hz:.1f}"
            )
            print(f"  measured_pc_rate_hz           â‰ˆ {avg_pc_rate:.1f}")
            print(f"  loss_vs_pi_stream             â‰ˆ {loss_pct:.1f}%")


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Run SensorIngestWorker + PiRecorder in a minimal Qt event loop\n"
            "and print per-sensor sample counts for a short window."
        ),
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--host-name",
        type=str,
        default=None,
        help="Optional host name from hosts.yaml (e.g. Pi06)",
    )
    parser.add_argument(
        "--seconds",
        type=float,
        default=5.0,
        help="How long to let the ingest worker run.",
    )
    parser.add_argument(
        "--extra-args",
        type=str,
        default="",
        help=(
            "Extra CLI args for mpu6050_multi_logger.py, e.g.:\n"
            "  --sample-rate-hz 300 --stream-every 3\n"
            "  --rate 100 --channels both --sensors 1,2,3 --stream-every 5"
        ),
    )
    args = parser.parse_args()

    app = QCoreApplication(sys.argv)

    inv = HostInventory()
    host_dict = pick_host(inv, args.host_name)
    rec = build_recorder(inv, host_dict)

    def on_stderr(line: str) -> None:
        print(f"[REMOTE STDERR] {line}", flush=True)

    print("\n=== Starting PiRecorder.stream_mpu6050() for ingest debug ===")
    stream = rec.stream_mpu6050(
        extra_args=args.extra_args,
        recording=False,
        on_stderr=on_stderr,
    )

    pi_cfg, stream = extract_pi_meta_and_wrap_stream(stream)

    dbg = IngestDebug(
        recorder=rec,
        stream=stream,
        seconds=args.seconds,
        pi_cfg=pi_cfg,
    )
    QTimer.singleShot(0, dbg.start)

    app.exec()

    print("Qt event loop exited; closing recorder...")
    try:
        rec.close()
    except Exception as exc:  # pragma: no cover
        print(f"[WARN] rec.close() raised: {exc!r}")


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= debug_pc_recorder_stream.py
# File: debug_pc_recorder_stream.py (ext: .py
# Dir : 
# Size: 10294 bytes
# Time: 30/11/2025 16:51
============================= debug_pc_recorder_stream.py
#!/usr/bin/env python
# debug_pc_recorder_stream.py
"""
Debug streaming using the SAME PiRecorder stack the GUI uses (no Qt).

Run from project root:

    python debug_pc_recorder_stream.py
    python debug_pc_recorder_stream.py --seconds 5
    python debug_pc_recorder_stream.py --host-name Pi06

This uses hosts.yaml via HostInventory to pick the Pi and then
PiRecorder.stream_mpu6050(...) to read JSON lines, just like RecorderTab.

Each MPU6050 sensor has up to 6 channels (ax, ay, az, gx, gy, gz), but the
GUI typically plots only three (ax, ay, gz) per sensor in the default view.
This script does not inspect channel values; it only counts how many
MpuSample rows arrive per sensor_id and estimates an effective stream rate.
"""

from __future__ import annotations

import argparse
from collections import Counter
from collections.abc import Iterator
from dataclasses import dataclass
import json
import re
from pathlib import Path
import sys
import time
from typing import Any, Dict, Optional

# --- Make src/ importable ----------------------------------------------------
ROOT = Path(__file__).resolve().parent
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

from sensepi.config.app_config import HostInventory  # type: ignore
from sensepi.remote.pi_recorder import PiRecorder  # type: ignore
from sensepi.sensors.mpu6050 import MpuSample, parse_line  # type: ignore


@dataclass
class PiStreamConfig:
    pi_device_sample_rate_hz: Optional[float] = None
    pi_stream_decimation: Optional[int] = None
    pi_stream_rate_hz: Optional[float] = None
    sensor_ids: list[int] | None = None

    @classmethod
    def from_meta_json(cls, obj: dict) -> "PiStreamConfig":
        return cls(
            pi_device_sample_rate_hz=float(obj.get("pi_device_sample_rate_hz"))
            if obj.get("pi_device_sample_rate_hz") is not None
            else None,
            pi_stream_decimation=int(obj.get("pi_stream_decimation"))
            if obj.get("pi_stream_decimation") is not None
            else None,
            pi_stream_rate_hz=float(obj.get("pi_stream_rate_hz"))
            if obj.get("pi_stream_rate_hz") is not None
            else None,
            sensor_ids=[int(s) for s in obj.get("sensor_ids", [])],
        )


def extract_pi_meta_and_wrap_stream(
    raw_stream: Iterator[str],
) -> tuple[PiStreamConfig | None, Iterator[str]]:
    """
    Consume any initial JSON meta header lines and return a cleaned
    stream iterator that yields only sample lines.
    """

    buffer: list[str] = []
    pi_cfg: PiStreamConfig | None = None

    # Try to read at most a few lines as potential meta headers
    for _ in range(3):
        try:
            line = next(raw_stream)
        except StopIteration:
            break
        if not line:
            continue
        # Try to parse JSON; ignore failures
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            buffer.append(line)
            break

        if isinstance(obj, dict) and obj.get("meta") == "mpu6050_stream_config":
            pi_cfg = PiStreamConfig.from_meta_json(obj)
            # do NOT put this line back into buffer
            continue
        else:
            buffer.append(line)
            break

    def _iter() -> Iterator[str]:
        for b in buffer:
            yield b
        for line in raw_stream:
            yield line

    return pi_cfg, _iter()


def pick_host(inv: HostInventory, name: str | None) -> Dict[str, Any]:
    """
    Pick a host entry from hosts.yaml by name (or the first one by default).
    """
    hosts = inv.list_hosts()
    if not hosts:
        raise SystemExit("No Pi hosts defined in hosts.yaml")

    if name:
        for h in hosts:
            if h.get("name") == name:
                return h
        raise SystemExit(
            f"Host {name!r} not found in hosts.yaml. "
            f"Available: {[h.get('name') for h in hosts]}"
        )

    # Default: first host
    return hosts[0]


def build_recorder(inv: HostInventory, host_dict: Dict[str, Any]) -> PiRecorder:
    """
    Construct a PiRecorder for the given host mapping from hosts.yaml.

    Uses HostInventory.to_remote_host(...) and HostInventory.scripts_dir_for(...)
    so the behaviour matches what the GUI's RecorderTab uses.
    """
    remote_host = inv.to_remote_host(host_dict)
    base_path = inv.scripts_dir_for(host_dict)
    print(
        f"Using host {host_dict.get('name', remote_host.host)} "
        f"at {remote_host.host}:{remote_host.port}, "
        f"base_path={base_path}"
    )
    return PiRecorder(remote_host, base_path=base_path)


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Stream MPU6050 samples via PiRecorder.stream_mpu6050() and print\n"
            "per-sensor sample counts and approximate effective stream rates."
        ),
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--host-name",
        type=str,
        default=None,
        help="Optional host name from hosts.yaml (e.g. Pi06)",
    )
    parser.add_argument(
        "--seconds",
        type=float,
        default=3.0,
        help="How long to read from the stream (wall-clock seconds).",
    )
    parser.add_argument(
        "--extra-args",
        type=str,
        default="",
        help=(
            "Extra CLI args for mpu6050_multi_logger.py, e.g.:\n"
            "  --sample-rate-hz 300 --stream-every 3\n"
            "  --rate 100 --channels both --sensors 1,2,3 --stream-every 5"
        ),
    )
    args = parser.parse_args()

    inv = HostInventory()
    host_dict = pick_host(inv, args.host_name)
    rec = build_recorder(inv, host_dict)

    counts: Counter[int] = Counter()
    total = 0
    seconds = float(args.seconds)
    if seconds < 0:
        seconds = 0.0

    pi_cfg: PiStreamConfig | None = None
    pi_cfg_from_stderr = PiStreamConfig()

    PI_STREAM_RE = re.compile(
        r"pi_device_sample_rate_hz=(?P<dev>[0-9.]+)\s+"
        r"pi_stream_decimation=(?P<dec>\d+)\s+"
        r"pi_stream_rate_hz=(?P<rate>[0-9.]+)"
    )

    def on_stderr(line: str) -> None:
        print(f"[REMOTE STDERR] {line}", flush=True)

        m = PI_STREAM_RE.search(line)
        if m:
            try:
                pi_cfg_from_stderr.pi_device_sample_rate_hz = float(
                    m.group("dev")
                )
                pi_cfg_from_stderr.pi_stream_decimation = int(m.group("dec"))
                pi_cfg_from_stderr.pi_stream_rate_hz = float(m.group("rate"))
            except Exception:
                pass

    stream = None
    try:
        print("\n=== Starting PiRecorder.stream_mpu6050() ===")
        print(f"extra_args: {args.extra_args!r}")
        stream = rec.stream_mpu6050(
            extra_args=args.extra_args,
            recording=False,
            on_stderr=on_stderr,
        )

        pi_cfg, stream = extract_pi_meta_and_wrap_stream(stream)
        if pi_cfg is None and any(
            [
                pi_cfg_from_stderr.pi_device_sample_rate_hz,
                pi_cfg_from_stderr.pi_stream_decimation,
                pi_cfg_from_stderr.pi_stream_rate_hz,
            ]
        ):
            pi_cfg = pi_cfg_from_stderr

        t_end = time.time() + seconds if seconds > 0 else None

        for raw in stream:
            if not raw:
                continue
            total += 1

            sample = parse_line(raw)
            if isinstance(sample, MpuSample) and sample.sensor_id is not None:
                counts[int(sample.sensor_id)] += 1

            # Show the first few lines for quick visual confirmation
            if total <= 5:
                print(f"[LINE] {raw.rstrip()}")

            if t_end is not None and time.time() >= t_end:
                break

    finally:
        # Try to close the stream iterator explicitly
        if stream is not None:
            close = getattr(stream, "close", None)
            if callable(close):
                try:
                    print("Closing stream iterator...")
                    close()
                except Exception as exc:  # pragma: no cover
                    print(f"[WARN] stream.close() raised: {exc!r}")

        print("Closing PiRecorder...")
        try:
            rec.close()
        except Exception as exc:  # pragma: no cover
            print(f"[WARN] rec.close() raised: {exc!r}")

    print("\n=== Stream summary (Pi vs PC) ===")
    print(f"Total lines read: {total}")

    elapsed = seconds if seconds > 0 else None

    # Pi config (if known)
    if pi_cfg and (
        pi_cfg.pi_device_sample_rate_hz is not None
        or pi_cfg.pi_stream_rate_hz is not None
    ):
        print("Pi config:")
        if pi_cfg.pi_device_sample_rate_hz is not None:
            print(
                f"  pi_device_sample_rate_hz = "
                f"{pi_cfg.pi_device_sample_rate_hz:.1f}"
            )
        if pi_cfg.pi_stream_decimation is not None:
            print(f"  pi_stream_decimation     = {pi_cfg.pi_stream_decimation}")
        if pi_cfg.pi_stream_rate_hz is not None:
            print(
                f"  pi_stream_rate_hz        = {pi_cfg.pi_stream_rate_hz:.1f}"
            )
    else:
        print("Pi config: (unknown in this run)")

    # PC measurements
    print("\nPC measurements (per sensor):")
    pc_rates: list[float] = []
    for sid in sorted(counts.keys()):
        count = counts[sid]
        line = f"  sensor_id={sid}: {count} samples"
        if elapsed and elapsed > 0:
            approx_rate = count / elapsed
            pc_rates.append(approx_rate)
            line += f" over {elapsed:.1f} s â†’ pc_effective_rate_hz â‰ˆ {approx_rate:.1f}"
        print(line)

    # Comparison Pi vs PC
    if pc_rates and pi_cfg and pi_cfg.pi_stream_rate_hz:
        avg_pc_rate = sum(pc_rates) / len(pc_rates)
        loss_pct = 100.0 * (1.0 - (avg_pc_rate / pi_cfg.pi_stream_rate_hz))
        print("\nComparison:")
        print(
            f"  expected_pc_rate_hz (from Pi) â‰ˆ {pi_cfg.pi_stream_rate_hz:.1f}"
        )
        print(f"  measured_pc_rate_hz           â‰ˆ {avg_pc_rate:.1f}")
        print(f"  loss_vs_pi_stream             â‰ˆ {loss_pct:.1f}%")


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= debug_pi_via_ssh.py
# File: debug_pi_via_ssh.py (ext: .py
# Dir : 
# Size: 4522 bytes
# Time: 30/11/2025 16:51
============================= debug_pi_via_ssh.py
from __future__ import annotations

import sys
import time
from typing import Tuple

import paramiko


# ====== EDIT THESE IF NEEDED ======
PI_HOST = "192.168.0.6"
PI_USER = "verwalter"
PI_PASSWORD = "!66442200"
PI_BASE_PATH = "~/sensor"  # same as hosts.yaml base_path
LOGGER_SCRIPT = "mpu6050_multi_logger.py"
PI_CONFIG = "pi_config.yaml"
# ==================================


def _run_remote(
    ssh: paramiko.SSHClient,
    command: str,
    *,
    print_output: bool = True,
) -> Tuple[int, str, str]:
    """Run a command on the Pi and return (exit_code, stdout, stderr)."""

    print(f"\n=== Running on Pi: {command}")
    stdin, stdout, stderr = ssh.exec_command(command)

    # Wait explicitly for command to finish
    exit_status = stdout.channel.recv_exit_status()

    out = stdout.read().decode("utf-8", errors="replace")
    err = stderr.read().decode("utf-8", errors="replace")

    if print_output:
        print("--- stdout ---")
        print(out if out.strip() else "(empty)")
        print("--- stderr ---")
        print(err if err.strip() else "(empty)")
        print(f"--- exit code: {exit_status} ---")

    return exit_status, out, err


def main() -> None:
    print("=== SensePi Pi debug via SSH ===")
    print(f"Host: {PI_HOST}, user: {PI_USER}")
    print(f"Base path on Pi: {PI_BASE_PATH}")
    print()

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    try:
        print("Connecting via username + password...")
        ssh.connect(
            PI_HOST,
            username=PI_USER,
            password=PI_PASSWORD,
            look_for_keys=False,
            allow_agent=False,
            timeout=10,
        )
        print("Connected OK.\n")

        # 1) Where are we and what is in ~/sensor?
        _run_remote(ssh, "pwd")
        _run_remote(ssh, f"ls -ld {PI_BASE_PATH}")
        _run_remote(ssh, f"cd {PI_BASE_PATH} && pwd && ls -l")

        # 2) Check Python & smbus2 import
        print("\n=== Check Python & smbus2 import ===")
        smbus_check = (
            "cd {base} && "
            "python3 - << 'EOF'\n"
            "try:\n"
            "    import smbus2\n"
            "    print('OK: smbus2 import worked')\n"
            "except Exception as e:\n"
            "    print('ERROR: smbus2 import failed:', e)\n"
            "EOF"
        ).format(base=PI_BASE_PATH)
        _run_remote(ssh, smbus_check)

        # 3) Check logger & config presence
        print("\n=== Check logger & pi_config.yaml existence ===")
        _run_remote(
            ssh,
            f"cd {PI_BASE_PATH} && "
            f"ls -l {LOGGER_SCRIPT} || echo '!! {LOGGER_SCRIPT} missing'",
        )
        _run_remote(
            ssh,
            f"cd {PI_BASE_PATH} && "
            f"ls -l {PI_CONFIG} || echo '!! {PI_CONFIG} missing'",
        )

        # 4) I2C device scan via logger --list
        print("\n=== Logger I2C scan: mpu6050_multi_logger.py --list ===")
        _run_remote(
            ssh,
            f"cd {PI_BASE_PATH} && python3 {LOGGER_SCRIPT} --list",
        )

        # 5) Short streaming test (what the GUI basically does)
        print("\n=== Short streaming test (stdout captured) ===")
        stream_cmd = (
            f"cd {PI_BASE_PATH} && "
            f"python3 {LOGGER_SCRIPT} "
            f"--config {PI_CONFIG} "
            f"--stream-stdout "
            f"--no-record "
            f"--stream-every 5 "
            f"--samples 50"
        )
        rc, out, err = _run_remote(ssh, stream_cmd)

        print("\n=== Summary of streaming test ===")
        print(f"Exit code: {rc}")
        # Show just first few lines of stdout for sanity
        out_lines = [ln for ln in out.splitlines() if ln.strip()]
        print(f"Stdout lines: {len(out_lines)}")
        for ln in out_lines[:5]:
            print("OUT:", ln)
        if len(out_lines) > 5:
            print("... (more lines truncated)")

        if err.strip():
            print("\nStderr (first 20 lines):")
            err_lines = err.splitlines()
            for ln in err_lines[:20]:
                print("ERR:", ln)
            if len(err_lines) > 20:
                print("... (more lines truncated)")
        else:
            print("\nStderr: (empty)")

        print("\n=== Debug finished ===")

    except Exception as exc:
        print(f"\nFATAL: SSH debug failed: {exc!r}")
    finally:
        try:
            ssh.close()
        except Exception:
            pass


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= decimation.py
# File: decimation.py (ext: .py
# Dir : 
# Size: 6996 bytes
# Time: 30/11/2025 16:51
============================= decimation.py
"""Decimation and smoothing helpers for SensePi sensor data.

This module exposes a streaming friendly :class:`Decimator` that transforms
high-rate sensor samples into plot-ready series by combining decimation,
optional exponential smoothing, and optional min/max envelopes.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional, Tuple

import numpy as np

WindowOutputs = Tuple[np.ndarray, np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]


@dataclass
class DecimationConfig:
    """Configuration for a decimator instance."""

    sensor_fs: float  # Sensor sampling frequency in Hz.
    plot_fs: float  # Desired plot refresh rate in Hz.
    use_envelope: bool = True
    window_mode: str = "block"  # "block" or "sliding".
    smoothing_alpha: Optional[float] = None  # Exponential smoothing factor.

    def __post_init__(self) -> None:
        if self.sensor_fs <= 0 or self.plot_fs <= 0:
            raise ValueError("sensor_fs and plot_fs must be positive.")
        if self.window_mode not in {"block", "sliding"}:
            raise ValueError(f"Unsupported window_mode '{self.window_mode}'.")
        if self.smoothing_alpha is not None:
            if not (0.0 < self.smoothing_alpha <= 1.0):
                raise ValueError("smoothing_alpha must be within (0, 1]; use None to disable.")

    def decimation_factor(self) -> int:
        """Return integer decimation factor (number of samples per output)."""
        D = int(self.sensor_fs / self.plot_fs)
        if D <= 0:
            raise ValueError(f"Invalid decimation factor: {D}")
        return D

    def window_step(self) -> int:
        """Return window stride measured in sensor samples."""
        D = self.decimation_factor()
        if self.window_mode == "block":
            return D
        # Sliding windows keep half of the previous window by default.
        return max(1, min(D, D // 2 or 1))


@dataclass
class Decimator:
    """Streaming decimator that converts raw samples to plot-friendly data."""

    config: DecimationConfig
    _buffer: np.ndarray = field(init=False, repr=False)
    _idx: int = field(init=False, default=0, repr=False)
    _y_lp: Optional[float] = field(init=False, default=None, repr=False)
    _buffer_t0: Optional[float] = field(init=False, default=None, repr=False)
    _window_step: int = field(init=False, repr=False)
    _dt: float = field(init=False, repr=False)

    def __post_init__(self) -> None:
        D = self.config.decimation_factor()
        self._buffer = np.empty(D, dtype=np.float32)
        self._idx = 0
        self._buffer_t0 = None
        self._window_step = self.config.window_step()
        self._dt = 1.0 / float(self.config.sensor_fs)
        if self.config.window_mode == "sliding" and self._window_step > D:
            self._window_step = D

    def reset(self) -> None:
        """Reset internal buffers and smoothing state."""
        self._idx = 0
        self._buffer_t0 = None
        if self.config.smoothing_alpha is not None:
            self._y_lp = None

    def process_block(self, samples: np.ndarray, start_time: float) -> WindowOutputs:
        """Decimate a contiguous 1D array of samples into window statistics.

        Parameters
        ----------
        samples:
            Raw samples acquired at `sensor_fs`.
        start_time:
            Absolute time (seconds) of the first entry inside `samples`.

        Returns
        -------
        t_dec:
            1-D array with timestamps (seconds) located at the midpoint of each
            decimated interval.
        y_mean:
            Mean value for each interval.
        y_min, y_max:
            Optional envelope arrays when `use_envelope=True`.
        """
        flat = np.asarray(samples)
        if flat.ndim == 0:
            flat = flat.reshape(1)
        else:
            flat = flat.reshape(-1)
        n_samples = flat.size
        if n_samples == 0:
            empty_t = np.empty(0, dtype=np.float64)
            empty_y = np.empty(0, dtype=np.float32)
            if self.config.use_envelope:
                empty_env = np.empty(0, dtype=np.float32)
                return empty_t, empty_y, empty_env, empty_env.copy()
            return empty_t, empty_y, None, None

        buffer = self._buffer
        idx = self._idx
        buf_t0 = self._buffer_t0
        dt = self._dt
        D = buffer.size
        stride = self._window_step
        alpha = self.config.smoothing_alpha
        y_lp = self._y_lp

        t_list = []
        mean_list = []
        min_list = [] if self.config.use_envelope else None
        max_list = [] if self.config.use_envelope else None

        sample_time = float(start_time)
        block_duration = D * dt

        for value in flat:
            sample_val = float(value)
            if alpha is not None:
                if y_lp is None:
                    y_lp = sample_val
                else:
                    y_lp = y_lp + alpha * (sample_val - y_lp)
                sample_val = y_lp

            if idx == 0 and buf_t0 is None:
                buf_t0 = sample_time

            buffer[idx] = sample_val
            idx += 1
            sample_time += dt

            if idx == D:
                block_view = buffer[:D]
                mean_val = float(block_view.mean(dtype=np.float64))
                mean_list.append(mean_val)
                if self.config.use_envelope:
                    min_val = float(block_view.min())
                    max_val = float(block_view.max())
                    min_list.append(min_val)
                    max_list.append(max_val)

                window_start = buf_t0 if buf_t0 is not None else (sample_time - block_duration)
                t_list.append(window_start + 0.5 * block_duration)

                if stride >= D:
                    idx = 0
                    buf_t0 = None
                else:
                    overlap = D - stride
                    if overlap > 0:
                        buffer[:overlap] = buffer[stride:D]
                        idx = overlap
                        buf_t0 = window_start + stride * dt
                    else:
                        idx = 0
                        buf_t0 = None

        self._idx = idx
        self._buffer_t0 = buf_t0
        if alpha is not None:
            self._y_lp = y_lp

        t_dec = np.asarray(t_list, dtype=np.float64)
        y_mean = np.asarray(mean_list, dtype=np.float32)
        if self.config.use_envelope:
            y_min = np.asarray(min_list, dtype=np.float32)
            y_max = np.asarray(max_list, dtype=np.float32)
        else:
            y_min = None
            y_max = None

        return t_dec, y_mean, y_min, y_max


def decimate_array(samples: np.ndarray, start_time: float, config: DecimationConfig) -> WindowOutputs:
    """Stateless convenience helper for one-off block processing."""
    decimator = Decimator(config)
    return decimator.process_block(samples, start_time)

------------------------------ END OF FILE ------------------------------

============================= docs\AI_AGENT_NOTES.md
# File: AI_AGENT_NOTES.md (ext: .md
# Dir : docs\
# Size: 4972 bytes
# Time: 30/11/2025 16:51
============================= docs\AI_AGENT_NOTES.md
# AI Agent Notes

## 1. Multi-rate architecture (short summary)

SensePi purposely runs three independent rates. The Pi logger samples sensors at the configured `--rate` (or `sensors.yaml` `sample_rate_hz`) and writes every point to disk when recording. The Pi-to-GUI stream uses `--stream-every` and `--stream-fields` so the logger can forward a lighter subset of samples over SSH without altering what gets recorded. The GUI itself refreshes via the `SignalsTab` QTimer (or manual refresh modes) so plots only redraw as fast as the Qt loop can stay responsive.

Acquisition, streaming, and plotting must stay decoupled: the Pi always records at the full sampling rate even if the stream is sparse, the stream can be throttled to keep bandwidth manageable, and the Qt refresh rate only dictates how often canvases repaint. Keeping these layers independent is what lets operators view trends in real time without jeopardizing data capture or UI responsiveness. When adding new behaviors, never couple GUI pacing to recorder pacingâ€”let the buffer/signal plumbing do the work.

## 2. Buffers and data flow

1. `raspberrypi_scripts/mpu6050_multi_logger.py` samples sensors and optionally writes JSON lines (`timestamp_ns`, `t_s`, `sensor_id`, `ax..gz`, etc.) to stdout while logging to disk.
2. `PiRecorder` (SSH wrapper) starts that logger on the Pi and exposes an iterable of streamed JSON lines back to the desktop app.
3. `RecorderTab` reads the iterator on a worker thread, parses each line into `MpuSample` via `sensepi.sensors.mpu6050.parse_line`, and emits Qt signals: `sample_received(object)` for tabs such as `SignalsTab` and `FftTab`, plus `rate_updated(str, float)` so the UI can show live throughput estimates.
4. Visualization tabs maintain `RingBuffer` instances for recent samples and periodically redraw Matplotlib canvases from those buffers.

AI agents must respect this pipeline. All heavy lifting (SSH, parsing, FFTs) belongs in worker threads or background tasks, not the GUI thread. Tabs should subscribe to `sample_received`, push data into buffers, and let timed redraws pull from those buffers. New sensors or plots should reuse this signal/slot + buffer pattern to avoid tight coupling and blocking operations.

## 3. Performance design principles

**Do**
- Use `RingBuffer` for sliding windows instead of unbounded Python lists.
- Downsample/decimate before plotting large streams so CPU and Matplotlib stay light.
- Keep Matplotlib canvases alive; update data via `Line2D.set_data` and trigger `FigureCanvasQTAgg.draw_idle()`.
- Run heavy I/O, parsing, or FFT work outside the GUI thread (worker threads, async tasks).

**Donâ€™t**
- Call blocking SSH/file/network operations from Qt slots in the main thread.
- Re-create Matplotlib figures/axes/lines for each frame if they can be updated in place.
- Bypass `RecorderTab` to talk directly to the Pi from other tabs; share its iterator/signals instead.

## 4. How to propose changes

- Keep patches small and scoped; prefer extending existing tabs/modules over introducing new frameworks.
- When adding a feature, update the relevant `tab_*.py` file, any `sensors.yaml` or `hosts.yaml` entries, and add/adjust docs under `docs/`.
- For performance-sensitive code, leave lightweight comments explaining non-obvious patterns (e.g., downsampling to keep CPU low).

## 5. Concrete example

```python
from ...core.ringbuffer import RingBuffer
from ...sensors.mpu6050 import MpuSample

class ExampleNewTab(QWidget):
    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self._buffer = RingBuffer[tuple[float, float]](capacity=5000)
        ...  # set up Matplotlib figure/canvas and QTimer

    @Slot(object)
    def handle_sample(self, sample: object) -> None:
        if not isinstance(sample, MpuSample):
            return
        t_s = float(sample.t_s) if sample.t_s is not None else sample.timestamp_ns * 1e-9
        self._buffer.append((t_s, float(sample.ax)))

    def redraw(self) -> None:
        # Downsample from buffer, update Line2D data, and call draw_idle()
        ...
```

Connect `RecorderTab.sample_received` to `handle_sample`, keep the redraw timer independent of the sample rate, and use the buffer contents for FFTs or additional plots as needed.

## 6. Acceptance criteria

- `docs/AI_AGENT_NOTES.md` (this file) summarizes the multi-rate architecture, buffer flow, and performance constraints.
- After reading it, future AI agents should know to respect the decoupled acquisition/streaming/plotting design, reuse the signal/buffer pipeline, and keep integrations small and well reasoned.

## 7. Log conventions quick reference

For any work that touches file paths, syncing, or naming rules, start with **README.md â†’ Log Conventions**. It documents exactly where the Pi and desktop store logs, how session names affect directory layout, the filename schema, and how decimation is recorded in the `.meta.json` sidecars.

------------------------------ END OF FILE ------------------------------

============================= docs\json_protocol.md
# File: json_protocol.md (ext: .md
# Dir : docs\
# Size: 934 bytes
# Time: 30/11/2025 16:51
============================= docs\json_protocol.md
# SensePi JSON streaming protocol

This document describes the JSON protocol used by the MPU6050 logger when streaming samples over stdout. The Raspberry Pi MPU6050 logger streams one JSON object per line. Each entry includes:

- `timestamp_ns` (int): monotonic timestamp in nanoseconds.
- `t_s` (float): seconds since the run started.
- `sensor_id` (string): identifier for the sensor/logger instance.
- `ax`, `ay`, `az` (float): acceleration axes.
- Optional `gx`, `gy`, `gz` (float): gyroscope axes when present.

Example payload:

```json
{"timestamp_ns": 1712500000000000000,
 "t_s": 0.123,
 "sensor_id": "mpu6050_1",
 "ax": 0.01, "ay": -0.02, "az": 1.02,
 "gx": 0.001, "gy": 0.002, "gz": -0.001}
```

Parsers in :mod:`sensepi.sensors` validate the required fields, log warnings for
missing or malformed data, and drop invalid lines. GUI streaming components only
forward samples that successfully decode.

------------------------------ END OF FILE ------------------------------

============================= docs\LEARNING_PATH.md
# File: LEARNING_PATH.md (ext: .md
# Dir : docs\
# Size: 7831 bytes
# Time: 30/11/2025 16:51
============================= docs\LEARNING_PATH.md
# SensePi Learning Path

This guide walks you through the SensePi codebase in small, hands-on steps.
Each milestone includes:
- Files to read (in order).
- A small change to make.
- What you should observe after the change.

## Milestone 1 â€“ Get oriented: main window and tabs

**Goal:** See how the GUI starts and where tabs are created.

**Read:**
1. `main.py` â€“ notice that it just forwards to `sensepi.gui.application`.
2. `src/sensepi/gui/application.py` â€“ follow how `MainWindow` is constructed.
3. `src/sensepi/gui/main_window.py` â€“ especially the `MainWindow` class that wires the tabs together.

**Task:**
- In `MainWindow.__init__` change `self.setWindowTitle("SensePi Recorder")` to something like `self.setWindowTitle("SensePi â€“ My Test")`.
- Launch the GUI with `python -m sensepi.gui.application` (from the repo root) to confirm you are editing the right entry point.

**Observe:** The title bar of the Qt window should show the new text after the app launches.

---

## Milestone 2 â€“ Connecting to the Raspberry Pi

**Goal:** Understand how Pi hosts are configured and shown in the GUI.

**Read:**
1. `src/sensepi/config/hosts.yaml` â€“ the YAML file that lists known Raspberry Pi hosts under the `pis:` array.
2. `src/sensepi/config/app_config.py` â€“ skim `HostConfig`, `HostInventory`, and how host records are loaded.
3. `src/sensepi/gui/tabs/tab_recorder.py` â€“ the `RecorderTab` host combo box and `_load_hosts`.

**Task:**
- Add a fake host entry to `hosts.yaml`, e.g.:
  ```yaml
  - name: MyTestPi
    host: 192.168.0.123
    user: pi
    password: "changeme"
    base_path: "/home/pi/sensor"
    data_dir: "/home/pi/logs"
    pi_config_path: "/home/pi/sensor/pi_config.yaml"
  ```
- Restart the GUI. Open the **Device** tab and drop down the host selector.

**Observe:** `MyTestPi` (or your chosen name) should appear in the host combo box, confirming that the YAML change was picked up.

---

## Milestone 3 â€“ Sensor settings and defaults

**Goal:** See how default sensor options are configured.

**Read:**
1. `src/sensepi/config/sensors.yaml` â€“ default sampling and per-sensor options.
2. `src/sensepi/gui/tabs/tab_recorder.py` â€“ the "MPU6050 settings" group built inside `_build_ui`.
3. `src/sensepi/gui/tabs/tab_settings.py` â€“ how the Settings tab edits those YAML-backed defaults.

**Task:**
- Change one of the hard-coded defaults in the Recorder tab, e.g. update `self.mpu_sensors_edit = QLineEdit("1,2,3", ...)` so it reads `"1"` instead.
- Run the GUI and switch to the **Device** tab.

**Observe:** The "Sensors" text field should now start with your modified default each time the app launches.

---

## Milestone 4 â€“ From Start button to Pi script

**Goal:** Trace what happens when you press â€œStartâ€.

**Read:**
1. `src/sensepi/gui/tabs/tab_signals.py` â€“ the `_on_start_clicked` slot and how it emits `start_stream_requested`.
2. `src/sensepi/gui/main_window.py` â€“ where that signal is connected to `_on_start_stream_requested`, which coordinates the tabs.
3. `src/sensepi/remote/pi_recorder.py` â€“ how the remote logging script is launched over SSH once `_on_start_stream_requested` delegates to `RecorderTab`.

**Task:**
- Add `print("Start clicked")` (or similar) near the top of `_on_start_clicked`.
- Launch the GUI from a terminal, open the **Live Signals** tab, and click **Start**.

**Observe:** The terminal running the GUI should print your message, proving that you have traced the button â†’ signal â†’ start pipeline path.

---

## Milestone 5 â€“ Live data pipeline and plotting

**Goal:** Understand how incoming samples reach the live plot.

**Read:**
1. `src/sensepi/remote/sensor_ingest_worker.py` â€“ how streamed samples are read and pushed into queues.
2. `src/sensepi/core/ringbuffer.py` (and nearby `timeseries_buffer.py`) â€“ how buffers store rolling samples for the GUI.
3. `src/sensepi/gui/tabs/tab_signals.py` â€“ the `_drain_samples` method that periodically consumes queued samples and updates the plot widget.

**Task:**
- Inside `_drain_samples`, after popping a batch from `_sample_queue`, add a debug statement such as `print(f"Drained {len(batch)} samples")`.
- Start a live stream (real Pi or loopback) so samples arrive.

**Observe:** The terminal prints how many samples were processed each timer tick, which helps you correlate ingest frequency with the live plotâ€™s smoothness.

---

## Milestone 6 â€“ Sampling rate and decimation

**Goal:** See how sampling and â€œstream every Nth sampleâ€ are configured.

**Read:**
1. `src/sensepi/gui/widgets/acquisition_settings.py` â€“ how the GUI lets you pick device and refresh rates and displays the effective stream rate.
2. `src/sensepi/config/sampling.py` â€“ the `SamplingConfig`/`RecordingMode` helpers that compute decimation (`stream_decimate`) and `stream_rate_hz`.

**Task:**
- Locate the `RECORDING_MODES` dictionary in `sampling.py`. For the `"high_fidelity"` mode, change `target_stream_hz` from `25.0` to match the device rate (e.g. `200.0`). This effectively changes the default â€œstream every Nth sampleâ€ ratio from 8 down to 1.
- Open the GUI, visit the **Live Signals** tab, and inspect the â€œGUI stream [Hz]â€ label in the Sampling box.

**Observe:** The label should show a much higher stream rate (equal to the device rate), confirming that the decimation setting â€“ and thus the â€œstream every Nth sampleâ€ value â€“ was updated.

---

## Milestone 7 â€“ Performance HUD and refresh modes

**Goal:** Explore performance tuning and the HUD overlay.

**Read:**
1. `src/sensepi/gui/main_window.py` â€“ the `QAction` named `_act_show_perf_hud` in the View menu that toggles the overlay.
2. `src/sensepi/gui/tabs/tab_signals.py` â€“ the `_perf_hud_label` and `set_perf_hud_visible` logic that draws live FPS/timing data.

**Task:**
- In `MainWindow.__init__`, change `_act_show_perf_hud.setChecked(False)` to `True` so the HUD starts enabled.
- Run the GUI, start streaming data, and keep the Live Signals tab visible.

**Observe:** The translucent performance HUD should be visible immediately (no menu click required) and update while samples stream.

---

## Milestone 8 â€“ Recording and offline logs

**Goal:** Understand how data is recorded and replayed.

**Read:**
1. `src/sensepi/dataio/csv_writer.py` â€“ how recordings are written to disk and where metadata lives.
2. `src/sensepi/gui/tabs/tab_offline.py` â€“ how the Recordings tab syncs logs over SSH and plots them via `plotter.build_plot_for_file`.

**Task:**
- Run a short recording with recording enabled (Device tab â†’ check â€œRecordingâ€ â†’ Start/Stop).
- Use the **Recordings** tab to click **Sync logs from Pi** and open the newest file.
- (Optional) Add `print(f"Loaded: {path}")` inside `OfflineTab.load_file` to see exactly which path is rendered.

**Observe:** A Matplotlib canvas should appear under the file list showing the recorded data, and (if you added the print) the terminal will log the path you opened.

---

## Milestone 9 â€“ Big-picture architecture

**Goal:** Summarise the role of each top-level package.

**Read:**
1. `src/sensepi/analysis/__init__.py`
2. `src/sensepi/config/__init__.py`
3. `src/sensepi/core/__init__.py`
4. `src/sensepi/data/__init__.py`
5. `src/sensepi/dataio/__init__.py`
6. `src/sensepi/gui/__init__.py`
7. `src/sensepi/remote/__init__.py`
8. `src/sensepi/sensors/__init__.py`
9. `src/sensepi/tools/__init__.py`

**Task:**
- For each package, jot down a one-sentence summary in your own notes describing what code lives there.
- (Optional) Sketch a simple diagram like â€œGUI â†’ Remote â†’ Core â†’ DataIO â†’ Analysisâ€ to reinforce how data flows through the system.

**Observe:** You should now be able to explain to another student where to look for GUI code, SSH/streaming helpers, buffering, and offline analysis utilities.

------------------------------ END OF FILE ------------------------------

============================= envelope_plot.py
# File: envelope_plot.py (ext: .py
# Dir : 
# Size: 4652 bytes
# Time: 30/11/2025 16:51
============================= envelope_plot.py
"""
Helpers for managing mean-line, envelope, and spike markers on Matplotlib axes.

These utilities are designed for live plots that refresh ~20â€“60 Hz while the
incoming sensor samples are decimated to a smaller set of points.  The helpers
keep Matplotlib artists alive between frames to avoid unnecessary allocations.
"""

from __future__ import annotations

from typing import Any, Optional, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
from matplotlib import colors as mcolors
from matplotlib.collections import PathCollection, PolyCollection
from matplotlib.lines import Line2D


def _as_array(values: Sequence[float] | np.ndarray) -> np.ndarray:
    """Return ``values`` as a NumPy array of ``float64``."""
    if isinstance(values, np.ndarray):
        return values.astype(float, copy=False)
    return np.asarray(values, dtype=float)


def _build_envelope_vertices(t: np.ndarray, y_min: np.ndarray, y_max: np.ndarray) -> np.ndarray:
    """Construct polygon vertices for a standard min/max envelope."""
    upper = np.column_stack((t, y_max))
    lower = np.column_stack((t[::-1], y_min[::-1]))
    return np.vstack((upper, lower))


def init_envelope_plot(
    ax: plt.Axes,
    color: str = "C0",
    alpha: float = 0.2,
    line_kwargs: Optional[dict[str, Any]] = None,
) -> Tuple[Line2D, PolyCollection]:
    """
    Initialize a line/envelope pair on ``ax``.

    Parameters
    ----------
    ax:
        Target axes for the artists.
    color:
        Base color applied to the mean line and envelope fill.
    alpha:
        Opacity for the envelope band.
    line_kwargs:
        Optional keyword arguments forwarded to ``Axes.plot`` for the mean line.
    """
    lw = 1.0
    if line_kwargs:
        lw = float(line_kwargs.get("lw", lw))
    line_opts = {"color": color, "lw": lw}
    if line_kwargs:
        line_opts.update(line_kwargs)

    (line,) = ax.plot([], [], **line_opts)

    face_color = mcolors.to_rgba(line.get_color(), alpha=alpha)
    envelope = PolyCollection(
        verts=[],
        facecolors=[face_color],
        edgecolors="none",
        antialiased=False,
    )
    ax.add_collection(envelope)
    return line, envelope


def update_envelope_plot(
    line: Line2D,
    envelope_coll: PolyCollection,
    t_dec: Sequence[float] | np.ndarray,
    y_mean: Sequence[float] | np.ndarray,
    y_min: Optional[Sequence[float] | np.ndarray],
    y_max: Optional[Sequence[float] | np.ndarray],
) -> PolyCollection:
    """
    Update the artists with newly decimated data.

    Returns the (possibly new) PolyCollection for the envelope so the caller can
    keep the latest reference.
    """
    t_arr = _as_array(t_dec)
    y_mean_arr = _as_array(y_mean)
    line.set_data(t_arr, y_mean_arr)

    if y_min is None or y_max is None or t_arr.size == 0:
        envelope_coll.set_verts([])
        return envelope_coll

    y_min_arr = _as_array(y_min)
    y_max_arr = _as_array(y_max)
    if t_arr.shape != y_min_arr.shape or t_arr.shape != y_max_arr.shape:
        raise ValueError("t_dec, y_min, and y_max must have matching lengths")

    verts = _build_envelope_vertices(t_arr, y_min_arr, y_max_arr)
    envelope_coll.set_verts([verts])
    return envelope_coll


def init_spike_markers(
    ax: plt.Axes,
    color: str = "red",
    marker: str = "x",
    size: float = 30.0,
    zorder: Optional[float] = None,
) -> PathCollection:
    """Create an empty scatter artist used to highlight spikes."""
    scatter = ax.scatter(
        [],
        [],
        s=size,
        color=color,
        marker=marker,
        zorder=zorder,
    )
    return scatter


def update_spike_markers(
    scatter: PathCollection,
    t_dec: Sequence[float] | np.ndarray,
    y_mean: Sequence[float] | np.ndarray,
    y_max: Optional[Sequence[float] | np.ndarray],
    spike_threshold: float,
) -> None:
    """
    Update spike markers based on an absolute threshold above the mean.

    ``spike_threshold`` is compared against ``(y_max - y_mean)`` per interval.
    """
    if y_max is None:
        scatter.set_offsets(np.empty((0, 2)))
        return

    t_arr = _as_array(t_dec)
    y_mean_arr = _as_array(y_mean)
    y_max_arr = _as_array(y_max)
    if t_arr.shape != y_mean_arr.shape or t_arr.shape != y_max_arr.shape:
        raise ValueError("t_dec, y_mean, and y_max must have matching lengths")

    mask = (y_max_arr - y_mean_arr) > float(spike_threshold)
    if not np.any(mask):
        scatter.set_offsets(np.empty((0, 2)))
        return

    t_spikes = t_arr[mask]
    y_spikes = y_max_arr[mask]
    points = np.column_stack((t_spikes, y_spikes))
    scatter.set_offsets(points)

------------------------------ END OF FILE ------------------------------

============================= live_plot.py
# File: live_plot.py (ext: .py
# Dir : 
# Size: 10030 bytes
# Time: 30/11/2025 16:51
============================= live_plot.py
"""Matplotlib helper that displays decimated samples with optional envelopes."""

from __future__ import annotations

from collections import deque
from dataclasses import dataclass, field
from typing import Any, Callable, Deque, Iterable, Optional, Protocol, Sequence, Tuple, Union, TYPE_CHECKING

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.animation import FuncAnimation

from envelope_plot import (
    init_envelope_plot,
    init_spike_markers,
    update_envelope_plot,
    update_spike_markers,
)

if TYPE_CHECKING:  # pragma: no cover - typing only
    from sensepi.config import SensePiConfig


class PlotChunkLike(Protocol):  # pragma: no cover - structural typing helper
    """Subset of the PlotUpdate interface used by LivePlot."""

    timestamps: np.ndarray
    mean: np.ndarray
    y_min: Optional[np.ndarray]
    y_max: Optional[np.ndarray]
    spike_mask: Optional[np.ndarray]


PlotTuple = Tuple[np.ndarray, np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]
FetchResult = Union[PlotChunkLike, PlotTuple, Sequence[PlotTuple], Sequence[PlotChunkLike], None]


def _as_1d_array(values: Sequence[float] | np.ndarray) -> np.ndarray:
    """Return ``values`` as a contiguous 1D float array."""
    arr = np.asarray(values, dtype=np.float64).reshape(-1)
    return arr


@dataclass
class LivePlot:
    """
    Manage a scrolling Matplotlib display for decimated sensor data.

    ``update_plot`` accepts either a :class:`PlotUpdate` instance or a tuple of
    ``(t_dec, y_mean, y_min, y_max)``.  The class stores the last
    ``window_seconds`` worth of data and reuses Matplotlib artists so that draw
    calls remain cheap enough for 20â€“60 Hz refresh rates on a Raspberry Pi.
    """

    window_seconds: float = 10.0
    spike_threshold: float = 0.5
    autoscale_margin: float = 0.05

    fig: plt.Figure = field(init=False)
    ax: plt.Axes = field(init=False)
    line = None
    envelope_coll = None
    spike_scatter = None

    _t: Deque[float] = field(init=False, default_factory=deque)
    _y_mean: Deque[float] = field(init=False, default_factory=deque)
    _y_min: Deque[float] = field(init=False, default_factory=deque)
    _y_max: Deque[float] = field(init=False, default_factory=deque)
    _animation: Optional[FuncAnimation] = field(init=False, default=None, repr=False)
    _envelope_enabled: bool = field(init=False, default=False, repr=False)

    def __post_init__(self) -> None:
        self.window_seconds = max(0.1, float(self.window_seconds))
        self.fig, self.ax = plt.subplots()
        self.line, self.envelope_coll = init_envelope_plot(self.ax, color="C0", alpha=0.2)
        self.spike_scatter = init_spike_markers(self.ax, color="red", marker="x", size=30.0)

        self.ax.set_xlabel("Time [s]")
        self.ax.set_ylabel("Sensor value")
        self.ax.grid(True)

    # ---------------------------------------------------------------- factory
    @classmethod
    def from_config(
        cls,
        cfg: "SensePiConfig",
        **overrides: Any,
    ) -> "LivePlot":
        """Build a :class:`LivePlot` using the relevant values from ``cfg``.

        ``overrides`` can supply extra keyword arguments (e.g. Matplotlib figure)
        that are forwarded to :class:`LivePlot`'s constructor.
        """

        window_seconds = float(getattr(cfg, "plot_window_seconds", 10.0))
        spike_threshold = float(getattr(cfg, "spike_threshold", 0.5))
        params = dict(overrides)
        params.setdefault("window_seconds", window_seconds)
        params.setdefault("spike_threshold", spike_threshold)
        return cls(**params)

    # ------------------------------------------------------------------ buffers
    def _trim_window(self) -> None:
        if not self._t:
            return
        t_latest = self._t[-1]
        t_min = t_latest - self.window_seconds
        while self._t and self._t[0] < t_min:
            self._t.popleft()
            self._y_mean.popleft()
            self._y_min.popleft()
            self._y_max.popleft()

    def _extend_deque(self, dest: Deque[float], values: Iterable[float]) -> None:
        dest.extend(float(v) for v in values)

    def add_data(
        self,
        t_dec: np.ndarray,
        y_mean: np.ndarray,
        y_min: Optional[np.ndarray],
        y_max: Optional[np.ndarray],
    ) -> None:
        """Append a decimated block into the rolling buffer."""
        if t_dec.size == 0:
            return
        has_envelope = y_min is not None and y_max is not None
        y_min_vals = y_min if has_envelope else y_mean
        y_max_vals = y_max if has_envelope else y_mean

        self._envelope_enabled = bool(has_envelope)
        self._extend_deque(self._t, t_dec)
        self._extend_deque(self._y_mean, y_mean)
        self._extend_deque(self._y_min, y_min_vals)
        self._extend_deque(self._y_max, y_max_vals)
        self._trim_window()

    # ---------------------------------------------------------------- redraw
    def redraw(self) -> None:
        """Update artists to reflect the current buffers."""
        if not self._t:
            return

        t_arr = np.fromiter(self._t, dtype=float)
        y_mean_arr = np.fromiter(self._y_mean, dtype=float)
        y_min_arr = np.fromiter(self._y_min, dtype=float)
        y_max_arr = np.fromiter(self._y_max, dtype=float)

        envelope_min = y_min_arr if self._envelope_enabled else None
        envelope_max = y_max_arr if self._envelope_enabled else None

        new_coll = update_envelope_plot(
            self.line,
            self.envelope_coll,
            t_arr,
            y_mean_arr,
            envelope_min,
            envelope_max,
        )
        if new_coll is not None:
            self.envelope_coll = new_coll

        update_spike_markers(
            self.spike_scatter,
            t_arr,
            y_mean_arr,
            envelope_max,
            self.spike_threshold,
        )

        t_end = t_arr[-1]
        t_start = max(t_end - self.window_seconds, t_arr[0])
        self.ax.set_xlim(t_start, t_end)

        y_low = float(np.nanmin([y_min_arr.min(), y_mean_arr.min()]))
        y_high = float(np.nanmax([y_max_arr.max(), y_mean_arr.max()]))
        if not np.isfinite(y_low) or not np.isfinite(y_high):
            y_low, y_high = -1.0, 1.0
        if y_high <= y_low:
            pad = max(1e-3, abs(y_high) * 0.05 + self.autoscale_margin)
            y_low -= pad
            y_high += pad
        else:
            pad = (y_high - y_low) * float(self.autoscale_margin)
            y_low -= pad
            y_high += pad
        self.ax.set_ylim(y_low, y_high)

        self.fig.canvas.draw_idle()

    # ---------------------------------------------------------------- control
    def update_plot(self, data_chunk: PlotChunkLike | PlotTuple | None) -> None:
        """
        Primary entry point used by animation/timer callbacks.

        ``data_chunk`` can be ``None`` (no update), a :class:`PlotUpdate` instance,
        or a tuple ``(t_dec, y_mean, y_min, y_max)``.  Supplying a two-element tuple
        ``(t_dec, y_mean)`` is also supported.
        """
        if data_chunk is None:
            return

        t_dec, y_mean, y_min, y_max = self._parse_chunk(data_chunk)
        if t_dec.size == 0:
            return
        self.add_data(t_dec, y_mean, y_min, y_max)
        self.redraw()

    def _parse_chunk(self, chunk: PlotChunkLike | PlotTuple) -> PlotTuple:
        if hasattr(chunk, "timestamps"):
            t_dec = _as_1d_array(chunk.timestamps)  # type: ignore[attr-defined]
            y_mean = _as_1d_array(chunk.mean)  # type: ignore[attr-defined]
            y_min = None
            y_max = None
            if getattr(chunk, "y_min", None) is not None:
                y_min = _as_1d_array(chunk.y_min)  # type: ignore[attr-defined]
            if getattr(chunk, "y_max", None) is not None:
                y_max = _as_1d_array(chunk.y_max)  # type: ignore[attr-defined]
            return t_dec, y_mean, y_min, y_max

        if not isinstance(chunk, tuple):
            raise TypeError("Unsupported data chunk type passed to LivePlot.")
        if len(chunk) == 2:
            t_dec, y_mean = chunk
            return (
                _as_1d_array(t_dec),
                _as_1d_array(y_mean),
                None,
                None,
            )
        if len(chunk) >= 4:
            t_dec, y_mean, y_min, y_max = chunk[:4]
            return (
                _as_1d_array(t_dec),
                _as_1d_array(y_mean),
                None if y_min is None else _as_1d_array(y_min),
                None if y_max is None else _as_1d_array(y_max),
            )
        raise ValueError("Expected tuple with 2 or 4 elements for plot data.")

    def start_animation(
        self,
        fetch_data: Callable[[], FetchResult],
        interval_ms: int = 40,
    ) -> FuncAnimation:
        """
        Drive ``update_plot`` via a Matplotlib ``FuncAnimation`` timer.

        ``fetch_data`` should return one of the accepted chunk formats or a
        sequence (e.g. list) of chunks.  Returning ``None`` results in a no-op,
        which makes it easy to poll ``queue.Queue`` objects with ``get_nowait``.
        """

        def _tick(_frame: int) -> None:
            result = fetch_data()
            if result is None:
                return
            if isinstance(result, Sequence) and result and not isinstance(result, tuple):
                for item in result:
                    self.update_plot(item)
            else:
                self.update_plot(result)  # type: ignore[arg-type]

        self._animation = FuncAnimation(
            self.fig,
            _tick,
            interval=max(1, int(interval_ms)),
            blit=False,
        )
        return self._animation

    def stop_animation(self) -> None:
        """Cancel the animation timer if one was created."""
        if self._animation is not None:
            self._animation.event_source.stop()
            self._animation = None

------------------------------ END OF FILE ------------------------------

============================= logs\.gitkeep
# File: .gitkeep (ext: .gitkeep
# Dir : logs\
# Size: 0 bytes
# Time: 30/11/2025 16:51
============================= logs\.gitkeep

------------------------------ END OF FILE ------------------------------

============================= main.py
# File: main.py (ext: .py
# Dir : 
# Size: 901 bytes
# Time: 30/11/2025 16:51
============================= main.py
from __future__ import annotations

import os
import sys
from pathlib import Path

# Make sure the 'src' directory is on sys.path so 'sensepi' can be imported
REPO_ROOT = Path(__file__).resolve().parent
SRC_DIR = REPO_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from sensepi.gui.application import main as run_gui_main


def main() -> None:
    run_gui_main(sys.argv)


if __name__ == "__main__":
    if os.getenv("SENSEPI_PROFILE", ""):
        import cProfile
        import io
        import pstats

        profiler = cProfile.Profile()
        profiler.enable()
        try:
            main()
        finally:
            profiler.disable()
            buffer = io.StringIO()
            stats = pstats.Stats(profiler, stream=buffer).sort_stats("cumulative")
            stats.print_stats(50)
            print(buffer.getvalue())
    else:
        main()

------------------------------ END OF FILE ------------------------------

============================= pi_recorder.py
# File: pi_recorder.py (ext: .py
# Dir : 
# Size: 297 bytes
# Time: 30/11/2025 16:51
============================= pi_recorder.py
from __future__ import annotations

"""
Legacy shim module.

This file exists for backward compatibility only.
New code should import from :mod:`sensepi.remote` instead::

    from sensepi.remote import PiRecorder
"""

from sensepi.remote import PiRecorder

__all__ = ["PiRecorder"]

------------------------------ END OF FILE ------------------------------

============================= profile_benchmark.py
# File: profile_benchmark.py (ext: .py
# Dir : 
# Size: 3882 bytes
# Time: 30/11/2025 16:51
============================= profile_benchmark.py
"""Helper script to profile the SensePi benchmark mode with cProfile."""

from __future__ import annotations

import argparse
import cProfile
import pstats
import sys
from pathlib import Path
from typing import List


def _ensure_src_on_path() -> None:
    repo_root = Path(__file__).resolve().parent
    src_dir = repo_root / "src"
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Profile SensePi benchmark mode")
    parser.add_argument(
        "--prof-output",
        type=str,
        default="benchmark_profile.prof",
        help="cProfile output file (default: benchmark_profile.prof)",
    )
    parser.add_argument(
        "--bench-rate",
        type=float,
        default=200.0,
        help="Synthetic input rate in Hz (default: 200)",
    )
    parser.add_argument(
        "--bench-duration",
        type=float,
        default=30.0,
        help="Benchmark duration in seconds (default: 30)",
    )
    parser.add_argument(
        "--bench-refresh",
        type=float,
        default=20.0,
        help="Plot refresh rate in Hz (default: 20)",
    )
    parser.add_argument(
        "--bench-channels",
        type=int,
        default=18,
        help="Number of charts to render (9 or 18)",
    )
    parser.add_argument(
        "--bench-sensors",
        type=int,
        default=3,
        help="Synthetic sensor count (default: 3)",
    )
    parser.add_argument(
        "--bench-log-interval",
        type=float,
        default=1.0,
        help="Seconds between benchmark log entries (default: 1)",
    )
    parser.add_argument(
        "--bench-csv",
        type=str,
        default="benchmark_results.csv",
        help="CSV file for benchmark metrics (default: benchmark_results.csv)",
    )
    parser.add_argument(
        "--bench-no-csv",
        action="store_true",
        help="Skip writing benchmark metrics to CSV",
    )
    parser.add_argument(
        "--bench-keep-open",
        action="store_true",
        help="Keep GUI open after benchmark completes",
    )
    parser.add_argument(
        "--print-stats",
        action="store_true",
        help="Print the top cumulative functions after profiling",
    )
    return parser.parse_args()


def _build_gui_argv(args: argparse.Namespace) -> List[str]:
    argv = ["profile_benchmark.py", "--benchmark"]
    argv.extend(["--bench-rate", str(args.bench_rate)])
    argv.extend(["--bench-duration", str(args.bench_duration)])
    argv.extend(["--bench-refresh", str(args.bench_refresh)])
    argv.extend(["--bench-channels", str(args.bench_channels)])
    argv.extend(["--bench-log-interval", str(args.bench_log_interval)])
    argv.extend(["--bench-sensors", str(args.bench_sensors)])
    if args.bench_no_csv:
        argv.append("--bench-no-csv")
    else:
        argv.extend(["--bench-csv", args.bench_csv])
    if args.bench_keep_open:
        argv.append("--bench-keep-open")
    return argv


def _run_gui(argv: List[str]) -> None:
    from sensepi.gui import application

    try:
        application.main(argv)
    except SystemExit as exc:  # Allow the GUI to request exit without killing profiling
        code = exc.code
        if code not in (0, None):
            raise


def main() -> None:
    _ensure_src_on_path()
    args = _parse_args()
    gui_argv = _build_gui_argv(args)
    prof_path = Path(args.prof_output).expanduser().resolve()
    profiler = cProfile.Profile()
    profiler.enable()
    try:
        _run_gui(gui_argv)
    finally:
        profiler.disable()
    profiler.dump_stats(str(prof_path))
    print(f"[profile] cProfile stats written to {prof_path}")
    if args.print_stats:
        stats = pstats.Stats(profiler)
        stats.sort_stats("cumulative").print_stats(20)


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= pyproject.toml
# File: pyproject.toml (ext: .toml
# Dir : 
# Size: 717 bytes
# Time: 30/11/2025 16:51
============================= pyproject.toml
[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "sensepi"
version = "0.1.0"
description = "PySide6 desktop app and Raspberry Pi helpers for SensePi data logging"
authors = [
  {name = "SensePi", email = "maintainers@example.com"}
]
readme = "README.md"
requires-python = ">=3.9"
dependencies = [
  "PySide6>=6.6",
  "numpy>=1.25",
  "scipy>=1.11",
  "matplotlib>=3.8",
  "paramiko>=3.3",
  "PyYAML>=6.0",
  "psutil>=5.9",
  "pyqtgraph>=0.13",
]

[project.scripts]
sensepi-gui = "sensepi.gui.application:main"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
exclude = ["archive*"]

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\debug_log_sample_rate.py
# File: debug_log_sample_rate.py (ext: .py
# Dir : raspberrypi_scripts\
# Size: 8330 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\debug_log_sample_rate.py
#!/usr/bin/env python
# raspberrypi_scripts/debug_log_sample_rate.py
"""
Estimate recorded sampling rate from MPU6050 log files on the Raspberry Pi.

This script is intended to be run on the Pi inside the `raspberrypi_scripts/`
directory. It inspects CSV or JSONL log files produced by
`mpu6050_multi_logger.py` and estimates the sampling rate from timestamps.

Each MPU6050 sensor can provide up to six numeric channels:

    ax, ay, az, gx, gy, gz

Many deployments only use a subset (often ax, ay, gz). This script does NOT
look at channel values; it only cares about timestamps and sensor_id in order
to validate that the *recorded* rate on the Pi matches the configured
sample_rate_hz, independent of GUI streaming and --stream-every.
"""

from __future__ import annotations

import argparse
import csv
import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple


def _load_rows_csv(path: Path) -> List[Dict[str, Any]]:
    """Load all rows from a CSV log into a list of dicts."""
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return [dict(row) for row in reader]


def _load_rows_jsonl(path: Path) -> List[Dict[str, Any]]:
    """Load all rows from a JSONL log into a list of dicts."""
    rows: List[Dict[str, Any]] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            text = line.strip()
            if not text:
                continue
            try:
                obj = json.loads(text)
            except json.JSONDecodeError:
                continue
            if isinstance(obj, dict):
                rows.append(obj)
    return rows


def _load_meta(path: Path) -> Optional[Dict[str, Any]]:
    """
    Load the .meta.json sidecar if present.

    For a log file /path/to/log.csv, the meta is expected at
    /path/to/log.csv.meta.json as written by mpu6050_multi_logger.py.
    """
    meta_path = path.with_suffix(path.suffix + ".meta.json")
    if not meta_path.exists():
        return None
    try:
        with meta_path.open("r", encoding="utf-8") as f:
            meta = json.load(f)
        if isinstance(meta, dict):
            return meta
    except Exception:
        return None
    return None


def _extract_times(
    rows: Iterable[Dict[str, Any]]
) -> Tuple[List[float], List[int]]:
    """
    Extract timestamps (seconds) and sensor_ids from rows.

    Preference order for time fields:
      1) t_s (float seconds or int nanoseconds, depending on logger)
      2) t_rel_s (float seconds, newer logs)
      3) timestamp_ns (int nanoseconds)

    We normalise everything to float seconds.
    """
    times: List[float] = []
    sensor_ids: List[int] = []

    for row in rows:
        t: Optional[float] = None

        if "t_s" in row and row["t_s"] not in ("", None):
            try:
                t_val = float(row["t_s"])
                # Heuristic: if it looks like nanoseconds, scale down
                if abs(t_val) > 1e12:
                    t = t_val * 1e-9
                else:
                    t = t_val
            except (TypeError, ValueError):
                t = None
        elif "t_rel_s" in row and row["t_rel_s"] not in ("", None):
            try:
                t = float(row["t_rel_s"])
            except (TypeError, ValueError):
                t = None
        elif "timestamp_ns" in row and row["timestamp_ns"] not in ("", None):
            try:
                t_ns = float(row["timestamp_ns"])
                t = t_ns * 1e-9
            except (TypeError, ValueError):
                t = None

        if t is None:
            continue

        times.append(t)

        sid_val = row.get("sensor_id")
        try:
            if sid_val is not None and sid_val != "":
                sensor_ids.append(int(sid_val))
        except (TypeError, ValueError):
            # Ignore unparsable sensor_id; report as unknown
            pass

    return times, sensor_ids


def _summarize_file(path: Path, explicit_sensor_id: Optional[int] = None) -> None:
    """Compute and print a sampling-rate summary for a single log file."""
    suffix = path.suffix.lower()
    if suffix.endswith(".csv"):
        rows = _load_rows_csv(path)
    elif suffix.endswith(".jsonl"):
        rows = _load_rows_jsonl(path)
    else:
        print(f"\n=== Sample rate check ===")
        print(f"File: {path}")
        print(f"  WARNING: unsupported extension {path.suffix!r}; skipping.")
        return

    print("\n=== Sample rate check ===")
    print(f"File: {path}")

    if not rows:
        print("  WARNING: file is empty; cannot estimate rate.")
        return

    times, sensor_ids = _extract_times(rows)
    if len(times) < 2:
        print(
            f"  WARNING: only {len(times)} timestamped samples; "
            "cannot estimate rate."
        )
        return

    t_first = times[0]
    t_last = times[-1]
    t_span = t_last - t_first
    n_samples = len(times)

    if t_span <= 0:
        print(
            f"  WARNING: non-positive time span ({t_span:.6f} s); "
            "cannot estimate rate."
        )
        return

    rate_est = n_samples / t_span

    # Determine sensor_id info
    sid_text = "(unknown)"
    if explicit_sensor_id is not None:
        sid_text = str(explicit_sensor_id)
    elif sensor_ids:
        unique_ids = sorted(set(sensor_ids))
        if len(unique_ids) == 1:
            sid_text = str(unique_ids[0])
        else:
            sid_text = f"mixed {unique_ids}"

    print(f"  sensor_id: {sid_text}")
    print(f"  samples: {n_samples}")
    print(f"  time_span: {t_span:.3f} s")
    print(f"  estimated_rate: {rate_est:.2f} Hz")

    meta = _load_meta(path)
    if meta:
        dev_rate = meta.get("device_rate_hz")
        requested = meta.get("requested_rate_hz", meta.get("sample_rate_hz"))
        stream_every = meta.get("stream_every")

        if dev_rate is not None:
            try:
                dev_rate_f = float(dev_rate)
                delta = rate_est - dev_rate_f
                pct = (delta / dev_rate_f * 100.0) if dev_rate_f != 0 else 0.0
                print(
                    f"  meta.device_rate_hz: {dev_rate_f:.2f} Hz "
                    f"(delta: {delta:+.2f} Hz, {pct:+.1f} %)")
            except (TypeError, ValueError):
                print(f"  meta.device_rate_hz: {dev_rate!r} (unparsable)")

        if requested is not None:
            try:
                req_f = float(requested)
                print(f"  meta.requested_rate_hz: {req_f:.2f} Hz")
            except (TypeError, ValueError):
                print(f"  meta.requested_rate_hz: {requested!r}")

        if stream_every is not None:
            print(f"  meta.stream_every: {stream_every}")
    else:
        print("  (no .meta.json sidecar found)")


def _iter_log_files(root: Path, pattern: str) -> Iterable[Path]:
    """Yield all files matching pattern under root, sorted by name."""
    for path in sorted(root.glob(pattern)):
        if path.is_file():
            yield path


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Estimate recorded sampling rate from MPU6050 log files.\n"
            "Intended to be run on the Raspberry Pi in raspberrypi_scripts/."
        )
    )
    parser.add_argument(
        "path",
        help="Path to a CSV/JSONL log file or to a directory of log files.",
    )
    parser.add_argument(
        "--glob",
        default="*.csv",
        help="Glob pattern when PATH is a directory (default: '*.csv').",
    )
    parser.add_argument(
        "--sensor-id",
        type=int,
        default=None,
        help=(
            "Optional sensor_id to associate with logs that do not contain a "
            "sensor_id column. If provided, it overrides any inferred ID."
        ),
    )
    args = parser.parse_args()

    target = Path(args.path)
    if target.is_dir():
        any_files = False
        for log_path in _iter_log_files(target, args.glob):
            any_files = True
            _summarize_file(log_path, explicit_sensor_id=args.sensor_id)
        if not any_files:
            print(f"No files matched {args.glob!r} in {target}")
    else:
        _summarize_file(target, explicit_sensor_id=args.sensor_id)


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\install_pi_deps.sh
# File: install_pi_deps.sh (ext: .sh
# Dir : raspberrypi_scripts\
# Size: 360 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\install_pi_deps.sh
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REQUIREMENTS_ROOT="$(dirname "$SCRIPT_DIR")"

sudo apt-get update
sudo apt-get install -y python3 python3-pip python3-venv

if [ -f "$REQUIREMENTS_ROOT/requirements-pi.txt" ]; then
  pip3 install --user -r "$REQUIREMENTS_ROOT/requirements-pi.txt"
fi

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\mpu6050_multi_logger.py
# File: mpu6050_multi_logger.py (ext: .py
# Dir : raspberrypi_scripts\
# Size: 41160 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\mpu6050_multi_logger.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
mpu6050_multi_logger.py
=======================
Raspberry Pi Zero â€” Multiâ€‘MPU6050 local logger (no MQTT)

Changes in this version
-----------------------
- Less intrusive file flushing:
  * Periodic flush thresholds increased (defaults: 2000 rows, 2.0 s).
  * os.fsync() is optional during periodic flushes (default: disabled),
    but still performed once at shutdown.
- New CLI knobs: --flush-every, --flush-seconds, --fsync-each-flush

Features
--------
- Supports up to **three** MPUâ€‘6050 sensors (default mapping below).
- Select which sensors to enable: --sensors 1,2,3
- Select which channels to record: --channels acc|gyro|both|default
  * default = AX, AY, and GZ (XY acceleration + yaw-rate around Z)
- Control sampling rate (Hz): --rate
- Local CSV or JSONL logging (one file per sensor) + metadata JSON.
- Driftâ€‘corrected sampling loop using time.monotonic_ns().
- Perâ€‘sensor writer thread for lowâ€‘latency I/O.
- Resilient to IÂ²C hiccups; keeps other sensors running.
- Device scan: --list prints addresses seen on bus 0 and 1.

Default mapping
---------------
Sensor 1 â†’ bus 1, address 0x68
Sensor 2 â†’ bus 1, address 0x69
Sensor 3 â†’ bus 0, address 0x68
(Override with --map "1:1-0x68,2:1-0x69,3:0-0x68")

Scaling (matches prior reference)
---------------------------------
Accel raw â†’ g = raw / 16384.0, then * 9.80665  â†’ m/sÂ² (Â±2 g range)
Gyro  raw â†’ dps = raw / 131.0                 â†’ deg/s (Â±250 Â°/s range)

Install (on Raspberry Pi OS)
----------------------------
sudo apt-get update
sudo apt-get install -y python3-pip python3-smbus i2c-tools
pip3 install smbus2 numpy
sudo raspi-config nonint do_i2c 0   # ensure I2C enabled

Examples
--------
# List devices on bus 0 and 1
python3 mpu6050_multi_logger.py --list

# Two sensors, 100 Hz, both acc+gyro, log 10 s
python3 mpu6050_multi_logger.py --rate 100 --sensors 1,2 --channels both --duration 10 --out ./logs

# Gyroâ€‘only from sensor 3 at 200 Hz until Ctrlâ€‘C
python3 mpu6050_multi_logger.py --rate 200 --sensors 3 --channels gyro --out ./logs

# "default" selection â€” AX, AY and GZ
python3 mpu6050_multi_logger.py --rate 100 --channels default

Configuration via YAML
----------------------
This logger can read defaults from a small YAML file on the Pi. Use
``--config /path/to/pi_config.yaml`` explicitly, or omit ``--config``
and a ``pi_config.yaml`` that lives next to this script will be used if
present.

The merge strategy is:

1. Read defaults from the ``mpu6050`` section.
2. Apply explicit command-line options on top (CLI overrides config).
3. For boolean flags such as ``--temp``, ``--no-record`` and
   ``--stream-stdout``, the config controls the default state and the
   CLI can only enable additional behaviour.
"""
from __future__ import annotations

import argparse
import csv
import json
import os
import queue
import signal
import socket
import sys
import threading
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from pi_logger_common import load_config
from sensepi.config.log_paths import (
    LOG_SUBDIR_MPU,
    build_log_file_paths,
    build_pi_session_dir,
    slugify_session_name,
)

try:
    from smbus2 import SMBus
except Exception as e:
    print("ERROR: smbus2 is required. Install with: pip3 install smbus2", file=sys.stderr)
    raise

# ---------------------------
# MPU6050 register constants
# ---------------------------
WHO_AM_I       = 0x75
PWR_MGMT_1     = 0x6B
SMPLRT_DIV     = 0x19
CONFIG         = 0x1A
GYRO_CONFIG    = 0x1B
ACCEL_CONFIG   = 0x1C

ACCEL_XOUT_H   = 0x3B
ACCEL_YOUT_H   = 0x3D
ACCEL_ZOUT_H   = 0x3F
GYRO_XOUT_H    = 0x43
GYRO_YOUT_H    = 0x45
GYRO_ZOUT_H    = 0x47
TEMP_OUT_H     = 0x41  # (optional) on-die temperature

# Scale factors for Â±2g and Â±250 dps
ACC_SF = 16384.0           # LSB/g
GYR_SF = 131.0             # LSB/(deg/s)
G_TO_MS2 = 9.80665

# DLPF bandwidth mapping (datasheet)
# index: (gyro_bw_Hz, accel_bw_Hz)
DLPF_BW = {
    0: (260, 256),
    1: (184, 188),
    2: (94, 98),
    3: (44, 42),
    4: (21, 20),
    5: (10, 10),
    6: (5, 5),
}
DLPF_DEFAULT = 3

# With DLPF enabled, internal rate is 1 kHz â†’ SampleRate = 1000/(1+SMPLRT_DIV)
INTERNAL_RATE_HZ = 1000.0


@dataclass
class SensorMap:
    bus: int
    addr: int


class MPU6050:
    """Minimal MPU6050 driver using smbus2 (no DMP)."""
    def __init__(self, bus: SMBus, addr: int):
        self.bus = bus
        self.addr = addr

    def _write_u8(self, reg: int, val: int) -> None:
        self.bus.write_byte_data(self.addr, reg, val & 0xFF)

    def _read_u8(self, reg: int) -> int:
        return self.bus.read_byte_data(self.addr, reg)

    def _read_i16(self, reg_h: int) -> int:
        hi = self.bus.read_byte_data(self.addr, reg_h)
        lo = self.bus.read_byte_data(self.addr, reg_h + 1)
        v = (hi << 8) | lo
        if v & 0x8000:
            v = -((~v & 0xFFFF) + 1)
        return v

    def who_am_i(self) -> int:
        return self._read_u8(WHO_AM_I)

    def initialize(self, dlpf_cfg: int, fs_accel: int = 0, fs_gyro: int = 0, rate_hz: float = 100.0) -> Tuple[int, float]:
        """
        fs_accel: 0â†’Â±2g, 1â†’Â±4g, 2â†’Â±8g, 3â†’Â±16g
        fs_gyro : 0â†’Â±250dps, 1â†’Â±500, 2â†’Â±1000, 3â†’Â±2000
        Returns (smplrt_div, actual_rate_hz)
        """
        # Wake up and select PLL with Xâ€‘gyro as clock source (datasheet Â§5.5)
        self._write_u8(PWR_MGMT_1, 0x01)
        time.sleep(0.05)

        # DLPF
        self._write_u8(CONFIG, dlpf_cfg & 0x07)

        # Full-scale ranges
        self._write_u8(GYRO_CONFIG, (fs_gyro & 0x03) << 3)
        self._write_u8(ACCEL_CONFIG, (fs_accel & 0x03) << 3)

        # Sample rate divider (with DLPF: internal = 1 kHz)
        div = int(round(INTERNAL_RATE_HZ / max(1.0, rate_hz)) - 1)
        if div < 0: div = 0
        if div > 255: div = 255
        self._write_u8(SMPLRT_DIV, div)
        actual = INTERNAL_RATE_HZ / (1.0 + div)
        return div, actual

    def read_accel(self) -> Tuple[int, int, int]:
        ax = self._read_i16(ACCEL_XOUT_H)
        ay = self._read_i16(ACCEL_YOUT_H)
        az = self._read_i16(ACCEL_ZOUT_H)
        return ax, ay, az

    def read_gyro(self) -> Tuple[int, int, int]:
        gx = self._read_i16(GYRO_XOUT_H)
        gy = self._read_i16(GYRO_YOUT_H)
        gz = self._read_i16(GYRO_ZOUT_H)
        return gx, gy, gz

    def read_temp_c(self) -> float:
        # Optional: T(Â°C) = (TEMP_OUT / 340) + 36.53
        raw = self._read_i16(TEMP_OUT_H)
        return (raw / 340.0) + 36.53


class AsyncWriter:
    """Async CSV/JSONL writer with periodic flush.

    Changes:
    - Defaults to flush_every=2000 rows, flush_seconds=2.0 seconds
    - Periodic flush doesn't fsync by default (can be enabled)
    - Final fsync is always performed at stop()

    One AsyncWriter instance runs in a background thread per sensor. The main
    sampling loop only enqueues rows into a queue, so slow disks cannot stall
    time-critical sensor reads.
    """
    def __init__(self, filepath: Path, fmt: str, header: List[str],
                 flush_every: int = 2000, flush_seconds: float = 2.0,
                 fsync_each_flush: bool = False):
        self.filepath = filepath
        self.meta_path = filepath.with_suffix(filepath.suffix + ".meta.json")
        self.fmt = fmt
        self.header = header
        self.flush_every = flush_every
        self.flush_seconds = flush_seconds
        self.fsync_each_flush = fsync_each_flush
        self._q: "queue.Queue[Optional[dict]]" = queue.Queue()
        self._t = threading.Thread(target=self._run, daemon=True)
        self._fh = None
        self._writer = None
        self._lines_since_flush = 0
        self._last_flush = time.monotonic()
        self._stopping = False

    def start(self):
        self.filepath.parent.mkdir(parents=True, exist_ok=True)
        self._fh = open(self.filepath, "w", newline="")
        if self.fmt == "csv":
            self._writer = csv.DictWriter(self._fh, fieldnames=self.header)
            self._writer.writeheader()
        self._t.start()

    def write(self, row: dict):
        self._q.put(row)

    def _run(self):
        while True:
            item = self._q.get()
            if item is None:
                # None is a sentinel pushed by stop(): flush any pending rows
                # and exit the writer thread cleanly.
                break
            if self.fmt == "csv":
                self._writer.writerow(item)
            else:
                self._fh.write(json.dumps(item, separators=(",", ":")) + "\n")
            self._lines_since_flush += 1
            now = time.monotonic()
            if self._lines_since_flush >= self.flush_every or (now - self._last_flush) >= self.flush_seconds:
                self._fh.flush()
                if self.fsync_each_flush:
                    os.fsync(self._fh.fileno())
                self._lines_since_flush = 0
                self._last_flush = now
        # final flush
        self._fh.flush()
        os.fsync(self._fh.fileno())  # ensure data hits storage at stop
        self._fh.close()

    def stop(self):
        if not self._stopping:
            self._stopping = True
            self._q.put(None)
            self._t.join()

    def write_metadata(self, meta: dict):
        with open(self.meta_path, "w") as mfh:
            json.dump(meta, mfh, indent=2)


def parse_sensor_map(s: str) -> Dict[int, 'SensorMap']:
    """
    Parse --map like: "1:1-0x68,2:1-0x69,3:0-0x68"
    Returns dict {1: SensorMap(bus, addr), ...}
    """
    out: Dict[int, SensorMap] = {}
    if not s:
        return out
    for part in s.split(","):
        part = part.strip()
        if not part:
            continue
        try:
            left, right = part.split(":")
            sid = int(left)
            bus_str, addr_str = right.split("-")
            bus = int(bus_str)
            addr = int(addr_str, 16) if addr_str.lower().startswith("0x") else int(addr_str)
            if sid not in (1, 2, 3):
                print(f"[WARN] Ignoring invalid sensor id in --map: {sid}", file=sys.stderr)
                continue
            out[sid] = SensorMap(bus=bus, addr=addr)
        except Exception:
            print(f"[WARN] Could not parse mapping entry '{part}', expected like 2:1-0x69", file=sys.stderr)
    return out


def default_mapping() -> Dict[int, 'SensorMap']:
    return {
        1: SensorMap(bus=1, addr=0x68),
        2: SensorMap(bus=1, addr=0x69),
        3: SensorMap(bus=0, addr=0x68),
    }


def scan_buses() -> None:
    print("Scanning IÂ²C buses 0 and 1 for MPU6050 (0x68/0x69)...")
    for bus_id in (0, 1):
        try:
            with SMBus(bus_id) as bus:
                found = []
                for addr in (0x68, 0x69):
                    try:
                        who = bus.read_byte_data(addr, WHO_AM_I)
                        found.append((addr, who))
                    except Exception:
                        pass
                if found:
                    for addr, who in found:
                        print(f"  Bus {bus_id}: addr 0x{addr:02X} WHO_AM_I=0x{who:02X}")
                else:
                    print(f"  Bus {bus_id}: (no 0x68/0x69 detected)")
        except FileNotFoundError:
            print(f"  Bus {bus_id}: not available (skip)")


def monotonic_controller(rate_hz: float):
    """Yield target monotonic_ns timestamps for a fixed sampling rate.

    Each step adds a fixed period to the *previous target* time, which keeps the
    long-term rate stable and avoids drift from small sleep() errors.
    """
    period = int(1e9 / rate_hz)
    next_t = time.monotonic_ns()
    while True:
        next_t += period
        yield next_t


def main():
    ap = argparse.ArgumentParser(description="Multiâ€‘MPU6050 local logger (CSV/JSONL). No MQTT.")
    ap.add_argument("--list", action="store_true", help="List detected 0x68/0x69 on bus 0 and 1 and exit")
    ap.add_argument("--sample-rate-hz", type=int, default=None,
                    help="Preferred sampling rate in Hz (50-1000)")
    ap.add_argument("--rate", type=float, help="Sampling rate in Hz (e.g., 10, 20, 50, 100, 200)", required=False)
    ap.add_argument("--sensors", type=str, default="1,2,3", help="Commaâ€‘separated sensor ids to enable (subset of 1,2,3)")
    ap.add_argument("--map", type=str, default="", help="Override mapping like '1:1-0x68,2:1-0x69,3:0-0x68'")
    ap.add_argument(
        "--channels",
        type=str,
        choices=["acc", "gyro", "both", "default"],
        default="both",
        help=(
            "Which channels to record: 'acc', 'gyro', 'both', or 'default' "
            "(AX, AY, and GZ). Default is 'both' so that all six axes are "
            "available for streaming."
        ),
    )
    ap.add_argument("--duration", type=float, default=None, help="Duration in seconds (optional)")
    ap.add_argument("--samples", type=int, default=None, help="Number of samples to capture (optional)")
    ap.add_argument("--out", type=str, default="./logs", help="Output folder")
    ap.add_argument("--log-file", type=str, default=None,
                    help="Optional JSONL log file that receives every sample across sensors")
    ap.add_argument(
        "--config",
        type=str,
        default=None,
        help=(
            "Path to YAML config file with defaults "
            "(falls back to 'pi_config.yaml' next to this script if omitted)."
        ),
    )
    ap.add_argument("--format", type=str, choices=["csv", "jsonl"], default="csv", help="Output format")
    ap.add_argument("--prefix", type=str, default="mpu", help="Output filename prefix")
    ap.add_argument("--session-name", type=str, default="", help="Optional session label used as a directory/prefix")
    ap.add_argument("--dlpf", type=int, default=DLPF_DEFAULT, help="DLPF cfg 0..6 (default 3â‰ˆ44 Hz)")
    ap.add_argument("--temp", action="store_true", help="Also log onâ€‘die temperature (Â°C)")
    # NEW: flushing controls
    ap.add_argument("--flush-every", type=int, default=2000,
                    help="Flush to file every N rows (default 2000)")
    ap.add_argument("--flush-seconds", type=float, default=2.0,
                    help="Also flush if this many seconds passed (default 2.0)")
    ap.add_argument("--fsync-each-flush", action="store_true",
                    help="Call os.fsync() on each periodic flush (slower; default: final only)")
    ap.add_argument(
        "--no-record",
        action="store_true",
        help="Disable file output (no CSV/JSONL or metadata files)."
    )
    ap.add_argument(
        "--stream-stdout",
        action="store_true",
        help="Stream each sensor sample to stdout as JSON lines for a remote GUI."
    )
    ap.add_argument(
        "--stream-every",
        type=int,
        default=1,
        help=(
            "Only stream every N-th sample per sensor (default: 1 = every sample). "
            "Effective GUI stream rate â‰ˆ device_rate_hz / N."
        ),
    )
    ap.add_argument(
        "--timing-warnings",
        action="store_true",
        help="Print overrun warnings to stderr (debugging only).",
    )
    # Default streaming fields:
    #   - timestamp_ns (int): monotonic time in nanoseconds
    #   - t_s          (float): seconds since the run started
    #   - sensor_id    (int): logical sensor index (1, 2, or 3)
    #   - ax, ay, az   (float): linear acceleration in m/s^2
    #   - gx, gy, gz   (float): angular rate in deg/s
    # `--stream-fields` controls which of the measured channels (ax..gz, temp_c)
    # are added on top of the always-present timestamp_ns, t_s and sensor_id.
    ap.add_argument(
        "--stream-fields",
        type=str,
        default="ax,ay,az,gx,gy,gz",
        help=(
            "Comma-separated list of data fields to include in streamed JSON, chosen from: "
            "ax, ay, az, gx, gy, gz, temp_c. Always includes timestamp_ns, t_s, and sensor_id."
        ),
    )

    args = ap.parse_args()
    args.stream_every = max(1, int(args.stream_every))

    if args.list:
        scan_buses()
        return 0

    # ------------------------------------------------------------------
    # YAML configuration merge
    # ------------------------------------------------------------------
    # Strategy:
    #   1. Load optional ``mpu6050`` section from a config file
    #      (explicit --config or pi_config.yaml next to this script).
    #   2. Treat those values as defaults.
    #   3. Any explicit CLI option overrides the corresponding config
    #      value. For boolean flags like --temp, --no-record and
    #      --stream-stdout, the config controls the default state and
    #      CLI can only enable additional behaviour.
    script_dir = Path(__file__).resolve().parent
    default_cfg_path = script_dir / "pi_config.yaml"

    if args.config:
        cfg_path = Path(args.config)
    elif default_cfg_path.exists():
        cfg_path = default_cfg_path
    else:
        cfg_path = None

    cfg = {}
    section = {}
    if cfg_path is not None:
        try:
            cfg = load_config(cfg_path)
            section = cfg.get("mpu6050", {}) or {}
        except Exception as exc:
            print(f"[WARN] Failed to load config {cfg_path}: {exc}", file=sys.stderr)
            section = {}

    argv = sys.argv[1:]

    def _flag_present(name: str) -> bool:
        prefix = name + "="
        return any(a == name or a.startswith(prefix) for a in argv)

    # sample rate (Hz) via --sample-rate-hz / --rate / config fallback
    cfg_rate = section.get("sample_rate_hz")
    resolved_rate = None
    if args.sample_rate_hz is not None:
        resolved_rate = float(args.sample_rate_hz)
    elif args.rate is not None:
        resolved_rate = float(args.rate)
    elif cfg_rate is not None:
        try:
            resolved_rate = float(cfg_rate)
        except Exception:
            print(f"[WARN] Invalid mpu6050.sample_rate_hz in config: {cfg_rate!r}", file=sys.stderr)

    # sensors list (e.g. [1, 2, 3])
    cfg_sensors = section.get("sensors")
    if not _flag_present("--sensors") and cfg_sensors is not None:
        if isinstance(cfg_sensors, (list, tuple)):
            args.sensors = ",".join(str(s) for s in cfg_sensors)
        else:
            args.sensors = str(cfg_sensors)

    # channels: acc|gyro|both|default
    cfg_channels = section.get("channels")
    if not _flag_present("--channels") and cfg_channels is not None:
        args.channels = str(cfg_channels)

    # DLPF 0..6
    cfg_dlpf = section.get("dlpf")
    if not _flag_present("--dlpf") and cfg_dlpf is not None:
        try:
            args.dlpf = int(cfg_dlpf)
        except Exception:
            print(f"[WARN] Invalid mpu6050.dlpf in config: {cfg_dlpf!r}", file=sys.stderr)

    # include_temperature -> --temp
    if section.get("include_temperature") and not args.temp:
        args.temp = True

    # output_dir -> --out
    cfg_out = section.get("output_dir") or section.get("out")
    if cfg_out is not None and not _flag_present("--out"):
        args.out = str(cfg_out)

    # Optional behaviour flags
    if section.get("no_record") and not args.no_record:
        args.no_record = True
    if section.get("stream_stdout") and not args.stream_stdout:
        args.stream_stdout = True

    cfg_stream_every = section.get("stream_every")
    if cfg_stream_every is not None and not _flag_present("--stream-every"):
        try:
            args.stream_every = max(1, int(cfg_stream_every))
        except Exception:
            print(f"[WARN] Invalid mpu6050.stream_every in config: {cfg_stream_every!r}", file=sys.stderr)

    cfg_stream_fields = section.get("stream_fields")
    if cfg_stream_fields and not _flag_present("--stream-fields"):
        args.stream_fields = str(cfg_stream_fields)

    cfg_duration = section.get("duration_s")
    if cfg_duration is not None and args.duration is None and not _flag_present("--duration"):
        try:
            args.duration = float(cfg_duration)
        except Exception:
            print(f"[WARN] Invalid mpu6050.duration_s in config: {cfg_duration!r}", file=sys.stderr)

    cfg_samples = section.get("samples")
    if cfg_samples is not None and args.samples is None and not _flag_present("--samples"):
        try:
            args.samples = int(cfg_samples)
        except Exception:
            print(f"[WARN] Invalid mpu6050.samples in config: {cfg_samples!r}", file=sys.stderr)

    cfg_session = section.get("session_name")
    if cfg_session and not _flag_present("--session-name") and not args.session_name:
        args.session_name = str(cfg_session)

    if resolved_rate is None or resolved_rate <= 0:
        print(
            "ERROR: provide a positive --sample-rate-hz/--rate (or set mpu6050.sample_rate_hz in pi_config.yaml).",
            file=sys.stderr,
        )
        return 2

    args.rate = resolved_rate
    args.sample_rate_hz = resolved_rate
    session_name = (args.session_name or "").strip()
    session_slug = slugify_session_name(session_name) if session_name else ""
    session_name_for_paths = session_name or None

    # Clamp requested rate to practical 4..1000 Hz with DLPF enabled (datasheet)
    req_rate = max(4.0, min(float(resolved_rate), 1000.0))

    # ------------------------------------------------------------------
    # Rates overview
    # ------------------------------------------------------------------
    # - args.rate / mpu6050.sample_rate_hz:
    #     Device sampling + recording rate (Hz) on the Pi. Every enabled
    #     MPU6050 sensor runs at this cadence and records every sample.
    # - args.stream_every:
    #     Stream decimation factor; only every N-th sample per sensor is
    #     emitted over stdout for remote GUIs to keep bandwidth manageable.
    # - GUI refresh rate:
    #     Controlled on the desktop side (SignalsTab QTimer). This timer
    #     controls how frequently the plots redraw and is independent from
    #     the Pi sampling rate unless the GUI is in "follow sampling rate"
    #     mode.

    try:
        enabled = sorted({int(x) for x in args.sensors.split(",") if x.strip()})
        enabled = [s for s in enabled if s in (1, 2, 3)]
    except Exception:
        print("ERROR: Could not parse --sensors. Use e.g. '1,3'", file=sys.stderr)
        return 2
    if not enabled:
        print("ERROR: No valid sensors selected.", file=sys.stderr)
        return 2

    # Build mapping
    mapping = default_mapping()
    mapping.update(parse_sensor_map(args.map))

    # Open bus handles per bus id (share across sensors on same bus)
    bus_handles: Dict[int, SMBus] = {}
    devices: Dict[int, MPU6050] = {}
    who_values: Dict[int, int] = {}
    smplrt_divs: Dict[int, int] = {}
    actual_rates: Dict[int, float] = {}
    errors: Dict[int, int] = {sid: 0 for sid in enabled}
    samples_written: Dict[int, int] = {sid: 0 for sid in enabled}
    overruns = 0

    # File writers per sensor
    base_out_dir = Path(args.out).expanduser().resolve()
    helper_base_dir = base_out_dir
    helper_sensor_prefix = LOG_SUBDIR_MPU
    if base_out_dir.name != LOG_SUBDIR_MPU:
        helper_sensor_prefix = ""
    else:
        helper_base_dir = base_out_dir.parent
    out_dir = build_pi_session_dir(
        sensor_prefix=helper_sensor_prefix,
        session_name=session_name_for_paths,
        base_dir=helper_base_dir,
    )
    try:
        out_dir.mkdir(parents=True, exist_ok=True)
    except Exception as exc:
        print(
            f"[WARN] Unable to create output directory {out_dir}: {exc}",
            file=sys.stderr,
        )
        return 1

    writers: Dict[int, AsyncWriter] = {}

    # Optional aggregated JSONL log file (single file for all sensors)
    log_file_handle = None
    log_file_path: Optional[Path] = None
    log_file_error = False
    if args.log_file:
        try:
            log_file_path = Path(args.log_file).expanduser()
            log_file_path.parent.mkdir(parents=True, exist_ok=True)
            log_file_handle = open(log_file_path, "a", buffering=1, encoding="utf-8")
        except Exception as exc:
            print(f"[WARN] Unable to open log file {args.log_file}: {exc}", file=sys.stderr)
            return 1

    # Header now includes time vector `t_s` (seconds since start)
    header = ["timestamp_ns", "t_s", "sensor_id"]
    # Mapping of --channels to exported axes:
    #   acc     -> ax, ay, az
    #   gyro    -> gx, gy, gz
    #   both    -> ax, ay, az, gx, gy, gz
    #   default -> ax, ay, gz  (NO az; helps keep streaming light-weight)
    ch_mode = args.channels.lower()
    if ch_mode == "acc":
        header += ["ax", "ay", "az"]
    elif ch_mode == "gyro":
        header += ["gx", "gy", "gz"]
    elif ch_mode == "both":
        header += ["ax", "ay", "az", "gx", "gy", "gz"]
    elif ch_mode == "default":
        header += ["ax", "ay", "gz"]
    else:
        print("ERROR: invalid channels", file=sys.stderr); return 2
    if args.temp:
        header += ["temp_c"]

    # Compute stream_fields: fields added on top of timestamp_ns, t_s, sensor_id.
    # The stream payload always includes:
    #   timestamp_ns : int  (monotonic time in nanoseconds)
    #   t_s          : float (seconds since the run started)
    #   sensor_id    : int  (logical sensor index: 1, 2 or 3)
    # `stream_fields` then selects which *measured* channels (ax..gz, temp_c,
    # etc.) are added on top.
    user_fields = [
        s.strip()
        for s in (getattr(args, "stream_fields", "") or "").split(",")
        if s.strip()
    ]

    # Valid data fields are everything in header except the time/sensor_id trio
    base_fields = {"timestamp_ns", "t_s", "sensor_id"}
    valid_fields = [c for c in header if c not in base_fields]

    if not user_fields:
        stream_fields = valid_fields
    else:
        stream_fields = [f for f in user_fields if f in valid_fields]
        if not stream_fields:
            print(
                "[WARN] --stream-fields did not match any known columns; "
                f"falling back to default {valid_fields}",
                file=sys.stderr,
            )
            stream_fields = valid_fields

    if args.no_record and not args.stream_stdout:
        print(
            "[WARN] --no-record specified without --stream-stdout; "
            "run will produce no output files and no streaming data.",
            file=sys.stderr,
        )

    hostname = socket.gethostname()
    start_iso = datetime.utcnow().isoformat() + "Z"
    start_mono_ns = time.monotonic_ns()
    start_dt = datetime.now()
    format_ext = "csv" if args.format == "csv" else "jsonl"

    # Initialize sensors
    for sid in enabled:
        bus_id = mapping[sid].bus
        addr = mapping[sid].addr
        try:
            # Each sensor is initialised in its own try/except block so that a
            # single failing device or I2C bus does not abort the entire run.
            if bus_id not in bus_handles:
                bus_handles[bus_id] = SMBus(bus_id)
            dev = MPU6050(bus_handles[bus_id], addr)
            who = dev.who_am_i()
            if who not in (0x68, 0x69):
                print(f"[WARN] Sensor {sid} WHO_AM_I=0x{who:02X} (expected 0x68/0x69). Continuing.", file=sys.stderr)
            div, actual = dev.initialize(dlpf_cfg=args.dlpf, fs_accel=0, fs_gyro=0, rate_hz=req_rate)
            devices[sid] = dev
            who_values[sid] = who
            smplrt_divs[sid] = div
            actual_rates[sid] = actual

            # Prepare writer only if recording is enabled
            if not args.no_record:
                try:
                    paths = build_log_file_paths(
                        sensor_prefix=args.prefix,
                        session_name=session_name_for_paths,
                        sensor_id=sid,
                        start_dt=start_dt,
                        format_ext=format_ext,
                        out_dir=out_dir,
                    )
                    writer = AsyncWriter(
                        paths.data_path, args.format, header,
                        flush_every=args.flush_every,
                        flush_seconds=args.flush_seconds,
                        fsync_each_flush=args.fsync_each_flush
                    )
                    writer.meta_path = paths.meta_path
                    writer.start()
                    gyro_bw, acc_bw = DLPF_BW.get(args.dlpf, (None, None))
                    meta = {
                        "start_utc": start_iso,
                        "hostname": hostname,
                        "sensor_id": sid,
                        "bus": bus_id,
                        "address_hex": f"0x{addr:02X}",
                        "who_am_i_hex": f"0x{who:02X}",
                        "requested_rate_hz": float(args.rate),
                        "clamped_rate_hz": req_rate,
                        "dlpf_cfg": args.dlpf,
                        "dlpf_gyro_bw_hz": gyro_bw,
                        "dlpf_accel_bw_hz": acc_bw,
                        "fs_accel": "Â±2g",
                        "fs_gyro": "Â±250dps",
                        "smplrt_div": div,
                        "device_rate_hz": round(actual, 6),
                        "channels": ch_mode,
                        "format": args.format,
                        "header": header,
                        "start_monotonic_ns": start_mono_ns,
                        "session_name": session_name,
                        "session_slug": session_slug,
                        "session_dir": str(out_dir),
                        "stream_every": int(args.stream_every),
                        "stream_fields": list(stream_fields),
                        "version": 3
                    }
                    meta["pi_device_sample_rate_hz"] = meta["device_rate_hz"]
                    meta["pi_stream_decimation"] = meta["stream_every"]
                    meta["pi_stream_rate_hz"] = (
                        meta["pi_device_sample_rate_hz"] / meta["pi_stream_decimation"]
                    )
                    writer.write_metadata(meta)
                    writers[sid] = writer
                    bw_str = f"DLPF={args.dlpf} (gyroâ‰ˆ{gyro_bw}Hz, accelâ‰ˆ{acc_bw}Hz)" if gyro_bw else f"DLPF={args.dlpf}"
                    print(f"[INFO] Sensor {sid}: bus={bus_id} addr=0x{addr:02X} WHO=0x{who:02X} div={div} device_rateâ‰ˆ{actual:.3f} Hz {bw_str}")
                except Exception as exc:
                    print(
                        f"[WARN] Failed to initialize output at {out_dir} for sensor {sid}: {exc}",
                        file=sys.stderr,
                    )
                    return 1
            # else:
            #     # In GUI streaming mode this is just noise, so keep it silent by default.
            #     # print(
            #     #     f"[INFO] Sensor {sid}: no-record mode (CSV files disabled); streaming only.",
            #     #     file=sys.stderr,
            #     # )
        except FileNotFoundError:
            print(f"[WARN] Bus {bus_id} not available; sensor {sid} skipped.", file=sys.stderr)
        except Exception as e:
            print(f"[WARN] Failed to init sensor {sid} on bus {bus_id} @0x{addr:02X}: {e}", file=sys.stderr)

    if not devices:
        print("ERROR: No sensors initialized. Exiting.", file=sys.stderr)
        return 2

    if args.stream_stdout:
        # args.stream_every already has a default of 1
        pi_stream_decimation = int(args.stream_every or 1)

        for sid in sorted(devices.keys()):
            # Actual per-sensor rate if available, otherwise fall back to requested
            pi_device_sample_rate_hz = float(actual_rates.get(sid, req_rate))
            pi_stream_rate_hz = pi_device_sample_rate_hz / pi_stream_decimation

            print(
                "[INFO][PI] streaming: "
                f"sensor={sid} "
                f"pi_device_sample_rate_hz={pi_device_sample_rate_hz:.1f} "
                f"pi_stream_decimation={pi_stream_decimation} "
                f"pi_stream_rate_hz={pi_stream_rate_hz:.1f} "
                f"stream_fields={stream_fields}",
                file=sys.stderr,
            )

        # Build a single meta header line for the whole run
        avg_device_rate = (
            sum(actual_rates.values()) / len(actual_rates)
            if actual_rates
            else float(req_rate)
        )
        meta_header = {
            "meta": "mpu6050_stream_config",
            "sensor_ids": sorted(int(sid) for sid in devices.keys()),
            "pi_device_sample_rate_hz": float(avg_device_rate),
            "pi_stream_decimation": pi_stream_decimation,
        }
        meta_header["pi_stream_rate_hz"] = (
            meta_header["pi_device_sample_rate_hz"]
            / meta_header["pi_stream_decimation"]
        )

        # Advertise the Pi-side stream configuration as an initial JSON header so
        # the desktop GUI knows the device rate and how many samples are skipped
        # by --stream-every when estimating the live stream rate.
        # Emit once on stdout before any samples
        print(json.dumps(meta_header), file=sys.stdout, flush=True)

    # Sampling control
    controller = monotonic_controller(req_rate)
    target_next = next(controller)

    # Graceful stop flags
    stop_flag = {"stop": False}

    def _handle_sigint(signum, frame):
        stop_flag["stop"] = True
    signal.signal(signal.SIGINT, _handle_sigint)
    signal.signal(signal.SIGTERM, _handle_sigint)

    # Determine stopping condition
    deadline_ns = None
    if args.duration is not None and args.duration > 0:
        deadline_ns = time.monotonic_ns() + int(args.duration * 1e9)
    max_samples = args.samples if (args.samples and args.samples > 0) else None

    try:
        n = 0
        warn_every = 50
        while True:
            # Sleep until the next *target* tick from monotonic_controller so the
            # loop tracks the requested sample rate, counting overruns instead of
            # silently drifting when iterations run late.
            now_ns = time.monotonic_ns()
            sleep_ns = target_next - now_ns
            if sleep_ns > 0:
                time.sleep(sleep_ns / 1e9)
            else:
                overruns += 1
                if args.timing_warnings and overruns % warn_every == 1:
                    over_ms = -sleep_ns / 1e6
                    print(
                        f"[WARN] Overrun: loop behind by {over_ms:.3f} ms (count={overruns})",
                        file=sys.stderr,
                    )

            # timestamp each read individually
            for sid, dev in list(devices.items()):
                try:
                    wall_ns = time.time_ns()
                    ts_ns = time.monotonic_ns()
                    t_s = (ts_ns - start_mono_ns) / 1e9
                    row = {"timestamp_ns": ts_ns, "t_s": t_s, "sensor_id": sid}

                    if ch_mode == "acc":
                        ax, ay, az = dev.read_accel()
                        row.update({
                            "ax": (ax / ACC_SF) * G_TO_MS2,
                            "ay": (ay / ACC_SF) * G_TO_MS2,
                            "az": (az / ACC_SF) * G_TO_MS2,
                        })
                    elif ch_mode == "gyro":
                        gx, gy, gz = dev.read_gyro()
                        row.update({
                            "gx": gx / GYR_SF,
                            "gy": gy / GYR_SF,
                            "gz": gz / GYR_SF,
                        })
                    elif ch_mode == "both":
                        ax, ay, az = dev.read_accel()
                        gx, gy, gz = dev.read_gyro()
                        row.update({
                            "ax": (ax / ACC_SF) * G_TO_MS2,
                            "ay": (ay / ACC_SF) * G_TO_MS2,
                            "az": (az / ACC_SF) * G_TO_MS2,
                            "gx": gx / GYR_SF,
                            "gy": gy / GYR_SF,
                            "gz": gz / GYR_SF,
                        })
                    elif ch_mode == "default":
                        # AX, AY and GZ (matches "original script" behavior)
                        ax, ay, _ = dev.read_accel()
                        _, _, gz = dev.read_gyro()
                        row.update({
                            "ax": (ax / ACC_SF) * G_TO_MS2,
                            "ay": (ay / ACC_SF) * G_TO_MS2,
                            "gz": gz / GYR_SF,
                        })

                    if args.temp:
                        try:
                            row["temp_c"] = dev.read_temp_c()
                        except Exception:
                            row["temp_c"] = float("nan")

                    # 1) Optional aggregated JSONL logging (one file for all samples)
                    if log_file_handle is not None and not log_file_error:
                        log_payload = {
                            "sensor_id": sid,
                            "t_s": wall_ns,
                            "timestamp_ns": ts_ns,
                            "t_rel_s": t_s,
                        }
                        for key in ("ax", "ay", "az", "gx", "gy", "gz", "temp_c"):
                            if key in row:
                                log_payload[key] = row[key]
                        try:
                            log_file_handle.write(json.dumps(log_payload, separators=(",", ":")) + "\n")
                        except Exception as exc:
                            log_file_error = True
                            print(f"[WARN] Failed to write to log file {log_file_path}: {exc}", file=sys.stderr)

                    # 2) Optional per-sensor file output
                    w = writers.get(sid)
                    if w is not None:
                        w.write(row)

                    # 3) Update per-sensor sample counter
                    samples_written[sid] += 1

                    # 4) Optional stdout streaming (decimated per sensor)
                    if args.stream_stdout and (samples_written[sid] % max(1, args.stream_every) == 0):
                        out_obj = {
                            "timestamp_ns": ts_ns,
                            "t_s": t_s,
                            "sensor_id": sid,
                        }
                        for key in stream_fields:
                            if key in row:
                                out_obj[key] = row[key]
                        print(json.dumps(out_obj, separators=(",", ":")), flush=True)
                except Exception as e:
                    errors[sid] += 1
                    if errors[sid] <= 10 or (errors[sid] % 100) == 0:
                        print(f"[WARN] Read error on sensor {sid}: {e} (count={errors[sid]})", file=sys.stderr)
                    continue

            n += 1
            if max_samples is not None and n >= max_samples:
                break
            if deadline_ns is not None and time.monotonic_ns() >= deadline_ns:
                break
            if stop_flag["stop"]:
                break
            target_next = next(controller)

    finally:
        # Stop writers and close buses
        for sid, w in writers.items():
            w.stop()
        for bus in bus_handles.values():
            try:
                bus.close()
            except Exception:
                pass
        if log_file_handle is not None:
            try:
                log_file_handle.close()
            except Exception:
                pass

        # Summary
        print("\n=== Run summary ===")
        print(f"Host: {hostname}")
        print(f"Started: {start_iso}")
        if session_name:
            if session_slug:
                print(f"Session: {session_name} (slug: {session_slug})")
            else:
                print(f"Session: {session_name}")
        for sid in enabled:
            if sid in samples_written:
                print(f" Sensor {sid}: samples={samples_written[sid]}, errors={errors.get(sid, 0)}")
        print(f" Overruns: {overruns}")
        print("Output directory:", out_dir)
        if log_file_path is not None:
            print("Aggregated log file:", log_file_path)


if __name__ == "__main__":
    sys.exit(main())

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\pi_config.yaml
# File: pi_config.yaml (ext: .yaml
# Dir : raspberrypi_scripts\
# Size: 393 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\pi_config.yaml
# GENERATED FILE - DO NOT EDIT BY HAND
# This file is derived from SamplingConfig and PiLoggerConfig.
device_rate_hz: 200
record_decimate: 4
stream_decimate: 8
record_rate_hz: 50.0
stream_rate_hz: 25.0
mpu6050:
  output_dir: ~/logs/mpu
  sample_rate_hz: 200
  record_decimate: 4
  stream_every: 8
  record_rate_hz: 50.0
  stream_rate_hz: 25.0
  sensors: [1, 2, 3]
  include_temperature: false

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\pi_logger_common.py
# File: pi_logger_common.py (ext: .py
# Dir : raspberrypi_scripts\
# Size: 367 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\pi_logger_common.py
"""Shared helpers for Raspberry Pi logger scripts."""

from pathlib import Path
from typing import Any, Dict

import yaml


def load_config(path: str | Path) -> Dict[str, Any]:
    config_path = Path(path)
    if not config_path.exists():
        return {}
    with config_path.open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh) or {}

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\README_rpi.md
# File: README_rpi.md (ext: .md
# Dir : raspberrypi_scripts\
# Size: 967 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\README_rpi.md
# SensePi Raspberry Pi scripts

This folder contains the lightweight logger scripts that should be copied to a Raspberry Pi. They record MPU6050 sensor data locally on the device and can optionally stream lines over stdout for live viewing.

## Files

- `mpu6050_multi_logger.py` â€“ multi-sensor MPU6050 logger.
- `pi_logger_common.py` â€“ shared helpers and configuration loading.
- `pi_config.yaml` â€“ example configuration file for rates, channels, and paths.
- `install_pi_deps.sh` â€“ installs Python dependencies on the Pi.
- `run_all_sensors.sh` â€“ simple launcher for the MPU6050 logger.

## Setup

On your workstation:

```bash
scp -r raspberrypi_scripts pi@<host>:/home/pi/
ssh pi@<host> "bash /home/pi/raspberrypi_scripts/install_pi_deps.sh"
```

Then edit pi_config.yaml on the Pi to match your logging directory and sensor layout, and run:

```bash
ssh pi@<host> "cd /home/pi/raspberrypi_scripts && ./run_all_sensors.sh"
```

------------------------------ END OF FILE ------------------------------

============================= raspberrypi_scripts\run_all_sensors.sh
# File: run_all_sensors.sh (ext: .sh
# Dir : raspberrypi_scripts\
# Size: 207 bytes
# Time: 30/11/2025 16:51
============================= raspberrypi_scripts\run_all_sensors.sh
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG="$SCRIPT_DIR/pi_config.yaml"

python3 "$SCRIPT_DIR/mpu6050_multi_logger.py" --config "$CONFIG"

------------------------------ END OF FILE ------------------------------

============================= README.md
# File: README.md (ext: .md
# Dir : 
# Size: 11755 bytes
# Time: 30/11/2025 16:51
============================= README.md
# SensePi Local Recording & Live View

This repository hosts a desktop application for managing Raspberry Piâ€“based sensor loggers alongside the scripts that run directly on the Pi. The PC/WSL side provides a PySide6 GUI for recording, live viewing, and analyzing data, while the Pi side supplies lightweight logger scripts for specific sensors. The desktop app focuses on inspecting logs from the MPU6050 logger.

New to the project? See [docs/LEARNING_PATH.md](docs/LEARNING_PATH.md) for a milestone-based walkthrough aimed at students.

## Architecture & Roles

- **Desktop GUI (PC/WSL)**
  - Starts/stops Pi loggers over SSH and synchronizes files over SFTP
  - Displays live time-domain and FFT plots
  - Pushes sensor defaults from `src/sensepi/config/sensors.yaml` into each Piâ€™s `pi_config.yaml`
  - Downloads recent CSV/JSONL logs for offline browsing
- **Raspberry Pi logger**
  - Runs the `raspberrypi_scripts/` CSV/JSONL logging scripts on the device
  - Streams JSON lines over stdout to the desktop (protocol: `docs/json_protocol.md`)

```mermaid
flowchart LR
  PC[Desktop GUI] <-->|SSH / SFTP| Pi[Raspberry Pi logger]
  Pi -->|JSON lines over stdout| PC
```

The JSON streaming format and field definitions are documented in `docs/json_protocol.md`.

Configuration files live with the GUI under `src/sensepi/config/hosts.yaml` and `src/sensepi/config/sensors.yaml`, while each Pi keeps its active settings in `raspberrypi_scripts/pi_config.yaml`; the GUIâ€™s Sync action pushes the desktop sensor defaults to that Pi file so every logger shares the same baseline.

## Decimation & Plotting Configuration

Low-latency recording, streaming, and visualization all pull their settings from the `SensePiConfig` dataclass (`src/sensepi/config/runtime.py`).  Load it from YAML via `sensepi.config.load_config`, tweak the fields you care about, then pass it to `sensepi.core.pipeline_wiring.build_pipeline` (for the recorder/streamer/plotter fan-out) and `LivePlot.from_config` (for Matplotlib demos such as `run_live_plot.py`).  Adjusting the config object once at startup keeps rasterizer, streamer, and recorder behaviour in sync without editing multiple modules.

Recommended starting points for human-friendly refresh rates:

- `plot_fs â‰ˆ 50 Hz` keeps the Matplotlib view smooth on Pi Zero 2 while keeping the decimator ratio large enough for 500â€“1000 Hz sensors.
- `smoothing_alpha â‰ˆ 0.2` corresponds to ~20â€“50 ms of IIR smoothingâ€”large enough to calm jitter but small enough to keep spikes visible.
- `plot_window_seconds â‰ˆ 5â€“10 s` balances temporal context and responsivity for the scrolling plot.
- `spike_threshold â‰ˆ 3Ã—` the noise standard deviation works well for highlighting interesting transients without littering the plot with markers.

Create a YAML file with the fields you need (unknown keys are ignored) and point the demo at it:

```yaml
pipeline:
  sensor_fs: 1000.0
  stream_fs: 40.0
  plot_fs: 48.0
  plot_window_seconds: 8.0
  smoothing_alpha: 0.2
  spike_threshold: 0.6
```

```bash
python run_live_plot.py --config configs/pipeline.yaml
```

`run_live_plot.py` also exposes `--plot-window`, `--spike-threshold`, and `--plot-fs` flags for quick experiments without editing the YAML file.

## Project layout

```
sense-pi-local-recording-live/
â”œâ”€â”€ main.py              # Thin launcher that delegates to sensepi.gui.application
â”œâ”€â”€ src/sensepi/tools/   # Local plotting helpers and CLI plotter
â”œâ”€â”€ pyproject.toml       # packaging metadata
â”œâ”€â”€ requirements.txt     # desktop dependencies
â”œâ”€â”€ requirements-pi.txt  # Pi dependencies
â”œâ”€â”€ src/sensepi/         # desktop application package
â”œâ”€â”€ raspberrypi_scripts/ # files copied to the Raspberry Pi
â”œâ”€â”€ data/                # raw and processed data (ignored)
â”œâ”€â”€ logs/                # application logs (ignored)
â””â”€â”€ archive/             # legacy and experimental files
```

## Run the GUI

From the project root you can launch the Qt application via the canonical
entrypoint:

```bash
python -m sensepi.gui.application
# or, after installing the package:
sensepi-gui
```

`main.py` simply delegates to the same launcher so either form works. The
tabbed interface exposes Recorder, Signals, FFT, Offline, and Settings tabs.
Configuration defaults live under `src/sensepi/config/` and can be customised
per host and sensor.

### SSH authentication

The desktop app connects to each Raspberry Pi using a username and password.
Populate `src/sensepi/config/hosts.yaml` (or the Settings tab) with the host,
port, username, and password for each Pi. SSH key authentication and agent
forwarding are not supported in this version.

## Download logs from the Pi

A common SensePi workflow is:

1. **Configure your Raspberry Pi host**  
   Open the **Settings** tab and make sure your Pi appears under *Raspberry Pi hosts* with a working `host`, `user`, `base_path`, and `data_dir`. Use **Sync config to Pi** to push the current sampling defaults whenever you change sensors or rates.

2. **Start a recording from the Signals tab**  
   Go to the **Signals** tab, pick your Pi host, choose a sample rate, and tick the **Recording** checkbox. Optionally provide a *Session name* to label the run, then click **Start**. Hint text beneath the buttons confirms exactly which directory on the Pi the CSV/JSONL files will land in.

3. **Stop the recording**  
   Press **Stop** when you have captured enough data. The status bar repeats the path of the logs on the Pi and reminds you to visit the **Offline logs** tab to sync them.

4. **Download logs to your desktop**  
   Switch to the **Offline logs** tab. Click **Sync logs from Pi** to pull any new `.csv` or `.jsonl` files from that hostâ€™s `data_dir` into your local `data/raw` folder. If you want a shortcut that also opens the newest file, use **Sync & open latest** instead.

5. **Inspect the recording offline**  
Use the *Offline log files* list to double-click a file, or let **Sync & open latest** select it automatically. The embedded Matplotlib viewer renders the data with the same conventions as the live **Signals** tab so you can inspect the captured session without staying connected to the Pi.

## Log Conventions (where your data lives)

SensePi keeps all of your raw logs as plain files, both on the Pi and on your computer. Knowing where they live â€“ and what the filenames mean â€“ makes it much easier to grab the right run later.

### On the Raspberry Pi

By default the Pi creates the following folders:

- `~/logs` â€“ root directory for all recordings on the device
- `~/logs/mpu` â€“ files produced by the MPU6050 IMU logger

When you start a recording you can supply an optional **session name** (for example, `Trial1`). If you do, the logger writes that run into a dedicated folder:

- `~/logs/mpu/Trial1/` â€“ all files for that session

Leaving the session field blank keeps the files directly under `~/logs/mpu`. Either way you will see one data file per physical sensor (e.g., `S1`, `S2`) plus a small metadata file.

### On the PC (after syncing)

Clicking **Sync logs from Pi** in the Offline tab pulls every new `.csv` or `.jsonl` file from the Pi into the desktop projectâ€™s data folder:

- `data/raw` â€“ root directory for downloaded logs

If the run had a session name, that folder is recreated locally:

- `data/raw/trial1/` â€“ the session folder after being slugified (lowercase, spaces to hyphens)

If no session name was supplied, the GUI groups logs by host and sensor type instead so devices never clobber one another:

- `data/raw/mypi/mpu/` â€“ files recorded on a Pi host named `mypi`

Everything remains normal files on disk, so you can back them up, version them, or open them in other tools.

### File name pattern

Data files use the following pattern:

```
[<session>_]<sensorPrefix>_S<sensorID>_<timestamp>.<ext>
```

Examples:

- `Trial1_mpu_S1_2025-11-30_04-53-33.csv`
- `mpu_S2_2025-11-30_04-53-33.jsonl`

Where:

- `<session>` is your session name, turned into a filesystem-safe slug (lowercase, hyphenated). If you left the field blank, this part disappears.
- `<sensorPrefix>` is a short code for the logger. For the IMU it is `mpu`.
- `S<sensorID>` is the sensor index on the logger (S1, S2, ...).
- `<timestamp>` is the recording start time in UTC formatted as `YYYY-MM-DD_HH-MM-SS`.
- `<ext>` is the file format: `.csv` or `.jsonl`.

Each data file is paired with a metadata sidecar whose name ends in `.meta.json`, for example:

- `Trial1_mpu_S1_2025-11-30_04-53-33.csv.meta.json`

The metadata records the sample rate, which axes were enabled, and other run settings. The Offline tab uses it to plot data correctly, so keep it beside the data file when copying or renaming logs.

### Sample rate and decimation

The MPU6050 can be sampled very quickly (hundreds of Hertz), but SensePi purposely **decimates** those samples for recording and streaming so files stay small and the live plots stay responsive. You will see three related rates in configs and metadata:

- **Device rate** â€“ how fast the sensor is polled on the Pi itself (for example, 200â€¯Hz).
- **Record rate** â€“ how often samples are written to the CSV/JSONL file (for example, keeping every 4th device sample â†’ 50â€¯Hz in the log).
- **Stream rate** â€“ how often samples are forwarded live over SSH to the GUI (for example, every 8th device sample â†’ 25â€¯Hz on the live plot).

Because of decimation the stored CSV may contain fewer samples per second than the raw device rate, which is an intentional tradeoff for smaller files and smoother UI updates. Inspect `sensors.yaml`, `pi_config.yaml`, or the `.meta.json` file for a given run to see the exact rates that were used.

## Configuration paths

By default the GUI stores output under `data/` and `logs/` inside the project
root. Set `SENSEPI_DATA_ROOT` or `SENSEPI_LOG_DIR` (they both understand `~`)
before launching the app to relocate those folders for packaged installs or
custom deployments. The paths shown in `src/sensepi/config/hosts.yaml` and
`raspberrypi_scripts/pi_config.yaml` are only examplesâ€”update them to match
each Piâ€™s filesystem layout.

## Sync config to Pi

The Settings tab offers a **Sync config to Pi** action. It builds a
`pi_config.yaml` from the desktop sensor defaults, validates that the remote
scripts/data directories exist over SSH, and uploads the YAML to the path
configured for the selected host. The desktop configuration is the source of
truth; use this button to push updates to your Pis.

## Plotting from CSV logs

The Matplotlib-based CLI plotter lives at `src/sensepi/tools/plotter.py`. Run
it directly or import its helpers for embedding in Qt tabs:

```bash
python -m sensepi.tools.plotter --file data/raw/your_log.csv
```

An offline analysis tab in the GUI reuses the same plotting logic to view
recent CSV/JSONL logs without starting a live stream.

## Raspberry Pi scripts

The `raspberrypi_scripts/` folder contains the low-level loggers that run on the Pi. Copy the folder to your device (e.g., `/home/pi/raspberrypi_scripts`) and install dependencies:

```bash
scp -r raspberrypi_scripts pi@<host>:/home/pi/
ssh pi@<host> "bash /home/pi/raspberrypi_scripts/install_pi_deps.sh"
```

Use `run_all_sensors.sh` as a simple helper to start the provided logger scripts. Adjust `pi_config.yaml` to set sample rates, output paths, and channel selections.

The streaming JSON wire protocol used by the loggers is documented in
`docs/json_protocol.md`.

## Legacy content

The `archive/` directory is reserved for older or experimental scripts that
you donâ€™t want to delete yet. You can move legacy entrypoints or prototypes
there to keep the main code paths focused on the current PySide6-based
workflow.

------------------------------ END OF FILE ------------------------------

============================= requirements-pi.txt
# File: requirements-pi.txt (ext: .txt
# Dir : 
# Size: 39 bytes
# Time: 30/11/2025 16:51
============================= requirements-pi.txt
numpy>=1.25
RPi.GPIO
smbus2
PyYAML

------------------------------ END OF FILE ------------------------------

============================= requirements.txt
# File: requirements.txt (ext: .txt
# Dir : 
# Size: 111 bytes
# Time: 30/11/2025 16:51
============================= requirements.txt
PySide6>=6.6
numpy>=1.25
scipy>=1.11
matplotlib>=3.8
paramiko>=3.3
PyYAML>=6.0
psutil>=5.9
pyqtgraph>=0.13

------------------------------ END OF FILE ------------------------------

============================= run_live_plot.py
# File: run_live_plot.py (ext: .py
# Dir : 
# Size: 3341 bytes
# Time: 30/11/2025 16:51
============================= run_live_plot.py
"""Standalone demo that drives LivePlot with a synthetic signal."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Iterator, Tuple

import matplotlib.pyplot as plt
import numpy as np

REPO_ROOT = Path(__file__).resolve().parent
SRC_DIR = REPO_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from sensepi.config import SensePiConfig, load_config

from live_plot import LivePlot


def fake_decimated_stream(
    chunk_size: int = 2,
    dt: float = 0.02,
    freq_hz: float = 1.2,
) -> Iterator[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:
    """
    Yield synthetic decimated data roughly resembling Plotter output.

    ``dt`` simulates the decimated sampling interval (50 Hz by default).  Each
    chunk provides ``chunk_size`` points so callers can mimic batched updates.
    """
    t = 0.0
    rand = np.random.default_rng()
    while True:
        t_vals = t + np.arange(chunk_size, dtype=float) * dt
        base = np.sin(2.0 * np.pi * freq_hz * t_vals)
        noise = 0.1 * rand.standard_normal(size=chunk_size)
        mean = base + noise
        width = 0.15 + 0.05 * rand.standard_normal(size=chunk_size)
        y_min = mean - np.abs(width)
        y_max = mean + np.abs(width)
        yield t_vals, mean, y_min, y_max
        t = float(t_vals[-1] + dt)


def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="SensePi LivePlot demo")
    parser.add_argument(
        "--config",
        type=Path,
        help="Optional YAML file describing SensePiConfig overrides",
    )
    parser.add_argument(
        "--plot-window",
        type=float,
        help="Override plot_window_seconds without editing the YAML",
    )
    parser.add_argument(
        "--spike-threshold",
        type=float,
        help="Override spike_threshold without editing the YAML",
    )
    parser.add_argument(
        "--plot-fs",
        type=float,
        help="Decimated frequency (Hz) for the synthetic input",
    )
    return parser


def _resolve_config(args: argparse.Namespace) -> SensePiConfig:
    cfg = load_config(args.config) if args.config else SensePiConfig()
    if args.plot_window is not None:
        cfg.plot_window_seconds = float(args.plot_window)
    if args.spike_threshold is not None:
        cfg.spike_threshold = float(args.spike_threshold)
    if args.plot_fs is not None:
        cfg.plot_fs = float(args.plot_fs)
    return cfg.sanitized()


def main(argv: list[str] | None = None) -> None:
    parser = _build_arg_parser()
    args = parser.parse_args(argv)
    cfg = _resolve_config(args)

    lp = LivePlot.from_config(cfg)

    dt = 1.0 / float(cfg.plot_fs)
    stream = fake_decimated_stream(dt=dt)

    def fetch():
        # In production replace with queue-draining logic that pulls PlotUpdate
        # objects from Plotter.queue. Returning ``None`` tells the animation loop
        # that no new data is available for this frame.
        try:
            return next(stream)
        except StopIteration:
            return None

    refresh_hz = max(1.0, min(60.0, cfg.plot_fs))
    interval_ms = max(5, int(round(1000.0 / refresh_hz)))
    lp.start_animation(fetch, interval_ms=interval_ms)
    plt.show()


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\
# Size: 44 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\__init__.py
"""SensePi desktop application package."""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\analysis\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\analysis\
# Size: 362 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\analysis\__init__.py
"""Signal analysis utilities (FFT, filtering, and feature extraction).

This package gathers pure-Python helpers that operate on NumPy arrays of
sensor samples. Modules such as :mod:`fft`, :mod:`filters`, :mod:`features`,
and :mod:`rate` stay free of Qt and I/O dependencies so they can be reused in
command-line scripts, automated tests, or GUI tabs alike.
"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\analysis\features.py
# File: features.py (ext: .py
# Dir : src\sensepi\analysis\
# Size: 258 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\analysis\features.py
"""Feature extraction helpers."""

import numpy as np


def rms(signal: np.ndarray) -> float:
    return float(np.sqrt(np.mean(np.square(signal))))


def peak_to_peak(signal: np.ndarray) -> float:
    return float(np.max(signal) - np.min(signal))

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\analysis\fft.py
# File: fft.py (ext: .py
# Dir : src\sensepi\analysis\
# Size: 376 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\analysis\fft.py
"""FFT helpers."""

import numpy as np


def compute_fft(signal: np.ndarray, sample_rate_hz: float) -> tuple[np.ndarray, np.ndarray]:
    """Compute frequency bins and magnitudes for a 1-D signal."""
    fft_result = np.fft.rfft(signal)
    freqs = np.fft.rfftfreq(signal.size, d=1.0 / sample_rate_hz)
    magnitude = np.abs(fft_result)
    return freqs, magnitude

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\analysis\filters.py
# File: filters.py (ext: .py
# Dir : src\sensepi\analysis\
# Size: 491 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\analysis\filters.py
"""Filtering helpers."""

from typing import Tuple

import numpy as np
from scipy import signal


def butter_lowpass(data: np.ndarray, cutoff_hz: float, sample_rate_hz: float, order: int = 4) -> np.ndarray:
    nyquist = 0.5 * sample_rate_hz
    normal_cutoff = cutoff_hz / nyquist
    b, a = signal.butter(order, normal_cutoff, btype="low", analog=False)
    return signal.filtfilt(b, a, data)


def detrend(data: np.ndarray) -> np.ndarray:
    return signal.detrend(data)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\analysis\rate.py
# File: rate.py (ext: .py
# Dir : src\sensepi\analysis\
# Size: 2068 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\analysis\rate.py
from __future__ import annotations

from collections import deque
from dataclasses import dataclass
from typing import Deque, Iterable


@dataclass
class RateEstimate:
    hz_effective: float
    hz_raw_status: float | None
    hz_ts_window: float | None
    quality: str


class RateController:
    """Estimate streaming rate from sample timestamps."""

    def __init__(self, window_size: int = 100, default_hz: float = 0.0) -> None:
        if window_size <= 1:
            raise ValueError("window_size must be > 1")
        self._times: Deque[float] = deque(maxlen=window_size)
        self.default_hz = float(default_hz)
        self._hz_status: float | None = None

    def add_sample_time(self, t: float) -> None:
        self._times.append(float(t))

    def update_from_status(self, hz: float | None = None) -> RateEstimate:
        if hz is None:
            hz = self.default_hz
        self._hz_status = float(hz)
        return RateEstimate(hz_effective=float(hz), hz_raw_status=float(hz), hz_ts_window=None, quality="status_only")

    @property
    def estimated_hz(self) -> float:
        if len(self._times) < 2:
            return self.default_hz
        t0 = self._times[0]
        t1 = self._times[-1]
        if t1 <= t0:
            return self.default_hz
        span = t1 - t0
        count = len(self._times) - 1
        return count / span if span > 0 else self.default_hz

    def estimate(self) -> RateEstimate:
        hz_ts = self.estimated_hz
        if self._hz_status is not None:
            hz_eff = 0.5 * self._hz_status + 0.5 * hz_ts
            return RateEstimate(hz_effective=hz_eff, hz_raw_status=self._hz_status, hz_ts_window=hz_ts, quality="fused")
        return RateEstimate(hz_effective=hz_ts, hz_raw_status=None, hz_ts_window=hz_ts, quality="ts_only")

    def reset(self) -> None:
        self._times.clear()
        self._hz_status = None

    def feed_times(self, times: Iterable[float]) -> None:
        for t in times:
            self.add_sample_time(t)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\config\
# Size: 556 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\__init__.py
"""Configuration objects and helpers for SensePi.

This package knows how to load/save YAML descriptors that capture the current
lab hardware setup, including:
- ``hosts.yaml`` with Raspberry Pi targets
- ``sensors.yaml`` and ``sampling.py`` defaults for each device
The resulting typed dataclasses (see :mod:`runtime`) are imported everywhere
else to configure recorders, streamers, and GUI defaults consistently.
"""

from .runtime import SensePiConfig, config_from_mapping, load_config

__all__ = ["SensePiConfig", "config_from_mapping", "load_config"]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\app_config.py
# File: app_config.py (ext: .py
# Dir : src\sensepi\config\
# Size: 16950 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\app_config.py
"""Default application paths and configuration helpers."""

from __future__ import annotations

import math
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, Optional

import yaml

from .pi_logger_config import PiLoggerConfig
from .sampling import SamplingConfig


DEFAULT_BASE_PATH = Path("~/sensor")
DEFAULT_DATA_DIR = Path("~/logs")


def load_sensor_defaults(path: Path) -> tuple[Dict[str, Any], SamplingConfig]:
    """Load ``sensors.yaml`` content and the corresponding SamplingConfig."""

    path = Path(path)
    if path.exists():
        with path.open("r", encoding="utf-8") as fh:
            raw = yaml.safe_load(fh) or {}
    else:
        raw = {}

    if not isinstance(raw, dict):
        raw = {}

    sampling = SamplingConfig.from_mapping(raw)
    return raw, sampling


def save_sensor_defaults(path: Path, raw: Mapping[str, Any], sampling: SamplingConfig) -> None:
    """Persist ``sensors.yaml`` while keeping ``sampling`` authoritative."""

    path = Path(path)
    data = dict(raw or {})
    data.update(sampling.to_mapping())

    sensors_block = data.get("sensors")
    if isinstance(sensors_block, Mapping):
        cleaned_sensors: Dict[str, Any] = {}
        for key, cfg in sensors_block.items():
            if isinstance(cfg, Mapping):
                sensor_cfg = dict(cfg)
                sensor_cfg.pop("sample_rate_hz", None)
                cleaned_sensors[str(key)] = sensor_cfg
            else:
                cleaned_sensors[str(key)] = cfg
        data["sensors"] = cleaned_sensors
    elif sensors_block is None:
        data["sensors"] = {}

    if path.parent and not path.parent.exists():
        path.parent.mkdir(parents=True, exist_ok=True)

    with path.open("w", encoding="utf-8") as fh:
        yaml.safe_dump(
            data,
            fh,
            default_flow_style=False,
            sort_keys=False,
        )


@dataclass
class AppPaths:
    """
    Commonly used paths for the desktop application.

    ``SENSEPI_DATA_ROOT`` and ``SENSEPI_LOG_DIR`` override the default
    ``data``/``logs`` folders relative to the repository root so that
    packaged installs and alternate layouts can store files elsewhere.
    """

    # repo_root points at the project root (one level above src/)
    repo_root: Path = Path(__file__).resolve().parents[3]
    data_root: Path = field(init=False)
    raw_data: Path = field(init=False)
    processed_data: Path = field(init=False)
    logs: Path = field(init=False)
    config_dir: Path = field(init=False)

    def __post_init__(self) -> None:
        env_data_root = os.environ.get("SENSEPI_DATA_ROOT")
        if env_data_root:
            self.data_root = Path(env_data_root).expanduser()
        else:
            self.data_root = self.repo_root / "data"

        env_logs_dir = os.environ.get("SENSEPI_LOG_DIR")
        if env_logs_dir:
            self.logs = Path(env_logs_dir).expanduser()
        else:
            self.logs = self.repo_root / "logs"

        self.raw_data = self.data_root / "raw"
        self.processed_data = self.data_root / "processed"
        self.config_dir = self.repo_root / "src" / "sensepi" / "config"

    def ensure(self) -> None:
        """Create directories if they do not yet exist."""
        for path in (self.data_root, self.raw_data, self.processed_data, self.logs):
            path.mkdir(parents=True, exist_ok=True)


@dataclass
class HostConfig:
    """Normalized host configuration derived from ``hosts.yaml`` entries."""

    name: str
    host: str
    user: str
    port: int
    base_path: Path
    data_dir: Path
    pi_config_path: Path
    password: Optional[str] = None


def _hz_to_interval_ms(value_hz: float, fallback_ms: int) -> int:
    """Convert a frequency in Hz into a positive integer interval in ms."""
    try:
        hz = float(value_hz)
    except (TypeError, ValueError):
        hz = 0.0
    if hz <= 0.0 or math.isnan(hz) or math.isinf(hz):
        return max(1, int(fallback_ms))
    interval = int(round(1000.0 / hz))
    return max(1, interval)


@dataclass
class PlotPerformanceConfig:
    """
    Tunable limits and refresh rates for the live plot / FFT tabs.

    These parameters cap resource usage so the GUI stays responsive even
    when multiple sensors or view presets are active.
    """

    signal_update_hz: float = 50.0
    time_window_seconds: float = 3.0
    fft_update_hz: float = 10.0
    max_signal_subplots: int = 18
    max_lines_per_subplot: int = 1
    signal_max_points_per_line: int = 2000

    def signal_refresh_interval_ms(self) -> int:
        """Return the timer interval that corresponds to ``signal_update_hz``."""
        return _hz_to_interval_ms(self.signal_update_hz, fallback_ms=50)

    def fft_refresh_interval_ms(self) -> int:
        """Return the timer interval that corresponds to ``fft_update_hz``."""
        return _hz_to_interval_ms(self.fft_update_hz, fallback_ms=500)

    def normalized_time_window_s(self) -> float:
        """Clamp the time-domain window length to a safe, positive range."""
        try:
            window = float(self.time_window_seconds)
        except (TypeError, ValueError):
            window = 3.0
        if not math.isfinite(window) or window <= 0.5:
            return 2.0
        return min(10.0, window)

    def normalized_max_subplots(self) -> int:
        try:
            value = int(self.max_signal_subplots)
        except (TypeError, ValueError):
            value = 18
        return max(1, value)

    def normalized_max_lines(self) -> int:
        try:
            value = int(self.max_lines_per_subplot)
        except (TypeError, ValueError):
            value = 1
        return max(1, value)

    def normalized_max_points(self) -> int:
        try:
            value = int(self.signal_max_points_per_line)
        except (TypeError, ValueError):
            value = 2000
        return max(100, value)


@dataclass
class AppConfig:
    """In-memory configuration snapshot used for Pi sync and GUI runtime."""

    sensor_defaults: Dict[str, Any] = field(default_factory=dict)
    signal_backend: str = "pyqtgraph"
    plot_performance: PlotPerformanceConfig = field(
        default_factory=PlotPerformanceConfig
    )
    sampling_config: SamplingConfig = field(
        default_factory=lambda: SamplingConfig(device_rate_hz=200.0)
    )

    def normalized_signal_backend(self) -> str:
        """Return the canonical backend identifier (``pyqtgraph`` or ``matplotlib``)."""
        backend = str(self.signal_backend or "").strip().lower()
        if backend in {"matplotlib", "mpl"}:
            return "matplotlib"
        if backend in {"pyqtgraph", "pg", "pyqt"}:
            return "pyqtgraph"
        return "pyqtgraph"


@dataclass
class SensorDefaults:
    """
    Helper for loading/saving sensor defaults (sensors.yaml).

    The authoritative sampling configuration lives under the top-level
    ``sampling`` key. Per-sensor entries inherit their rates from that
    block and no longer carry ``sample_rate_hz`` fields, removing any
    ambiguity about the source of truth.
    """

    sensors_file: Path = AppPaths().config_dir / "sensors.yaml"

    def _normalize(
        self,
        data: Dict[str, Any],
        sampling: SamplingConfig | None = None,
    ) -> Dict[str, Any]:
        normalized = dict(data) if isinstance(data, dict) else {}

        sampling_cfg = sampling or SamplingConfig.from_mapping(normalized)
        sampling_block = sampling_cfg.to_mapping()["sampling"]

        sensors_block = normalized.get("sensors")
        cleaned_sensors: Dict[str, Any] = {}
        if isinstance(sensors_block, Mapping):
            for key, cfg in sensors_block.items():
                if isinstance(cfg, Mapping):
                    sensor_cfg = dict(cfg)
                    sensor_cfg.pop("sample_rate_hz", None)
                    cleaned_sensors[str(key)] = sensor_cfg
                else:
                    cleaned_sensors[str(key)] = cfg
        normalized["sampling"] = sampling_block
        normalized["sensors"] = cleaned_sensors
        normalized.pop("mpu6050", None)
        return normalized

    def load(self) -> Dict[str, Any]:
        """Load and return the full sensors.yaml mapping (or ``{}`` if missing)."""
        raw, sampling = load_sensor_defaults(self.sensors_file)
        return self._normalize(raw, sampling)

    def load_sampling_config(self, data: Dict[str, Any] | None = None) -> SamplingConfig:
        if data is None:
            _, sampling = load_sensor_defaults(self.sensors_file)
            return sampling
        return SamplingConfig.from_mapping(data)

    def save(self, data: Dict[str, Any]) -> None:
        """
        Write the given mapping back to ``sensors.yaml``.

        Callers are expected to start from :meth:`load` so that unknown keys
        are preserved.
        """
        sampling_cfg = SamplingConfig.from_mapping(data)
        normalized = self._normalize(data, sampling_cfg)
        save_sensor_defaults(self.sensors_file, normalized, sampling_cfg)

    # ------------------------------------------------------------------
    # Convenience helpers for RecorderTab / other callers
    # ------------------------------------------------------------------
    def build_mpu6050_cli_args(
        self,
        overrides: Mapping[str, Any] | None = None,
    ) -> List[str]:
        """
        Build CLI arguments for ``mpu6050_multi_logger.py`` from defaults.

        Parameters
        ----------
        overrides:
            Optional mapping with keys like ``sample_rate_hz``, ``channels``,
            ``dlpf`` or ``include_temperature``.  Any non-``None`` value in
            *overrides* replaces the default read from :mod:`sensors.yaml`.
        """
        config = self.load()
        sampling_cfg = SamplingConfig.from_mapping(config)
        sensors = config.get("sensors") or {}
        base = dict(sensors.get("mpu6050", {}) or {})
        base["sample_rate_hz"] = sampling_cfg.device_rate_hz
        if overrides:
            for key, value in overrides.items():
                if value is not None:
                    base[key] = value
        return build_mpu6050_cli_args(base)


@dataclass
class HostInventory:
    """
    Hosts and SSH defaults for Raspberry Pis, backed by ``hosts.yaml``.

    Expected structure (extra keys are allowed and preserved):

    .. code-block:: yaml

        pis:
          - name: lab-pi
            host: 192.168.0.6
            user: pi
            password: "hunter2"
            base_path: ~/sensor
            port: 22
    """

    hosts_file: Path = AppPaths().config_dir / "hosts.yaml"

    def load(self) -> Dict[str, Any]:
        """Load and return the raw mapping from ``hosts.yaml`` (or ``{}``)."""
        if not self.hosts_file.exists():
            return {}
        with self.hosts_file.open("r", encoding="utf-8") as fh:
            return yaml.safe_load(fh) or {}

    def save(self, data: Dict[str, Any]) -> None:
        """Write *data* back to ``hosts.yaml``."""
        self.hosts_file.parent.mkdir(parents=True, exist_ok=True)
        with self.hosts_file.open("w", encoding="utf-8") as fh:
            yaml.safe_dump(
                data,
                fh,
                default_flow_style=False,
                sort_keys=False,
            )

    def list_hosts(self) -> List[Dict[str, Any]]:
        """
        Return a list of host dictionaries from the ``pis`` key.

        Each entry is copied so callers can mutate safely.
        """
        data = self.load()
        pis = data.get("pis") or []
        return [dict(item) for item in pis]

    def save_hosts(self, hosts: Iterable[Mapping[str, Any]]) -> None:
        """
        Replace the ``pis`` list in ``hosts.yaml`` with *hosts*.

        All other top-level keys in the file are preserved.
        """
        existing = self.load()
        existing["pis"] = [dict(h) for h in hosts]
        self.save(existing)

    # ------------------------------------------------------------------
    # Helpers for turning config dicts into runtime objects
    # ------------------------------------------------------------------
    def scripts_dir_for(self, host_cfg: Mapping[str, Any]) -> Path:
        """Return the scripts/base path for a host, with ``~`` expanded."""
        raw = (
            host_cfg.get("base_path")
            or host_cfg.get("scripts_dir")
            or DEFAULT_BASE_PATH
        )
        return Path(str(raw)).expanduser()

    def to_host_config(self, host_cfg: Mapping[str, Any]) -> HostConfig:
        """Convert a host mapping from YAML into a normalized :class:`HostConfig`."""

        name = str(host_cfg.get("name", host_cfg.get("host", "pi")))
        host = str(host_cfg.get("host", "raspberrypi.local"))
        user = str(host_cfg.get("user", "pi"))
        port = int(host_cfg.get("port", 22))
        password = host_cfg.get("password")

        base_path = Path(str(host_cfg.get("base_path", DEFAULT_BASE_PATH))).expanduser()
        data_dir = Path(str(host_cfg.get("data_dir", DEFAULT_DATA_DIR))).expanduser()
        pi_cfg = Path(
            str(host_cfg.get("pi_config_path", base_path / "pi_config.yaml"))
        ).expanduser()

        return HostConfig(
            name=name,
            host=host,
            user=user,
            port=port,
            base_path=base_path,
            data_dir=data_dir,
            pi_config_path=pi_cfg,
            password=password,
        )

    def to_remote_host(self, host_cfg: Mapping[str, Any]):
        """
        Convert a host dict into :class:`sensepi.remote.ssh_client.Host`.

        This keeps parsing and ``~``-expansion in one place so GUI code
        can construct a ready-to-use SSH host without reimplementing the
        schema.
        """
        from ..remote.ssh_client import Host as RemoteHost

        cfg = self.to_host_config(host_cfg)
        return RemoteHost(
            name=cfg.name,
            host=cfg.host,
            user=cfg.user,
            password=cfg.password,
            port=cfg.port,
        )


# ---------------------------------------------------------------------------
# CLI argument builders for the Pi loggers
# ---------------------------------------------------------------------------

def build_mpu6050_cli_args(config: Mapping[str, Any]) -> List[str]:
    """
    Construct CLI args for ``mpu6050_multi_logger.py`` from a mapping.

    Expected keys in *config* (all optional except ``sample_rate_hz``):

    - ``sample_rate_hz`` (float / int)
    - ``channels`` (``acc``, ``gyro``, ``both``, or ``default``)
    - ``dlpf`` (0..6)
    - ``include_temperature`` (bool)
    """
    args: List[str] = []

    rate = config.get("sample_rate_hz")
    if rate is None:
        raise ValueError("mpu6050 defaults must include 'sample_rate_hz'")
    args.extend(["--rate", str(rate)])

    channels = config.get("channels")
    if channels:
        args.extend(["--channels", str(channels)])

    dlpf = config.get("dlpf")
    if dlpf is not None:
        args.extend(["--dlpf", str(dlpf)])

    if config.get("include_temperature"):
        args.append("--temp")

    return args

def build_pi_config_for_host(host_cfg: HostConfig, app_cfg: AppConfig) -> PiLoggerConfig:
    """
    Build the :class:`PiLoggerConfig` that will be written to ``pi_config.yaml``.

    Currently this mirrors only the MPU6050 logger configuration.
    """
    sensors = app_cfg.sensor_defaults or {}
    sampling_cfg = app_cfg.sampling_config
    if not isinstance(sampling_cfg, SamplingConfig):
        sampling_cfg = SamplingConfig.from_mapping(sensors)
    pi_cfg = PiLoggerConfig.from_sampling(sampling_cfg)

    sensor_defaults = sensors.get("sensors") or {}
    mpu_defaults = dict(sensor_defaults.get("mpu6050", {}) or {})
    sensors_list = mpu_defaults.get("sensors", [1, 2, 3])
    if isinstance(sensors_list, str):
        sensors_list = [s.strip() for s in sensors_list.split(",") if s.strip()]

    mpu_cfg = {
        "output_dir": str(host_cfg.data_dir / "mpu"),
        "sample_rate_hz": int(pi_cfg.device_rate_hz),
        "record_decimate": int(pi_cfg.record_decimate),
        "stream_every": int(pi_cfg.stream_decimate),
        "record_rate_hz": float(pi_cfg.record_rate_hz),
        "stream_rate_hz": float(pi_cfg.stream_rate_hz),
        "channels": str(mpu_defaults.get("channels", "default")),
        "dlpf": int(mpu_defaults.get("dlpf", 3)),
        "include_temperature": bool(mpu_defaults.get("include_temperature", False)),
        "sensors": sensors_list,
    }

    pi_cfg.sections = {
        "generated": "GENERATED FILE - DO NOT EDIT BY HAND",
        "mpu6050": mpu_cfg,
    }
    return pi_cfg


------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\constants.py
# File: constants.py (ext: .py
# Dir : src\sensepi\config\
# Size: 161 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\constants.py
"""Application-wide constants and feature flags."""

# Enable/disable lightweight FPS / latency instrumentation for plots.
ENABLE_PLOT_PERF_METRICS: bool = True

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\hosts.yaml
# File: hosts.yaml (ext: .yaml
# Dir : src\sensepi\config\
# Size: 415 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\hosts.yaml
# Sample host definitions for the desktop GUI.
# Update the host/user/password/path values to match your environment.
# Using "~/..." is supported and expands to each user's home directory.
pis:
  - name: Pi06
    host: 192.168.0.6
    user: verwalter
    password: "!66442200"
    base_path: "/home/verwalter/sensor"
    data_dir: "/home/verwalter/logs"
    pi_config_path: "/home/verwalter/sensor/pi_config.yaml"

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\log_paths.py
# File: log_paths.py (ext: .py
# Dir : src\sensepi\config\
# Size: 3793 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\log_paths.py
"""Shared helpers that codify the SensePi log directory conventions.

These helpers keep the Raspberry Pi recorders, offline sync tools, and
GUI in agreement about where logs are written and how files are named.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import datetime as _dt
import re

from .app_config import AppPaths as _AppPaths

# ---- Constants ---------------------------------------------------------

# The defaults mirror pi_config.yaml so the Pi loggers behave as before.
DEFAULT_PI_LOG_ROOT = Path("~").expanduser() / "logs"

# Match the desktop application's AppPaths raw-data directory. Fall back
# to a repo-relative path so imports succeed even if app_config breaks.
try:
    DEFAULT_PC_RAW_ROOT = _AppPaths().raw_data
except Exception:  # pragma: no cover - extremely defensive
    DEFAULT_PC_RAW_ROOT = Path("data") / "raw"

# Sensor-specific subdirectory names. These names appear on both Pi and PC.
LOG_SUBDIR_MPU = "mpu"


# ---- Session slugging --------------------------------------------------

def slugify_session_name(name: str) -> str:
    """Return a filesystem-safe slug for a user-provided session name."""

    normalized = name.strip().lower()
    normalized = re.sub(r"[^a-z0-9]+", "-", normalized)
    normalized = normalized.strip("-")
    return normalized or "session"


# ---- Pi-side helpers ---------------------------------------------------

def build_pi_session_dir(
    sensor_prefix: str,
    session_name: Optional[str],
    base_dir: Path | str | None = None,
) -> Path:
    """Return the directory on the Pi where this session's logs will be stored."""

    root = Path(base_dir).expanduser() if base_dir is not None else DEFAULT_PI_LOG_ROOT
    sensor_root = root / sensor_prefix

    if not session_name:
        return sensor_root

    slug = slugify_session_name(session_name)
    return sensor_root / slug


# ---- PC-side helpers ---------------------------------------------------

def build_pc_session_root(
    raw_root: Path,
    host_slug: str,
    session_name: Optional[str],
    sensor_prefix: str,
) -> Path:
    """Return the local root directory for logs downloaded from the Pi."""

    raw_root = Path(raw_root)

    if session_name:
        session_slug = slugify_session_name(session_name)
        return raw_root / session_slug

    slug = slugify_session_name(host_slug or "host")
    sensor_prefix = sensor_prefix.strip("/ ")
    if sensor_prefix:
        return raw_root / slug / sensor_prefix
    return raw_root / slug


# ---- Filename helpers --------------------------------------------------

def _format_start_ts(start_dt: _dt.datetime) -> str:
    """Return the canonical timestamp string used in log filenames."""

    return start_dt.strftime("%Y-%m-%d_%H-%M-%S")


@dataclass(frozen=True)
class LogFilePaths:
    """Container with the generated data path and .meta.json sidecar path."""

    data_path: Path
    meta_path: Path


def build_log_file_paths(
    sensor_prefix: str,
    session_name: Optional[str],
    sensor_id: int,
    start_dt: _dt.datetime,
    format_ext: str,
    out_dir: Path,
) -> LogFilePaths:
    """Return full paths for a log file and its metadata companion."""

    ts_str = _format_start_ts(start_dt)
    session_slug = slugify_session_name(session_name) if session_name else ""

    if session_slug:
        stem = f"{session_slug}_{sensor_prefix}_S{sensor_id}_{ts_str}"
    else:
        stem = f"{sensor_prefix}_S{sensor_id}_{ts_str}"

    format_ext = format_ext.lstrip(".")
    filename = f"{stem}.{format_ext}"

    data_path = out_dir / filename
    meta_path = data_path.with_suffix(data_path.suffix + ".meta.json")
    return LogFilePaths(data_path=data_path, meta_path=meta_path)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\pi_logger_config.py
# File: pi_logger_config.py (ext: .py
# Dir : src\sensepi\config\
# Size: 3862 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\pi_logger_config.py
"""Helpers for configuring the Raspberry Pi logger."""
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List

from .sampling import SamplingConfig


@dataclass
class PiLoggerConfig:
    """
    Concrete configuration for the Pi logger derived from SamplingConfig.
    """

    device_rate_hz: float
    record_decimate: int
    stream_decimate: int
    record_rate_hz: float
    stream_rate_hz: float
    sections: Dict[str, Any] | None = None
    logger_script: str = "mpu6050_multi_logger.py"
    extra_cli: Dict[str, Any] | None = None

    @classmethod
    def from_sampling(cls, sampling: SamplingConfig, **kwargs: Any) -> "PiLoggerConfig":
        """
        Construct a PiLoggerConfig from the SamplingConfig source of truth.
        """

        decimation = sampling.compute_decimation()
        return cls(
            device_rate_hz=float(sampling.device_rate_hz),
            record_decimate=int(decimation["record_decimate"]),
            stream_decimate=int(decimation["stream_decimate"]),
            record_rate_hz=float(decimation["record_rate_hz"]),
            stream_rate_hz=float(decimation["stream_rate_hz"]),
            **kwargs,
        )

    # ------------------------------------------------------------------ serialization
    def to_pi_config_dict(self) -> Dict[str, Any]:
        """
        Convert into a mapping suitable for YAML serialization.
        """

        data: Dict[str, Any] = {
            "device_rate_hz": float(self.device_rate_hz),
            "record_decimate": int(self.record_decimate),
            "stream_decimate": int(self.stream_decimate),
            "record_rate_hz": float(self.record_rate_hz),
            "stream_rate_hz": float(self.stream_rate_hz),
        }
        extra_sections = self.sections or {}
        for key, value in extra_sections.items():
            data[key] = value
        return data

    def render_pi_config_yaml(self) -> str:
        """
        Return the generated pi_config.yaml text with the DO NOT EDIT header.
        """

        from textwrap import dedent

        import yaml

        header = dedent(
            """
            # GENERATED FILE - DO NOT EDIT BY HAND
            # This file is derived from SamplingConfig and PiLoggerConfig.
            """
        ).strip("\n")
        body = yaml.safe_dump(self.to_pi_config_dict(), sort_keys=False)
        return header + "\n\n" + body

    def write_pi_config_yaml(self, path: Path) -> None:
        """
        Write the generated configuration to *path*.
        """

        path = Path(path)
        if path.parent and not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(self.render_pi_config_yaml())


def _format_extra_flags(extra: Dict[str, Any] | None) -> List[str]:
    if not extra:
        return []

    flags: List[str] = []
    for key, value in extra.items():
        flag = f"--{str(key).replace('_', '-')}"
        if isinstance(value, bool):
            if value:
                flags.append(flag)
        elif value is not None:
            flags.extend([flag, str(value)])
    return flags


def build_logger_args(pi_cfg: PiLoggerConfig) -> List[str]:
    """Build argument list for ``mpu6050_multi_logger.py`` using ``pi_cfg``."""

    args: List[str] = [
        "--sample-rate-hz",
        f"{pi_cfg.device_rate_hz:.0f}",
        "--stream-every",
        str(int(pi_cfg.stream_decimate)),
    ]

    args.extend(_format_extra_flags(pi_cfg.extra_cli))
    return args


def build_logger_command(pi_cfg: PiLoggerConfig) -> List[str]:
    """
    Construct the command used to start the Pi logger process.
    """

    parts: List[str] = ["python3", "-u", pi_cfg.logger_script]
    parts.extend(build_logger_args(pi_cfg))
    return parts

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\runtime.py
# File: runtime.py (ext: .py
# Dir : src\sensepi\config\
# Size: 3623 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\runtime.py
"""Runtime configuration helpers for the ingestion/plotting pipeline."""

from __future__ import annotations

from dataclasses import dataclass, fields
from pathlib import Path
from typing import Any, Mapping, MutableMapping

import yaml


@dataclass(slots=True)
class SensePiConfig:
    """
    Tuning knobs for how samples are recorded, streamed, and visualized.

    The defaults assume ~500 Hz sensors feeding a ~50 Hz UI and network stream.
    """

    sensor_fs: float = 500.0
    recording_enabled: bool = True
    recording_chunk_seconds: float = 1.0

    streaming_enabled: bool = True
    stream_fs: float = 50.0

    plotting_enabled: bool = True
    plot_fs: float = 50.0
    plot_window_seconds: float = 10.0
    smoothing_alpha: float = 0.2
    use_envelope: bool = True
    spike_threshold: float = 0.5

    # Thread bridge sizing
    stream_queue_size: int = 8
    plot_queue_size: int = 8

    def sanitized(self) -> SensePiConfig:
        """Return a copy with derived limits applied."""
        alpha = self.smoothing_alpha
        if alpha is not None:
            alpha = max(1e-6, min(1.0, float(alpha)))
        return SensePiConfig(
            sensor_fs=max(1.0, float(self.sensor_fs)),
            recording_enabled=bool(self.recording_enabled),
            recording_chunk_seconds=max(0.01, float(self.recording_chunk_seconds)),
            streaming_enabled=bool(self.streaming_enabled),
            stream_fs=max(1.0, float(self.stream_fs)),
            plotting_enabled=bool(self.plotting_enabled),
            plot_fs=max(1.0, float(self.plot_fs)),
            plot_window_seconds=max(0.5, float(self.plot_window_seconds)),
            smoothing_alpha=alpha,
            use_envelope=bool(self.use_envelope),
            spike_threshold=float(self.spike_threshold),
            stream_queue_size=max(1, int(self.stream_queue_size)),
            plot_queue_size=max(1, int(self.plot_queue_size)),
        )


def _recognized_fields() -> set[str]:
    """Return the dataclass field names accepted by :class:`SensePiConfig`."""
    return {f.name for f in fields(SensePiConfig)}


def _normalize_mapping(data: Mapping[str, Any]) -> MutableMapping[str, Any]:
    """Flatten known nesting patterns (e.g. top-level ``pipeline`` key)."""
    if "pipeline" in data and isinstance(data["pipeline"], Mapping):
        merged: MutableMapping[str, Any] = {}
        for key, value in data.items():
            if key == "pipeline":
                merged.update(value)
            else:
                merged[key] = value
        return merged
    return dict(data)


def config_from_mapping(data: Mapping[str, Any] | None) -> SensePiConfig:
    """Build :class:`SensePiConfig` from ``data`` (ignoring unknown keys)."""
    if not data:
        return SensePiConfig()
    normalized = _normalize_mapping(data)
    known = _recognized_fields()
    payload = {key: normalized[key] for key in normalized.keys() & known}
    return SensePiConfig(**payload).sanitized()


def load_config(path: str | Path | None) -> SensePiConfig:
    """
    Load configuration from ``path``.

    Missing files fall back to default :class:`SensePiConfig`.
    """
    if path is None:
        return SensePiConfig()
    cfg_path = Path(path)
    if not cfg_path.exists():
        return SensePiConfig()
    with cfg_path.open("r", encoding="utf-8") as fh:
        raw = yaml.safe_load(fh) or {}
    if not isinstance(raw, Mapping):
        raise ValueError(f"Expected mapping in {cfg_path}, got {type(raw).__name__}")
    return config_from_mapping(raw)


__all__ = ["SensePiConfig", "config_from_mapping", "load_config"]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\sampling.py
# File: sampling.py (ext: .py
# Dir : src\sensepi\config\
# Size: 4930 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\sampling.py
"""Unified sampling configuration and helpers."""
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Mapping, Optional


@dataclass(frozen=True)
class RecordingMode:
    key: str
    label: str
    target_record_hz: Optional[float]  # None = "raw" (no decimation)
    target_stream_hz: float  # desired GUI stream rate


RECORDING_MODES: Dict[str, RecordingMode] = {
    "low_fidelity": RecordingMode(
        key="low_fidelity",
        label="Low fidelity (25 Hz)",
        target_record_hz=25.0,
        target_stream_hz=25.0,
    ),
    "high_fidelity": RecordingMode(
        key="high_fidelity",
        label="High fidelity (50 Hz)",
        target_record_hz=50.0,
        target_stream_hz=25.0,
    ),
    "raw": RecordingMode(
        key="raw",
        label="Raw (device rate)",
        target_record_hz=None,  # means record at device_rate_hz
        target_stream_hz=25.0,
    ),
}


@dataclass
class SamplingConfig:
    """
    Single source of truth for sampling.

    device_rate_hz: what the sensor is *actually* sampled at on the Pi.
    mode_key: selects a RecordingMode from RECORDING_MODES.
    """

    device_rate_hz: float
    mode_key: str = "high_fidelity"

    @property
    def mode(self) -> RecordingMode:
        """Return the resolved recording mode (defaults to high_fidelity)."""
        return RECORDING_MODES.get(self.mode_key, RECORDING_MODES["high_fidelity"])

    def compute_decimation(self) -> dict:
        """
        Compute integer decimation factors and resulting effective rates
        for recording and streaming, based on device_rate_hz + mode.
        """

        mode = self.mode

        # Recording decimation
        if mode.target_record_hz is None:  # raw mode
            record_decimate = 1
        else:
            record_decimate = max(1, round(self.device_rate_hz / mode.target_record_hz))
        record_rate_hz = self.device_rate_hz / record_decimate

        # GUI stream decimation
        stream_decimate = max(1, round(self.device_rate_hz / mode.target_stream_hz))
        stream_rate_hz = self.device_rate_hz / stream_decimate

        return {
            "record_decimate": record_decimate,
            "record_rate_hz": record_rate_hz,
            "stream_decimate": stream_decimate,
            "stream_rate_hz": stream_rate_hz,
        }

    @classmethod
    def from_mapping(
        cls,
        mapping: Mapping[str, Any] | None,
        *,
        default_device_rate: float = 200.0,
        default_mode: str = "high_fidelity",
    ) -> "SamplingConfig":
        """
        Construct a SamplingConfig from a mapping such as sensors.yaml.

        Supported shape::

            sampling:
              device_rate_hz: 200
              mode: high_fidelity
        """
        sampling_block = mapping.get("sampling") if isinstance(mapping, Mapping) else None
        device_rate = default_device_rate
        mode_key = default_mode

        if isinstance(sampling_block, Mapping):
            device_rate = sampling_block.get("device_rate_hz", device_rate)  # type: ignore[arg-type]
            mode_key = sampling_block.get("mode", mode_key)  # type: ignore[arg-type]

        # legacy fallback: look for a per-sensor sample rate if the sampling block is
        # missing. This smooths upgrades from the old sensors.yaml structure.
        sensors = mapping.get("sensors") if isinstance(mapping, Mapping) else None
        if isinstance(sensors, Mapping) and not isinstance(sampling_block, Mapping):
            mpu_cfg = sensors.get("mpu6050") if isinstance(sensors.get("mpu6050"), Mapping) else None
            if isinstance(mpu_cfg, Mapping):
                device_rate = mpu_cfg.get("sample_rate_hz", device_rate)  # type: ignore[arg-type]

        try:
            rate = float(device_rate)
        except (TypeError, ValueError):
            rate = float(default_device_rate)

        mode_key_str = str(mode_key or default_mode)
        if mode_key_str not in RECORDING_MODES:
            mode_key_str = default_mode
        return cls(device_rate_hz=rate, mode_key=mode_key_str)

    def to_mapping(self) -> dict:
        """
        Serialize the sampling config back into a mapping suitable for YAML.
        """
        return {
            "sampling": {
                "device_rate_hz": float(self.device_rate_hz),
                "mode": self.mode.key,
            }
        }


@dataclass
class GuiSamplingDisplay:
    device_rate_hz: float
    record_rate_hz: float
    stream_rate_hz: float
    mode_label: str

    @classmethod
    def from_sampling(cls, sampling: SamplingConfig) -> "GuiSamplingDisplay":
        dec = sampling.compute_decimation()
        return cls(
            device_rate_hz=sampling.device_rate_hz,
            record_rate_hz=dec["record_rate_hz"],
            stream_rate_hz=dec["stream_rate_hz"],
            mode_label=sampling.mode.label,
        )

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\config\sensors.yaml
# File: sensors.yaml (ext: .yaml
# Dir : src\sensepi\config\
# Size: 258 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\config\sensors.yaml
# Default sensor + sampling configuration
# The authoritative sample rate lives under the top-level `sampling` block.
sampling:
  device_rate_hz: 200
  mode: high_fidelity

sensors:
  mpu6050:
    channels: default
    dlpf: 3
    include_temperature: false

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\core\
# Size: 931 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\__init__.py
"""Core streaming pipeline: sessions, buffers, and data flow.

This package sits between remote ingest and the GUI by coordinating recorder
sessions, in-memory buffers, and the fan-out pipeline that feeds plots,
streaming sockets, and disk writers.
"""

# Data structures shared by the pipeline (historically imported from here)
from .ringbuffer import RingBuffer
from .timeseries_buffer import TimeSeriesBuffer

# High-level controllers and wiring helpers
from .pipeline import (
    NullSink,
    Pipeline,
    PlotUpdate,
    Plotter,
    Recorder,
    SampleSink,
    Streamer,
)
from .pipeline_wiring import PipelineHandles, build_pipeline
from .recorder_session import RecorderSession

__all__ = [
    "RingBuffer",
    "TimeSeriesBuffer",
    "Pipeline",
    "Plotter",
    "Recorder",
    "Streamer",
    "PlotUpdate",
    "NullSink",
    "SampleSink",
    "PipelineHandles",
    "build_pipeline",
    "RecorderSession",
]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\live_stream.py
# File: live_stream.py (ext: .py
# Dir : src\sensepi\core\
# Size: 922 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\live_stream.py
"""Helpers for consuming live data from remote loggers."""

from typing import Any, Callable, Iterable

from ..sensors.mpu6050 import parse_line as parse_mpu


def stream_lines(
    lines: Iterable[str],
    parser: Callable[[str], Any],
    callback: Callable[[Any], None],
) -> None:
    """Parse incoming lines and forward decoded samples to a callback."""
    for line in lines:
        if not line:
            continue
        sample = parser(line)
        if sample is None:
            continue
        callback(sample)


def select_parser(sensor_type: str) -> Callable[[str], Any]:
    """
    Select the appropriate line parser for a given sensor type name.

    Currently only the MPU6050 logger is supported.
    """
    st = sensor_type.strip().lower()
    if st in {"mpu6050", "mpu-6050"}:
        return parse_mpu
    raise ValueError(f"Unknown sensor_type {sensor_type!r}")

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\models.py
# File: models.py (ext: .py
# Dir : src\sensepi\core\
# Size: 455 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\models.py
"""Shared dataclasses for SensePi sessions and samples."""

from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional


@dataclass
class SessionInfo:
    name: str
    sensor_type: str
    sample_rate_hz: float
    started_at: datetime
    output_path: Path


@dataclass
class LiveSample:
    timestamp_ns: int
    values: tuple[float, ...]
    sensor: Optional[str] = None

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\pipeline.py
# File: pipeline.py (ext: .py
# Dir : src\sensepi\core\
# Size: 9002 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\pipeline.py
"""Sensor sample fan-out pipeline for recording, streaming, and plotting."""

from __future__ import annotations

from dataclasses import dataclass, field
import threading
from typing import Callable, Optional, Protocol, Sequence, Tuple
from queue import Empty, Full, Queue

import numpy as np

from decimation import DecimationConfig, Decimator

__all__ = [
    "SampleSink",
    "Recorder",
    "Streamer",
    "PlotUpdate",
    "Plotter",
    "Pipeline",
    "NullSink",
]

SampleArray = np.ndarray
StreamPayload = Tuple[np.ndarray, np.ndarray]


class SampleSink(Protocol):
    """Common interface implemented by Recorder/Streamer/Plotter."""

    def handle_samples(self, t: np.ndarray, x: np.ndarray) -> None:  # pragma: no cover - protocol
        ...


class SampleBlockWriter(Protocol):
    """Protocol for recorder backends."""

    def write_samples(self, t: np.ndarray, x: np.ndarray) -> None:  # pragma: no cover - protocol
        ...


def _call_writer(writer: SampleBlockWriter | Callable[[np.ndarray, np.ndarray], None], t: np.ndarray, x: np.ndarray) -> None:
    if hasattr(writer, "write_samples"):
        writer.write_samples(t, x)  # type: ignore[attr-defined]
    else:
        writer(t, x)


def _offer_queue(queue: Queue, item: object) -> None:
    """Best-effort put used for GUI-facing queues.

    When the queue is full we drop the oldest item and keep the newest one
    instead, which keeps the UI responsive under back-pressure.
    """
    try:
        queue.put_nowait(item)
    except Full:
        try:
            queue.get_nowait()
        except Empty:
            pass
        queue.put_nowait(item)


@dataclass(slots=True)
class Recorder(SampleSink):
    """Stores raw samples to disk or any callable writer."""

    writer: SampleBlockWriter | Callable[[np.ndarray, np.ndarray], None]
    sensor_fs: float
    chunk_seconds: float = 1.0
    copy_blocks: bool = True

    _chunk_size: int = field(init=False)

    def __post_init__(self) -> None:
        if self.sensor_fs <= 0.0:
            raise ValueError("sensor_fs must be positive.")
        self._chunk_size = max(1, int(round(max(0.001, float(self.chunk_seconds)) * self.sensor_fs)))

    def handle_samples(self, t: np.ndarray, x: np.ndarray) -> None:
        times = np.asarray(t, dtype=np.float64).reshape(-1)
        values = np.asarray(x).reshape(-1)
        if times.size != values.size:
            raise ValueError("timestamps and samples must have the same length.")
        if times.size == 0:
            return
        chunk = self._chunk_size
        start = 0
        while start < times.size:
            end = min(times.size, start + chunk)
            t_view = times[start:end]
            x_view = values[start:end]
            if self.copy_blocks:
                t_view = np.array(t_view, copy=True)
                x_view = np.array(x_view, copy=True)
            _call_writer(self.writer, t_view, x_view)
            start = end


@dataclass(slots=True)
class Streamer(SampleSink):
    """Decimates and sends samples over the network."""

    sensor_fs: float
    stream_fs: float
    transport: Callable[[np.ndarray, np.ndarray], None] | None = None
    queue: Queue[StreamPayload] | None = None

    def __post_init__(self) -> None:
        if self.sensor_fs <= 0.0 or self.stream_fs <= 0.0:
            raise ValueError("sensor_fs and stream_fs must be positive.")
        cfg = DecimationConfig(
            sensor_fs=self.sensor_fs,
            plot_fs=self.stream_fs,
            use_envelope=False,
            smoothing_alpha=None,
        )
        self._decimator = Decimator(cfg)

    def handle_samples(self, t: np.ndarray, x: np.ndarray) -> None:
        if x is None or t is None:
            return
        values = np.asarray(x, dtype=np.float32).reshape(-1)
        times = np.asarray(t, dtype=np.float64).reshape(-1)
        if values.size == 0 or times.size == 0:
            return
        start_time = float(times[0])
        # Convert the high-rate sensor stream into a lighter, decimated stream
        # before shipping it across the network or into the GUI queue.
        t_dec, y_mean, _, _ = self._decimator.process_block(values, start_time=start_time)
        if t_dec.size == 0:
            return
        payload = (t_dec, y_mean)
        # The same decimated payload can be sent immediately over the transport
        # and/or handed to a queue that another thread (e.g. Qt) will drain.
        if self.transport is not None:
            self.transport(t_dec, y_mean)
        if self.queue is not None:
            _offer_queue(self.queue, payload)


@dataclass(slots=True)
class PlotUpdate:
    """Decimated plot data (mean/envelope/spikes) for the Signals tab."""

    timestamps: np.ndarray
    mean: np.ndarray
    y_min: Optional[np.ndarray] = None
    y_max: Optional[np.ndarray] = None
    spike_mask: Optional[np.ndarray] = None


@dataclass(slots=True)
class Plotter(SampleSink):
    """Prepares decimated/smoothed data for live plotting."""

    sensor_fs: float
    plot_fs: float
    smoothing_alpha: Optional[float] = 0.2
    use_envelope: bool = True
    spike_threshold: float = 0.5
    queue: Queue[PlotUpdate] | None = None

    _latest_update: Optional[PlotUpdate] = field(init=False, default=None, repr=False)
    _lock: threading.Lock = field(init=False, default_factory=threading.Lock, repr=False)

    def __post_init__(self) -> None:
        if self.sensor_fs <= 0.0 or self.plot_fs <= 0.0:
            raise ValueError("sensor_fs and plot_fs must be positive.")
        cfg = DecimationConfig(
            sensor_fs=self.sensor_fs,
            plot_fs=self.plot_fs,
            use_envelope=self.use_envelope,
            smoothing_alpha=self.smoothing_alpha,
        )
        self._decimator = Decimator(cfg)

    def handle_samples(self, t: np.ndarray, x: np.ndarray) -> None:
        if x is None or t is None:
            return
        values = np.asarray(x, dtype=np.float32).reshape(-1)
        times = np.asarray(t, dtype=np.float64).reshape(-1)
        if values.size == 0 or times.size == 0:
            return
        # Second decimation stage: compress the raw stream into a small
        # mean/envelope representation that the GUI can draw every refresh.
        t_dec, y_mean, y_min, y_max = self._decimator.process_block(values, start_time=float(times[0]))
        if t_dec.size == 0:
            return
        update = PlotUpdate(
            timestamps=t_dec,
            mean=y_mean,
            y_min=y_min,
            y_max=y_max,
            spike_mask=self._compute_spike_mask(y_mean, y_max),
        )
        with self._lock:
            self._latest_update = update
        if self.queue is not None:
            _offer_queue(self.queue, update)

    def latest_update(self) -> Optional[PlotUpdate]:
        with self._lock:
            return self._latest_update

    def drain_queue(self) -> list[PlotUpdate]:
        if self.queue is None:
            return []
        items: list[PlotUpdate] = []
        while True:
            try:
                items.append(self.queue.get_nowait())
            except Empty:
                break
        if items:
            with self._lock:
                self._latest_update = items[-1]
        return items

    def _compute_spike_mask(self, y_mean: np.ndarray, y_max: Optional[np.ndarray]) -> Optional[np.ndarray]:
        if y_max is None:
            return None
        threshold = float(self.spike_threshold)
        if threshold <= 0:
            return None
        diff = y_max - y_mean
        return diff > threshold


@dataclass(slots=True)
class NullSink(SampleSink):
    """No-op sink used when a pipeline stage is disabled."""

    def handle_samples(self, t: np.ndarray, x: np.ndarray) -> None:  # pragma: no cover - trivial
        return


@dataclass(slots=True)
class Pipeline:
    """Fan out raw samples to recorder/streamer/plotter sinks."""

    recorder: SampleSink = field(default_factory=NullSink)
    streamer: SampleSink = field(default_factory=NullSink)
    plotter: SampleSink = field(default_factory=NullSink)

    def handle_samples(self, t: Sequence[float] | np.ndarray, x: Sequence[float] | np.ndarray) -> None:
        times = np.asarray(t, dtype=np.float64).reshape(-1)
        values = np.asarray(x).reshape(-1)
        if times.size != values.size:
            raise ValueError("t and x must have the same number of elements.")
        # Fan out the same block of samples to each sink; each sink can apply
        # its own decimation or buffering policy (recording, streaming, plotting).
        self.recorder.handle_samples(times, values)
        self.streamer.handle_samples(times, values)
        self.plotter.handle_samples(times, values)

    def on_new_sample(self, timestamp: float, value: float) -> None:
        """Append a single sample (convenience helper)."""
        t_arr = np.asarray([timestamp], dtype=np.float64)
        x_arr = np.asarray([value])
        self.handle_samples(t_arr, x_arr)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\pipeline_wiring.py
# File: pipeline_wiring.py (ext: .py
# Dir : src\sensepi\core\
# Size: 3704 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\pipeline_wiring.py
"""Factory helpers that wire a :class:`Pipeline` from configuration."""

from __future__ import annotations

from dataclasses import dataclass
from queue import Queue
from typing import Callable, Optional

import numpy as np

from ..config import SensePiConfig
from .pipeline import PlotUpdate, Plotter, Recorder, StreamPayload, Streamer, NullSink, Pipeline, SampleBlockWriter


RecorderWriter = SampleBlockWriter | Callable[[np.ndarray, np.ndarray], None]
TransportFn = Callable[[np.ndarray, np.ndarray], None]


@dataclass(slots=True)
class PipelineHandles:
    """Return value from :func:`build_pipeline` containing ready-to-use pieces."""

    pipeline: Pipeline
    # Queue drained by the networking / Qt layer for live streaming (optional).
    stream_queue: Queue[StreamPayload] | None = None
    # Queue drained by the plotting layer to update live plots (optional).
    plot_queue: Queue[PlotUpdate] | None = None


def build_pipeline(
    cfg: SensePiConfig,
    *,
    recorder_writer: Optional[RecorderWriter] = None,
    stream_transport: Optional[TransportFn] = None,
    stream_queue: Queue[StreamPayload] | None = None,
    plot_queue: Queue[PlotUpdate] | None = None,
) -> PipelineHandles:
    """Construct a :class:`Pipeline` that fans out raw samples to the configured sinks.

    Recording sees the full device sample rate, while streaming and plotting
    use decimated views to keep network and GUI load manageable.

    Parameters
    ----------
    cfg:
        Runtime configuration (usually loaded from YAML).
    recorder_writer:
        Callable that persists raw samples. When omitted, the recorder becomes a
        :class:`NullSink`, even if recording is enabled in the config.
    stream_transport:
        Function invoked with decimated stream data (e.g. network sender).
    stream_queue:
        Optional :class:`queue.Queue` for streaming payloads. When ``None`` and
        streaming is enabled, a bounded queue sized via ``cfg.stream_queue_size``
        is created automatically.
    plot_queue:
        Optional :class:`queue.Queue`` receiving :class:`PlotUpdate` objects.
        Created automatically using ``cfg.plot_queue_size`` when omitted.
    """

    normalized = cfg.sanitized()

    # Recorder
    if normalized.recording_enabled and recorder_writer is not None:
        recorder = Recorder(
            writer=recorder_writer,
            sensor_fs=normalized.sensor_fs,
            chunk_seconds=normalized.recording_chunk_seconds,
        )
    else:
        recorder = NullSink()

    # Streamer
    if normalized.streaming_enabled:
        stream_queue = stream_queue or Queue(maxsize=normalized.stream_queue_size)
        streamer = Streamer(
            sensor_fs=normalized.sensor_fs,
            stream_fs=normalized.stream_fs,
            transport=stream_transport,
            queue=stream_queue,
        )
    else:
        stream_queue = None
        streamer = NullSink()

    # Plotter
    if normalized.plotting_enabled:
        plot_queue = plot_queue or Queue(maxsize=normalized.plot_queue_size)
        plotter = Plotter(
            sensor_fs=normalized.sensor_fs,
            plot_fs=normalized.plot_fs,
            smoothing_alpha=normalized.smoothing_alpha,
            use_envelope=normalized.use_envelope,
            spike_threshold=normalized.spike_threshold,
            queue=plot_queue,
        )
    else:
        plot_queue = None
        plotter = NullSink()

    pipeline = Pipeline(recorder=recorder, streamer=streamer, plotter=plotter)
    return PipelineHandles(pipeline=pipeline, stream_queue=stream_queue, plot_queue=plot_queue)


__all__ = ["PipelineHandles", "build_pipeline", "RecorderWriter", "TransportFn"]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\recorder_session.py
# File: recorder_session.py (ext: .py
# Dir : src\sensepi\core\
# Size: 1596 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\recorder_session.py
from __future__ import annotations

"""Coordinator for running remote loggers and writing data locally."""

from datetime import datetime
from pathlib import Path
from typing import Iterable, Sequence

from ..dataio import csv_writer, file_paths
from ..remote.pi_recorder import PiRecorder
from .models import SessionInfo


class RecorderSession:
    def __init__(
        self,
        pi_recorder: PiRecorder,
        session_name: str,
        sensor_type: str,
        sample_rate_hz: float,
    ) -> None:
        self.pi_recorder = pi_recorder
        self.session_name = session_name
        self.sensor_type = sensor_type
        self.sample_rate_hz = sample_rate_hz
        self.session_dir: Path = file_paths.session_directory(session_name)
        self.meta = SessionInfo(
            name=session_name,
            sensor_type=sensor_type,
            sample_rate_hz=sample_rate_hz,
            started_at=datetime.utcnow(),
            output_path=self.session_dir,
        )

    def start(self, script: str, args: Iterable[str] | None = None) -> int:
        """
        Start a remote logger script via :class:`PiRecorder`.

        Returns the PID of the started remote process.
        """
        self.session_dir.mkdir(parents=True, exist_ok=True)
        return self.pi_recorder.start_logger(script, args)

    def save_rows(
        self,
        filename: str,
        headers: Sequence[str],
        rows: Iterable[Sequence[float]],
    ) -> None:
        csv_writer.write_rows(self.session_dir / filename, headers, rows)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\ringbuffer.py
# File: ringbuffer.py (ext: .py
# Dir : src\sensepi\core\
# Size: 1829 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\ringbuffer.py
from __future__ import annotations

from collections.abc import Iterable
from typing import Generic, TypeVar

T = TypeVar("T")


class RingBuffer(Generic[T]):
    """
    Fixed-size ring buffer for streaming data.
    Overwrites the oldest entries when full.
    """

    def __init__(self, capacity: int) -> None:
        if capacity <= 0:
            raise ValueError("capacity must be positive")
        self._capacity = capacity
        self._data: list[T | None] = [None] * capacity
        self._start = 0
        self._size = 0

    def append(self, item: T) -> None:
        idx = (self._start + self._size) % self._capacity
        self._data[idx] = item
        if self._size < self._capacity:
            self._size += 1
        else:
            self._start = (self._start + 1) % self._capacity

    def clear(self) -> None:
        self._data = [None] * self._capacity
        self._start = 0
        self._size = 0

    def __len__(self) -> int:  # pragma: no cover - trivial
        return self._size

    def __getitem__(self, index: int) -> T:
        """Support buf[i] and buf[-1] indexing over the logical contents (0 = oldest)."""
        size = self._size
        if size == 0:
            raise IndexError("RingBuffer is empty")

        if index < 0:
            index += size

        if index < 0 or index >= size:
            raise IndexError("RingBuffer index out of range")

        physical = (self._start + index) % self._capacity
        item = self._data[physical]
        assert item is not None
        return item

    def __iter__(self) -> Iterable[T]:
        for i in range(self._size):
            idx = (self._start + i) % self._capacity
            item = self._data[idx]
            if item is not None:
                yield item

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\stream_reader.py
# File: stream_reader.py (ext: .py
# Dir : src\sensepi\core\
# Size: 7471 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\stream_reader.py
from __future__ import annotations

"""
Utilities for ingesting JSONL sensor streams and dispatching them into
thread-safe channel ring buffers that the GUI can consume.
"""

import json
import logging
import threading
from collections.abc import Iterable, Mapping
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from .ringbuffer import RingBuffer

logger = logging.getLogger(__name__)

DEFAULT_RINGBUFFER_CAPACITY = 5000
_SKIP_FIELDS = {
    "sensor_id",
    "t_s",
    "timestamp_ns",
    "timestamp",
    "ts",
}

Number = float
ChannelName = str
SensorId = str
BufferKey = Tuple[SensorId, ChannelName]
ChannelSample = Tuple[Number, Number]


class ChannelBuffer:
    """Ring buffer plus lock for one channel of streaming data.

    The RLock allows a producer thread to append samples while consumer
    threads take snapshots without corrupting the underlying RingBuffer.
    """

    def __init__(self, capacity: int = DEFAULT_RINGBUFFER_CAPACITY) -> None:
        self._buffer: RingBuffer[ChannelSample] = RingBuffer(capacity)
        self._lock = threading.RLock()

    def append(self, timestamp: Number, value: Number) -> None:
        """Append a new sample (timestamp, value)."""
        with self._lock:
            self._buffer.append((float(timestamp), float(value)))

    def snapshot(self) -> List[ChannelSample]:
        """Return a thread-safe copy of the logical contents for read-only use."""
        with self._lock:
            return list(self._buffer)

    def __len__(self) -> int:
        with self._lock:
            return len(self._buffer)

    def latest(self) -> Optional[ChannelSample]:
        """Return the newest sample, or ``None`` if the buffer is empty."""
        with self._lock:
            if len(self._buffer) == 0:
                return None
            return self._buffer[-1]


class ChannelBufferStore:
    """Mapping of (sensor_id, channel) -> ChannelBuffer used by the stream reader.

    A single writer thread appends samples, while readers take snapshots
    for plotting or analysis without blocking the ingest loop.
    """

    def __init__(self, capacity: int = DEFAULT_RINGBUFFER_CAPACITY) -> None:
        self._capacity = capacity
        self._buffers: Dict[BufferKey, ChannelBuffer] = {}
        self._lock = threading.RLock()

    def append(self, sensor_id: SensorId, channel: ChannelName, timestamp: Number, value: Number) -> None:
        buf = self.get_or_create(sensor_id, channel)
        buf.append(timestamp, value)

    def get_or_create(self, sensor_id: SensorId, channel: ChannelName) -> ChannelBuffer:
        key = (sensor_id, channel)
        with self._lock:
            # Lazily create per-channel buffers so only channels that actually
            # appear in the stream consume memory.
            buf = self._buffers.get(key)
            if buf is None:
                buf = ChannelBuffer(self._capacity)
                self._buffers[key] = buf
            return buf

    def get(self, sensor_id: SensorId, channel: ChannelName) -> Optional[ChannelBuffer]:
        with self._lock:
            return self._buffers.get((sensor_id, channel))

    def items(self) -> List[Tuple[BufferKey, ChannelBuffer]]:
        """Return a snapshot list of (key, buffer) pairs."""
        with self._lock:
            return list(self._buffers.items())

    def clear(self) -> None:
        with self._lock:
            self._buffers.clear()


def reader_loop(
    stream: Iterable[str],
    buffers: ChannelBufferStore,
    *,
    stop_event: Optional[threading.Event] = None,
) -> None:
    """Read JSONL records from a line-oriented stream and fill channel buffers.

    This is intended to run in a background thread: it stops when the
    input stream is exhausted or when an optional ``stop_event`` is set.
    """
    for raw_line in stream:
        if stop_event is not None and stop_event.is_set():
            break

        line = raw_line.strip()
        if not line:
            continue

        try:
            record = json.loads(line)
        except json.JSONDecodeError as exc:
            logger.warning("Dropping malformed JSON line: %s (%s)", line, exc)
            continue

        if not isinstance(record, Mapping):
            logger.debug("Skipping non-object JSON payload: %r", record)
            continue

        try:
            _dispatch_record(record, buffers)
        except Exception:
            logger.exception("Failed to dispatch record: %r", record)


def _dispatch_record(record: Mapping[str, Any], buffers: ChannelBufferStore) -> None:
    sensor_id = record.get("sensor_id")
    if sensor_id is None:
        logger.debug("Record missing sensor_id: %r", record)
        return
    sensor_id_str = str(sensor_id)

    timestamp = _extract_timestamp(record)
    if timestamp is None:
        logger.debug("Record missing usable timestamp: %r", record)
        return

    appended = False
    for key, value in record.items():
        if key in _SKIP_FIELDS:
            continue
        numeric_value = _coerce_number(value)
        if numeric_value is None:
            continue
        buffers.append(sensor_id_str, str(key), timestamp, numeric_value)
        appended = True

    if not appended:
        logger.debug("No numeric channels found in record: %r", record)


def _extract_timestamp(record: Mapping[str, Any]) -> Optional[Number]:
    t_raw = record.get("t_s")
    if t_raw is not None:
        ts = _coerce_number(t_raw)
        if ts is not None:
            return ts
    ts_ns = record.get("timestamp_ns")
    if ts_ns is not None:
        ns_val = _coerce_number(ts_ns)
        if ns_val is not None:
            return ns_val * 1e-9
    return None


def _coerce_number(value: Any) -> Optional[Number]:
    if isinstance(value, bool):
        return None
    if isinstance(value, (int, float)):
        return float(value)
    try:
        return float(value)
    except (TypeError, ValueError):
        return None


@dataclass
class StreamReaderHandle:
    thread: threading.Thread
    stop_event: threading.Event
    buffers: ChannelBufferStore

    def stop(self, *, join: bool = False, timeout: Optional[float] = None) -> None:
        self.stop_event.set()
        if join:
            self.thread.join(timeout)

    def is_alive(self) -> bool:
        return self.thread.is_alive()


def start_reader(
    stream: Iterable[str],
    *,
    buffers: Optional[ChannelBufferStore] = None,
    capacity: int = DEFAULT_RINGBUFFER_CAPACITY,
    thread_name: Optional[str] = None,
) -> StreamReaderHandle:
    """
    Start a background thread that ingests JSON lines from *stream*.
    """

    store = buffers or ChannelBufferStore(capacity=capacity)
    stop_event = threading.Event()

    def _target() -> None:
        reader_loop(stream, store, stop_event=stop_event)

    thread = threading.Thread(
        target=_target,
        name=thread_name or "SensePiStreamReader",
        daemon=True,
    )
    thread.start()
    return StreamReaderHandle(thread=thread, stop_event=stop_event, buffers=store)


def start_reader_on_stdin(
    *,
    capacity: int = DEFAULT_RINGBUFFER_CAPACITY,
    thread_name: Optional[str] = None,
) -> StreamReaderHandle:
    """
    Convenience wrapper that starts the reader on ``sys.stdin``.
    """
    import sys

    return start_reader(
        sys.stdin,
        capacity=capacity,
        thread_name=thread_name or "SensePiStreamReader(stdin)",
    )

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\core\timeseries_buffer.py
# File: timeseries_buffer.py (ext: .py
# Dir : src\sensepi\core\
# Size: 3411 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\core\timeseries_buffer.py
from __future__ import annotations

import math
from collections.abc import Iterable, Iterator
from typing import Dict, Tuple

import numpy as np

from .ringbuffer import RingBuffer

NS_PER_SECOND = 1_000_000_000

TimeSeriesSample = tuple[int, float]
BufferKey = Tuple[int, str]


def calculate_capacity(window_seconds: float, max_rate_hz: float, *, margin: float = 1.1) -> int:
    """
    Compute how many samples are needed to cover ``window_seconds`` at
    ``max_rate_hz`` with an optional ``margin``.
    """
    samples = window_seconds * max_rate_hz * margin
    return max(1, int(math.ceil(samples)))


def initialize_buffers_for_channels(
    sensor_ids: Iterable[int],
    channels: Iterable[str],
    *,
    window_seconds: float,
    max_rate_hz: float,
    margin: float = 1.1,
) -> Dict[BufferKey, "TimeSeriesBuffer"]:
    """
    Pre-create :class:`TimeSeriesBuffer` objects for expected (sensor, channel)
    combinations so buffers are ready when samples start arriving.
    """
    capacity = calculate_capacity(window_seconds, max_rate_hz, margin=margin)
    return {
        (int(sensor_id), channel): TimeSeriesBuffer(capacity)
        for sensor_id in sensor_ids
        for channel in channels
    }


def ns_to_seconds(ts_ns: np.ndarray) -> np.ndarray:
    """Convert nanosecond timestamps into floating-point seconds."""
    if ts_ns.size == 0:
        return np.empty(0, dtype=np.float64)
    return ts_ns.astype(np.float64) / float(NS_PER_SECOND)


class TimeSeriesBuffer:
    """
    Convenience wrapper around :class:`RingBuffer` to manage (t_ns, value)
    samples and retrieve arbitrary time windows.
    """

    __slots__ = ("_buffer",)

    def __init__(self, capacity: int) -> None:
        self._buffer: RingBuffer[TimeSeriesSample] = RingBuffer(capacity)

    def append(self, timestamp_ns: int, value: float) -> None:
        """Append a ``(timestamp_ns, value)`` tuple to the buffer."""
        self._buffer.append((int(timestamp_ns), float(value)))

    def clear(self) -> None:
        self._buffer.clear()

    def __len__(self) -> int:
        return len(self._buffer)

    def __iter__(self) -> Iterator[TimeSeriesSample]:
        return iter(self._buffer)

    def latest_timestamp_ns(self) -> int | None:
        """Return the newest timestamp in nanoseconds."""
        if len(self._buffer) == 0:
            return None
        return int(self._buffer[-1][0])

    def get_window(self, start_ns: int, end_ns: int) -> tuple[np.ndarray, np.ndarray]:
        """
        Return timestamps and values in the interval ``[start_ns, end_ns]``.

        Both arrays are NumPy ``float64``/``int64`` for efficient downstream use.
        """
        if end_ns < start_ns:
            start_ns, end_ns = end_ns, start_ns

        data = list(self._buffer)
        if not data:
            return np.empty(0, dtype=np.int64), np.empty(0, dtype=np.float64)

        count = len(data)
        times = np.fromiter((sample[0] for sample in data), dtype=np.int64, count=count)
        values = np.fromiter((sample[1] for sample in data), dtype=np.float64, count=count)

        start_idx = np.searchsorted(times, start_ns, side="left")
        end_idx = np.searchsorted(times, end_ns, side="right")
        if end_idx <= start_idx:
            return np.empty(0, dtype=np.int64), np.empty(0, dtype=np.float64)

        return times[start_idx:end_idx], values[start_idx:end_idx]


------------------------------ END OF FILE ------------------------------

============================= src\sensepi\data\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\data\
# Size: 835 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\data\__init__.py
"""Generic streaming data buffers and queues for sensor samples.

Lightweight containers in this package (e.g. :mod:`stream_buffer`) provide
in-memory storage and fan-out helpers that shuttle sensor samples between
threads. They stay decoupled from Qt or network code so they can be reused in
the GUI, remote workers, and offline scripts.
"""

from __future__ import annotations

__all__ = [
    "BufferConfig",
    "StreamingDataBuffer",
]

from .stream_buffer import BufferConfig, StreamingDataBuffer

# Backwards-compatible re-exports for historical buffer classes in sensepi.core.
try:
    from ..core import RingBuffer, TimeSeriesBuffer
except ImportError:  # pragma: no cover - optional dependency during docs builds
    pass
else:  # pragma: no cover - simple namespace wiring
    __all__ += ["RingBuffer", "TimeSeriesBuffer"]


------------------------------ END OF FILE ------------------------------

============================= src\sensepi\data\stream_buffer.py
# File: stream_buffer.py (ext: .py
# Dir : src\sensepi\data\
# Size: 8474 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\data\stream_buffer.py
"""Central ring buffer for recent streaming samples."""

from __future__ import annotations

from collections import deque
from dataclasses import dataclass
import math
from typing import Deque, Iterable, Iterator, List, MutableMapping, Optional

from ..sensors.mpu6050 import MpuSample

SensorKey = int | str


@dataclass
class BufferConfig:
    """Configuration for :class:`StreamingDataBuffer`."""

    max_seconds: float = 6.0
    sample_rate_hz: float = 200.0
    capacity_margin: float = 1.2
    max_samples_per_sensor: int | None = None

    def capacity(self) -> int:
        """
        Return the maximum number of samples to retain per sensor.

        This value is used as a backstop to avoid unbounded growth when
        timestamps are missing or invalid.
        """
        if self.max_samples_per_sensor is not None:
            return max(1, int(self.max_samples_per_sensor))

        seconds = max(0.1, float(self.max_seconds))
        rate = max(1.0, float(self.sample_rate_hz))
        margin = max(1.0, float(self.capacity_margin))
        estimate = seconds * rate * margin
        return max(1, int(math.ceil(estimate)))


class StreamingDataBuffer:
    """
    Multi-sensor ring buffer for :class:`MpuSample` instances.

    Instances are expected to be owned and mutated from the Qt main thread so
    simple Python containers are sufficient.
    """

    def __init__(self, config: BufferConfig | None = None) -> None:
        self._config = config or BufferConfig()
        self._buffers: MutableMapping[SensorKey, Deque[MpuSample]] = {}

    # ------------------------------------------------------------------ ingest
    def add_samples(self, samples: Iterable[MpuSample]) -> None:
        """Append samples to per-sensor deques, enforcing a sliding time window.

        Each sensor_id gets its own ring-like deque; `_truncate` keeps the
        buffer size bounded so recent data is available without unbounded growth.
        """
        for sample in samples:
            if sample is None:
                continue
            sensor_id = self._sensor_key_from_sample(sample)
            buf = self._buffers.setdefault(sensor_id, deque())
            buf.append(sample)
            self._truncate(sensor_id)

    # ------------------------------------------------------------------- query
    def get_sensor_ids(self) -> List[SensorKey]:
        """Return a snapshot of all sensor IDs currently present in the buffer."""
        return list(self._buffers.keys())

    def get_recent_samples(
        self,
        sensor_id: SensorKey,
        seconds: float | None = None,
        max_samples: int | None = None,
    ) -> List[MpuSample]:
        """Return recent samples for ``sensor_id`` ordered by time.

        The optional ``seconds`` limit trims older samples using their
        timestamps, while ``max_samples`` caps how many points are returned.

        Parameters
        ----------
        sensor_id:
            Sensor identifier to query.
        seconds:
            Maximum age of samples to return. Falls back to ``config.max_seconds``
            when ``None``.
        max_samples:
            Optional hard limit on the number of samples returned.
        """
        buf = self._buffers.get(self._normalize_sensor_id(sensor_id))
        if not buf:
            return []

        window_s = self._resolve_window(seconds)
        max_items = max_samples if max_samples is not None else 0
        latest_time = self._sample_time(buf[-1])
        threshold = None if latest_time is None else latest_time - window_s

        result: List[MpuSample] = []
        for sample in reversed(buf):
            if max_items and len(result) >= max_items:
                break
            sample_time = self._sample_time(sample)
            if threshold is not None and sample_time is not None:
                if sample_time < threshold:
                    break
            result.append(sample)
        result.reverse()
        return result

    def iter_all_samples(self, seconds: float | None = None) -> Iterator[MpuSample]:
        """Yield samples for all sensors ordered by sensor ID."""
        for sensor_id in sorted(self._buffers.keys(), key=str):
            for sample in self.get_recent_samples(sensor_id, seconds=seconds):
                yield sample

    def latest_timestamp(self, sensor_id: SensorKey | None = None) -> Optional[float]:
        """Return the latest timestamp in seconds for a sensor or across all sensors."""
        if sensor_id is not None:
            buf = self._buffers.get(self._normalize_sensor_id(sensor_id))
            if not buf:
                return None
            return self._sample_time(buf[-1])

        latest: Optional[float] = None
        for buf in self._buffers.values():
            if not buf:
                continue
            ts = self._sample_time(buf[-1])
            if ts is None:
                continue
            if latest is None or ts > latest:
                latest = ts
        return latest

    def get_axis_series(
        self,
        sensor_id: SensorKey,
        axis: str,
        seconds: float | None = None,
        max_samples: int | None = None,
    ) -> tuple[List[float], List[float]]:
        """
        Return synchronized timestamps and axis values for ``sensor_id``.

        Missing or NaN axis values are included as-is so the caller can decide
        how to handle them.
        """
        attr = axis.lower()
        samples = self.get_recent_samples(sensor_id, seconds=seconds, max_samples=max_samples)
        timestamps: List[float] = []
        values: List[float] = []
        for sample in samples:
            ts = self._sample_time(sample)
            value = getattr(sample, attr, None)
            if ts is None or value is None:
                continue
            timestamps.append(ts)
            values.append(float(value))
        return timestamps, values

    def clear(self, sensor_id: SensorKey | None = None) -> None:
        """Drop samples for ``sensor_id`` or the entire buffer when omitted."""
        if sensor_id is None:
            self._buffers.clear()
            return
        self._buffers.pop(self._normalize_sensor_id(sensor_id), None)

    # ----------------------------------------------------------------- helpers
    def _truncate(self, sensor_id: SensorKey) -> None:
        buf = self._buffers.get(sensor_id)
        if not buf:
            return

        # Always clamp by capacity to protect against unbounded growth.
        capacity = self._config.capacity()
        while len(buf) > capacity:
            buf.popleft()

        # Additionally enforce the max_seconds window when timestamps are valid.
        max_seconds = float(self._config.max_seconds)
        if max_seconds <= 0 or not buf:
            return

        latest_time = self._sample_time(buf[-1])
        if latest_time is None:
            return

        threshold = latest_time - max_seconds
        while buf:
            oldest_time = self._sample_time(buf[0])
            if oldest_time is None or oldest_time >= threshold:
                break
            buf.popleft()

    def _resolve_window(self, seconds: float | None) -> float:
        if seconds is None:
            return max(0.0, float(self._config.max_seconds))
        try:
            return max(0.0, float(seconds))
        except (TypeError, ValueError):
            return max(0.0, float(self._config.max_seconds))

    @staticmethod
    def _normalize_sensor_id(sensor_id: SensorKey | None) -> SensorKey:
        if sensor_id is None:
            return 0
        try:
            return int(sensor_id)
        except (TypeError, ValueError):
            return str(sensor_id)

    def _sensor_key_from_sample(self, sample: MpuSample) -> SensorKey:
        sensor_value = getattr(sample, "sensor_id", None)
        return self._normalize_sensor_id(sensor_value)

    @staticmethod
    def _sample_time(sample: MpuSample | None) -> Optional[float]:
        if sample is None:
            return None
        t_s = getattr(sample, "t_s", None)
        if t_s is not None:
            try:
                return float(t_s)
            except (TypeError, ValueError):
                pass
        timestamp_ns = getattr(sample, "timestamp_ns", None)
        if timestamp_ns is None:
            return None
        try:
            return float(timestamp_ns) * 1e-9
        except (TypeError, ValueError):
            return None

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\dataio\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\dataio\
# Size: 347 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\dataio\__init__.py
"""Data input/output helpers (CSV/JSON logs and file paths).

Utility modules here keep disk-level concerns isolated from the rest of the
application:
- :mod:`csv_writer` emits structured sensor logs.
- :mod:`log_loader` parses CSV/JSON recordings for offline review.
- :mod:`file_paths` centralises directory layout for logs and cache files.
"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\dataio\csv_writer.py
# File: csv_writer.py (ext: .py
# Dir : src\sensepi\dataio\
# Size: 453 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\dataio\csv_writer.py
"""CSV writing helpers for recorded sensor data."""

import csv
from pathlib import Path
from typing import Iterable, Sequence


def write_rows(path: Path, headers: Sequence[str], rows: Iterable[Sequence[float]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(headers)
        writer.writerows(rows)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\dataio\file_paths.py
# File: file_paths.py (ext: .py
# Dir : src\sensepi\dataio\
# Size: 368 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\dataio\file_paths.py
"""Helpers for constructing standard file paths."""

from datetime import datetime
from pathlib import Path

from ..config.app_config import AppPaths


def session_directory(name: str, base: Path | None = None) -> Path:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    root = base or AppPaths().raw_data
    return root / f"{name}_{timestamp}"

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\dataio\log_loader.py
# File: log_loader.py (ext: .py
# Dir : src\sensepi\dataio\
# Size: 838 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\dataio\log_loader.py
"""Utilities for loading recorded CSV logs."""

from pathlib import Path
from typing import Iterable, List, Sequence

import numpy as np


def load_csv(path: Path) -> np.ndarray:
    """Load a CSV file containing numeric data."""
    return np.loadtxt(path, delimiter=",")


def chunk_array(array: np.ndarray, chunk_size: int) -> Iterable[np.ndarray]:
    """Yield fixed-size chunks from an array."""
    total = array.shape[0]
    for start in range(0, total, chunk_size):
        yield array[start : start + chunk_size]


def merge_logs(paths: Sequence[Path]) -> np.ndarray:
    """Load multiple CSV logs and concatenate them along the first axis."""
    arrays: List[np.ndarray] = [load_csv(path) for path in paths]
    if not arrays:
        return np.empty((0, 0))
    return np.concatenate(arrays, axis=0)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 351 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\__init__.py
"""Desktop GUI implementation built with PySide6/Qt.

Panels in :mod:`gui.tabs` surface configuration, live FFT/streaming plots, and
offline log viewers, while :mod:`gui.widgets` houses shared Qt components.
This layer orchestrates the Qt event loop and delegates streaming/recording to
the :mod:`sensepi.remote` and :mod:`sensepi.core` packages.
"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\application.py
# File: application.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 5375 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\application.py
"""Application bootstrap for the PySide6 GUI."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Tuple

import matplotlib as mpl

from PySide6.QtCore import QLoggingCategory
from PySide6.QtWidgets import QApplication, QMainWindow

from .benchmark import BenchmarkDriver, BenchmarkOptions
from .main_window import MainWindow
from ..config.app_config import AppConfig

_MPL_CONFIGURED = False


def configure_matplotlib_for_realtime() -> None:
    """
    Apply global Matplotlib tweaks that improve interactive / real-time performance.

    This should be called once before any figures are created.
    """
    global _MPL_CONFIGURED
    if _MPL_CONFIGURED:
        return

    try:
        mpl.style.use("fast")
    except Exception:
        pass

    rc = mpl.rcParams
    rc["path.simplify"] = True
    rc["path.simplify_threshold"] = 0.2
    rc["agg.path.chunksize"] = 10000
    rc["axes.grid"] = True
    rc["figure.autolayout"] = True

    _MPL_CONFIGURED = True


def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="SensePi GUI controller")
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="Run the GUI in synthetic benchmark mode (no Pi connection)",
    )
    parser.add_argument(
        "--bench-rate",
        type=float,
        default=200.0,
        help="Synthetic input rate in Hz (default: 200)",
    )
    parser.add_argument(
        "--bench-duration",
        type=float,
        default=30.0,
        help="Benchmark duration in seconds (default: 30)",
    )
    parser.add_argument(
        "--bench-refresh",
        type=float,
        default=20.0,
        help="Plot refresh rate in Hz when benchmarking (default: 20)",
    )
    parser.add_argument(
        "--bench-channels",
        type=int,
        default=18,
        help="Number of charts to show when benchmarking (9 or 18)",
    )
    parser.add_argument(
        "--bench-log-interval",
        type=float,
        default=1.0,
        help="Seconds between benchmark log snapshots (default: 1.0)",
    )
    parser.add_argument(
        "--bench-sensors",
        type=int,
        default=3,
        help="Synthetic sensor count (default: 3)",
    )
    parser.add_argument(
        "--bench-csv",
        type=str,
        default="benchmark_results.csv",
        help="CSV file for benchmark metrics (default: benchmark_results.csv)",
    )
    parser.add_argument(
        "--bench-no-csv",
        action="store_true",
        help="Skip writing benchmark metrics to CSV",
    )
    parser.add_argument(
        "--bench-keep-open",
        action="store_true",
        help="Keep the GUI open after benchmark completes",
    )
    parser.add_argument(
        "--signal-backend",
        choices=("pyqtgraph", "matplotlib"),
        default="pyqtgraph",
        help="Signal plot backend to use (default: pyqtgraph)",
    )
    return parser


def _parse_cli_args(
    argv: list[str],
) -> tuple[argparse.Namespace, list[str]]:
    parser = _build_arg_parser()
    args, qt_args = parser.parse_known_args(argv[1:])
    qt_argv = [argv[0], *qt_args]
    return args, qt_argv


def create_app(
    argv: list[str] | None = None,
    *,
    app_config: AppConfig | None = None,
) -> Tuple[QApplication, QMainWindow]:
    """
    Create the QApplication and main SensePi window.

    Parameters
    ----------
    argv:
        Optional argument list to pass to :class:`QApplication`.

    Returns
    -------
    app:
        The QApplication instance (owned by caller).
    window:
        The main window instance with all tabs set up.
    """
    qt_args = argv if argv is not None else sys.argv
    configure_matplotlib_for_realtime()
    app = QApplication.instance() or QApplication(qt_args)

    # Suppress noisy QObject::connect warnings from QStyleHints and similar internals
    QLoggingCategory.setFilterRules("qt.core.qobject.connect=false")

    window = MainWindow(app_config=app_config)
    return app, window


def main(argv: list[str] | None = None) -> None:
    raw_argv = argv if argv is not None else sys.argv
    args, qt_argv = _parse_cli_args(raw_argv)
    app_config = AppConfig(signal_backend=args.signal_backend)
    app, win = create_app(qt_argv, app_config=app_config)

    benchmark_driver: BenchmarkDriver | None = None
    if args.benchmark:
        csv_path = None
        if not args.bench_no_csv:
            csv_path = Path(args.bench_csv).expanduser().resolve()
        benchmark_driver = BenchmarkDriver(
            app=app,
            window=win,
            options=BenchmarkOptions(
                rate_hz=float(args.bench_rate),
                duration_s=float(args.bench_duration),
                refresh_hz=float(args.bench_refresh),
                channel_count=int(max(1, args.bench_channels)),
                log_interval_s=max(0.1, float(args.bench_log_interval)),
                csv_path=csv_path,
                sensor_count=max(1, int(args.bench_sensors)),
                keep_open=bool(args.bench_keep_open),
            ),
        )
        setattr(win, "_benchmark_driver", benchmark_driver)

    win.show()
    if benchmark_driver is not None:
        benchmark_driver.start()
    raise SystemExit(app.exec())


if __name__ == "__main__":
    main()

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\benchmark.py
# File: benchmark.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 5744 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\benchmark.py
"""Synthetic benchmark helpers for the SensePi GUI."""

from __future__ import annotations

import csv
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from PySide6.QtCore import QObject, QTimer, Qt
from PySide6.QtWidgets import QApplication

from ..perf_system import get_process_cpu_percent
from .main_window import MainWindow


@dataclass
class BenchmarkOptions:
    rate_hz: float = 200.0
    duration_s: float = 30.0
    refresh_hz: float = 20.0
    channel_count: int = 18
    log_interval_s: float = 1.0
    csv_path: Optional[Path] = Path("benchmark_results.csv")
    sensor_count: int = 3
    keep_open: bool = False


class BenchmarkDriver(QObject):
    """Drive the GUI in synthetic benchmark mode."""

    def __init__(
        self,
        app: QApplication,
        window: MainWindow,
        options: BenchmarkOptions,
    ) -> None:
        super().__init__(window)
        self._app = app
        self._window = window
        self._options = options
        self._timer = QTimer(self)
        self._timer.setTimerType(Qt.PreciseTimer)
        interval_ms = int(max(100, round(options.log_interval_s * 1000.0)))
        self._timer.setInterval(interval_ms)
        self._timer.timeout.connect(self._on_tick)
        self._start_monotonic: float = 0.0
        self._rows: List[dict[str, float]] = []
        self._started = False
        self._finished = False

    def start(self) -> None:
        if self._started:
            return
        self._started = True
        signals_tab = self._window.signals_tab
        refresh_hz = max(0.5, float(self._options.refresh_hz))
        refresh_interval_ms = max(1, int(round(1000.0 / refresh_hz)))
        signals_tab.fixed_interval_ms = refresh_interval_ms
        signals_tab.set_refresh_mode("fixed")
        signals_tab.set_view_mode_by_channels(self._options.channel_count)
        signals_tab.set_perf_hud_visible(True)

        sensor_count = max(1, int(self._options.sensor_count))
        sensor_ids = list(range(1, sensor_count + 1))
        signals_tab.on_stream_started()
        signals_tab.start_synthetic_stream(
            rate_hz=self._options.rate_hz,
            sensor_ids=sensor_ids,
        )
        signals_tab.update_stream_rate("mpu6050", self._options.rate_hz)

        self._start_monotonic = time.perf_counter()
        self._timer.start()
        print(
            (
                "[benchmark] running at %.1f Hz for %.1f s, GUI refresh %.1f Hz, "
                "sensors=%d"
            )
            % (
                self._options.rate_hz,
                self._options.duration_s,
                refresh_hz,
                sensor_count,
            )
        )
        if self._options.duration_s <= 0.0:
            QTimer.singleShot(0, self._finish)

    def _on_tick(self) -> None:
        elapsed = time.perf_counter() - self._start_monotonic
        snap = self._window.signals_tab.get_perf_snapshot()
        fps = float(snap.get("fps", 0.0) or 0.0)
        target_fps = float(snap.get("target_fps", 0.0) or 0.0)
        timer_hz = float(snap.get("timer_hz", 0.0) or 0.0)
        avg_frame_ms = float(snap.get("avg_frame_ms", 0.0) or 0.0)
        avg_latency_ms = float(snap.get("avg_latency_ms", 0.0) or 0.0)
        max_latency_ms = float(snap.get("max_latency_ms", 0.0) or 0.0)
        approx_drop = snap.get("approx_dropped_fps")
        if approx_drop is None:
            approx_drop = snap.get("approx_dropped_frames_per_sec", 0.0)
        approx_drop = float(approx_drop or 0.0)
        cpu_percent = get_process_cpu_percent()

        row = {
            "t": elapsed,
            "fps": fps,
            "target_fps": target_fps,
            "timer_hz": timer_hz,
            "avg_frame_ms": avg_frame_ms,
            "avg_latency_ms": avg_latency_ms,
            "max_latency_ms": max_latency_ms,
            "approx_dropped_fps": approx_drop,
            "cpu_percent": cpu_percent,
        }
        self._rows.append(row)

        print(
            (
                "[benchmark] t=%5.1fs fps=%5.1f/%5.1f timer=%4.1fHz frame=%5.2fms "
                "lat=%5.2f/%5.2fms drop=%5.2f cpu=%5.1f%%"
            )
            % (
                elapsed,
                fps,
                target_fps,
                timer_hz,
                avg_frame_ms,
                avg_latency_ms,
                max_latency_ms,
                approx_drop,
                cpu_percent,
            )
        )

        if elapsed >= self._options.duration_s:
            self._finish()

    def _finish(self) -> None:
        if self._finished:
            return
        self._finished = True
        self._timer.stop()
        signals_tab = self._window.signals_tab
        signals_tab.stop_synthetic_stream()
        signals_tab.on_stream_stopped()
        if self._options.csv_path and self._rows:
            self._write_csv(self._options.csv_path)
        print("[benchmark] completed; duration %.1f s" % self._options.duration_s)
        if not self._options.keep_open:
            self._app.quit()

    def _write_csv(self, path: Path) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = [
            "t",
            "fps",
            "target_fps",
            "timer_hz",
            "avg_frame_ms",
            "avg_latency_ms",
            "max_latency_ms",
            "approx_dropped_fps",
            "cpu_percent",
        ]
        with path.open("w", newline="") as fh:
            writer = csv.DictWriter(fh, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows({k: row.get(k, 0.0) for k in fieldnames} for row in self._rows)
        print(f"[benchmark] metrics written to {path}")

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\main_window.py
# File: main_window.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 7104 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\main_window.py
"""Main window for the SensePi GUI."""

from __future__ import annotations

from PySide6.QtGui import QAction, QCloseEvent
from PySide6.QtWidgets import QMainWindow, QTabWidget, QVBoxLayout, QWidget

from ..config.app_config import AppConfig, AppPaths
from .tabs.tab_fft import FftTab
from .tabs.tab_logs import LogsTab
from .tabs.tab_offline import OfflineTab
from .tabs.tab_recorder import RecorderTab
from .tabs.tab_settings import SettingsTab
from .tabs.tab_signals import SignalsTab, create_signal_plot_widget


class MainWindow(QMainWindow):
    def __init__(self, app_config: AppConfig | None = None) -> None:
        super().__init__()
        self.setWindowTitle("SensePi Recorder")

        self._app_config = app_config or AppConfig()

        self._tabs = QTabWidget()

        app_paths = AppPaths()

        self.recorder_tab = RecorderTab()
        backend = self._app_config.normalized_signal_backend()
        plot_window_s = self._app_config.plot_performance.normalized_time_window_s()
        plot_widget = create_signal_plot_widget(
            parent=None,
            backend=backend,
            max_seconds=plot_window_s,
        )
        self.signals_tab = SignalsTab(
            self.recorder_tab,
            plot_widget=plot_widget,
            app_config=self._app_config,
        )
        self.fft_tab = FftTab(
            self.recorder_tab,
            self.signals_tab,
            app_config=self._app_config,
        )
        self.signals_tab.fft_refresh_interval_changed.connect(
            self.fft_tab.set_refresh_interval_ms
        )
        self.settings_tab = SettingsTab()
        self.offline_tab = OfflineTab(app_paths, self.recorder_tab)
        self.logs_tab = LogsTab(app_paths)

        # Order tabs by the typical workflow from device setup through analysis.
        self._tabs.addTab(self.recorder_tab, self.tr("Device"))
        self._tabs.addTab(self.settings_tab, self.tr("Sensors && Rates"))
        self._tabs.addTab(self.signals_tab, self.tr("Live Signals"))
        self._tabs.addTab(self.fft_tab, self.tr("Spectrum"))
        self._tabs.addTab(self.offline_tab, self.tr("Recordings"))
        self._tabs.addTab(self.logs_tab, self.tr("App Logs"))

        container = QWidget()
        layout = QVBoxLayout(container)
        layout.addWidget(self._tabs)

        self.setCentralWidget(container)
        view_menu = self.menuBar().addMenu("&View")
        self._act_show_perf_hud = QAction("Show Performance HUD", self)
        self._act_show_perf_hud.setCheckable(True)
        self._act_show_perf_hud.setChecked(False)
        self._act_show_perf_hud.toggled.connect(
            self.signals_tab.set_perf_hud_visible
        )
        view_menu.addAction(self._act_show_perf_hud)

        self.recorder_tab.recording_started.connect(
            self.signals_tab.on_stream_started
        )
        self.recorder_tab.recording_started.connect(
            self.fft_tab.on_stream_started
        )

        self.recorder_tab.recording_stopped.connect(
            self.signals_tab.on_stream_stopped
        )
        self.recorder_tab.recording_stopped.connect(
            self.fft_tab.on_stream_stopped
        )
        self.recorder_tab.recording_stopped.connect(
            self._on_recording_stopped
        )

        self.signals_tab.start_stream_requested.connect(
            self._on_start_stream_requested
        )
        self.signals_tab.stop_stream_requested.connect(
            self._on_stop_stream_requested
        )

        self.recorder_tab.recording_error.connect(
            self.signals_tab.handle_error
        )
        self.recorder_tab.rate_updated.connect(
            self.signals_tab.update_stream_rate
        )
        self.recorder_tab.rate_updated.connect(
            self.fft_tab.update_stream_rate
        )
        self.settings_tab.hostsUpdated.connect(
            self.recorder_tab.on_hosts_updated
        )
        self.settings_tab.sensorsUpdated.connect(
            self.recorder_tab.on_sensors_updated
        )

    def _on_start_stream_requested(self, recording: bool) -> None:
        """
        Called when the user presses Start in the Live Signals tab.
        Delegates to RecorderTab using the current Device tab settings.
        """
        try:
            acquisition = self.signals_tab.current_acquisition_settings()
            stream_rate_hz = acquisition.effective_stream_rate_hz

            # Update GUI refresh behaviour before data starts flowing
            if acquisition.signals_mode == "adaptive":
                self.signals_tab.set_refresh_mode(
                    "follow_sampling_rate", stream_rate_hz
                )
            else:
                self.signals_tab.fixed_interval_ms = acquisition.signals_refresh_ms
                self.signals_tab.set_refresh_mode("fixed")

            self.signals_tab.update_stream_rate("mpu6050", stream_rate_hz)
            self.fft_tab.set_refresh_interval_ms(acquisition.fft_refresh_ms)
            self.fft_tab.update_stream_rate("mpu6050", stream_rate_hz)

            session_name = self.signals_tab.session_name()
            self.recorder_tab.start_live_stream(
                recording=recording,
                acquisition=acquisition,
                session_name=session_name or None,
            )
        except Exception as exc:
            self.recorder_tab.report_error(f"Failed to start stream: {exc!r}")

    def _on_stop_stream_requested(self) -> None:
        try:
            self.recorder_tab.stop_live_stream()
        except Exception as exc:
            self.recorder_tab.report_error(f"Failed to stop stream: {exc!r}")

    def _on_recording_stopped(self) -> None:
        """
        Called whenever a recording run finishes.
        Hints that offline sync is now the next step.
        """
        status = self.statusBar()

        remote_dir = None
        try:
            remote_dir = self.recorder_tab.current_remote_data_dir()
        except AttributeError:
            remote_dir = None

        if remote_dir:
            dest_text = (
                remote_dir.as_posix()
                if hasattr(remote_dir, "as_posix")
                else str(remote_dir)
            )
            msg = (
                f"Recording stopped. Logs saved to {dest_text} on the Pi. "
                "Open the Recordings tab to sync and replay this session."
            )
        else:
            msg = (
                "Recording stopped. Open the Recordings tab to sync logs from "
                "the Pi and replay previous sessions."
            )

        status.showMessage(msg, 8000)

        idx = self._tabs.indexOf(self.offline_tab)
        if idx >= 0:
            self._tabs.setCurrentIndex(idx)

    def closeEvent(self, event: QCloseEvent) -> None:
        try:
            self.recorder_tab.stop_live_stream(wait=True)
        except Exception as exc:  # pragma: no cover - best-effort shutdown
            self.recorder_tab.report_error(
                f"Failed to stop stream on close: {exc!r}"
            )
        super().closeEvent(event)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\perf_metrics.py
# File: perf_metrics.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 1818 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\perf_metrics.py
"""Lightweight performance metrics used by plotting widgets."""

from __future__ import annotations

from collections import deque
from dataclasses import dataclass, field
from typing import Deque

MAX_SAMPLES_PERF = 300


@dataclass
class PlotPerfStats:
    """Ring-buffer style tracking of recent redraw performance."""

    frame_times: Deque[float] = field(default_factory=lambda: deque(maxlen=MAX_SAMPLES_PERF))
    frame_durations: Deque[float] = field(default_factory=lambda: deque(maxlen=MAX_SAMPLES_PERF))
    sample_to_draw_latencies: Deque[float] = field(default_factory=lambda: deque(maxlen=MAX_SAMPLES_PERF))

    def record_frame(self, start_ts: float, end_ts: float) -> None:
        """Add a frame timestamp and its duration."""
        self.frame_times.append(end_ts)
        self.frame_durations.append(end_ts - start_ts)

    def record_latency(self, latency_s: float) -> None:
        """Track latency between sample arrival and it being drawn."""
        self.sample_to_draw_latencies.append(latency_s)

    def compute_fps(self) -> float:
        if len(self.frame_times) < 2:
            return 0.0
        dt = self.frame_times[-1] - self.frame_times[0]
        if dt <= 0:
            return 0.0
        return (len(self.frame_times) - 1) / dt

    def avg_frame_ms(self) -> float:
        if not self.frame_durations:
            return 0.0
        return 1000.0 * sum(self.frame_durations) / len(self.frame_durations)

    def avg_latency_ms(self) -> float:
        if not self.sample_to_draw_latencies:
            return 0.0
        return 1000.0 * sum(self.sample_to_draw_latencies) / len(self.sample_to_draw_latencies)

    def max_latency_ms(self) -> float:
        if not self.sample_to_draw_latencies:
            return 0.0
        return 1000.0 * max(self.sample_to_draw_latencies)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\pg_signal_plot_widget.py
# File: pg_signal_plot_widget.py (ext: .py
# Dir : src\sensepi\gui\
# Size: 362 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\pg_signal_plot_widget.py
"""Compatibility shim for the PyQtGraph SignalPlotWidget implementation."""

from __future__ import annotations

from .tabs.tab_signals import SignalPlotWidget

# Historically, the PyQtGraph implementation lived in this module. Keep the
# import path alive so older code can still request PyQtGraphSignalPlotWidget.
PyQtGraphSignalPlotWidget = SignalPlotWidget


------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 192 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\__init__.py
"""Tab widgets for the SensePi GUI."""

from __future__ import annotations

# TODO: move SampleKey + live buffers into a shared LiveDataStore for FFT/Signals tabs.
SampleKey = tuple[int, str]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_fft.py
# File: tab_fft.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 27661 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_fft.py
"""Live FFT / spectrum tab for MPU6050 samples."""

from __future__ import annotations

import logging
import time
from typing import Dict, Optional, Sequence, Tuple, TYPE_CHECKING

import numpy as np
from PySide6.QtCore import QTimer, Slot
from PySide6.QtWidgets import (
    QCheckBox,
    QComboBox,
    QDoubleSpinBox,
    QFormLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QVBoxLayout,
    QWidget,
)

from matplotlib.axes import Axes
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg
from matplotlib.figure import Figure
from matplotlib.lines import Line2D

from ...analysis import filters
from ...config.app_config import AppConfig, PlotPerformanceConfig
from ...core.ringbuffer import RingBuffer
from ...data import StreamingDataBuffer
from ...tools.debug import debug_enabled
from . import SampleKey

if TYPE_CHECKING:  # pragma: no cover - circular import guard
    from .tab_recorder import RecorderTab
    from .tab_signals import SignalsTab

DEFAULT_FFT_WINDOW_S = 2.0
MIN_FFT_WINDOW_S = 0.5
MAX_FFT_WINDOW_S = 10.0

DEFAULT_FFT_UPDATE_MS = 500  # fallback if config missing
MIN_FFT_UPDATE_MS = 50
MAX_FFT_UPDATE_MS = 2000

DEFAULT_MAX_FREQUENCY_HZ = 200.0  # cap plotted frequency if useful

logger = logging.getLogger(__name__)


class FftTab(QWidget):
    """FFT tab that reuses the shared streaming buffer via RecorderTab."""

    def __init__(
        self,
        recorder_tab: RecorderTab,
        signals_tab: "SignalsTab | None" = None,
        parent: Optional[QWidget] = None,
        app_config: AppConfig | None = None,
    ) -> None:
        super().__init__(parent)

        self._recorder_tab = recorder_tab
        self._signals_tab: SignalsTab | None = signals_tab
        self._app_config: AppConfig = app_config or AppConfig()
        plot_perf = getattr(self._app_config, "plot_performance", None)
        if not isinstance(plot_perf, PlotPerformanceConfig):
            plot_perf = PlotPerformanceConfig()
        self._plot_perf_config: PlotPerformanceConfig = plot_perf
        self._max_subplots = self._plot_perf_config.normalized_max_subplots()
        self._stream_rate_hz: float = 0.0
        self._last_rendered_latest_ts: Optional[float] = None
        self._force_next_update: bool = True
        self._fft_axes: Dict[SampleKey, Axes] = {}
        self._fft_lines: Dict[SampleKey, Line2D] = {}
        self._current_layout: Tuple[Tuple[int, ...], Tuple[str, ...]] | None = None
        # Bound how many samples each FFT uses so the GUI stays responsive.
        self._max_fft_samples = 4096
        self._fft_decimation_target = 2048
        self._fft_size = 512
        self._fft_sample_rate_hz: float = 1.0
        self._fft_freqs = np.fft.rfftfreq(self._fft_size, 1.0 / self._fft_sample_rate_hz)
        self._fft_window = np.hanning(self._fft_size)
        self._default_ylim = (0.0, 1.0)

        # Figure / canvas -------------------------------------------------------
        self._figure = Figure(figsize=(5, 3), tight_layout=True)
        self._canvas = FigureCanvasQTAgg(self._figure)

        # Controls --------------------------------------------------------------
        controls_group = QGroupBox("FFT settings")
        form = QFormLayout(controls_group)

        # View selection
        top_row = QHBoxLayout()
        self.view_mode_combo = QComboBox()
        self.view_mode_combo.addItem(
            "AX / AY / GZ (9 charts)", userData="default3"
        )
        self.view_mode_combo.addItem(
            "All axes (18 charts)", userData="all6"
        )

        top_row.addWidget(QLabel("View:"))
        top_row.addWidget(self.view_mode_combo)
        top_row.addStretch()
        form.addRow(top_row)

        # FFT window length (seconds)
        self.window_spin = QDoubleSpinBox()
        self.window_spin.setRange(MIN_FFT_WINDOW_S, MAX_FFT_WINDOW_S)
        self.window_spin.setSingleStep(0.5)
        default_window_s = self._plot_perf_config.normalized_time_window_s()
        self.window_spin.setValue(default_window_s)
        form.addRow("Window (s):", self.window_spin)

        # Detrend / lowpass options
        self.detrend_check = QCheckBox("Detrend")
        self.lowpass_check = QCheckBox("Low-pass filter")

        self.lowpass_cutoff = QDoubleSpinBox()
        self.lowpass_cutoff.setRange(0.1, 5000.0)
        self.lowpass_cutoff.setSingleStep(1.0)
        self.lowpass_cutoff.setValue(100.0)
        form.addRow(self.detrend_check)
        row_lp = QHBoxLayout()
        row_lp.addWidget(self.lowpass_check)
        row_lp.addWidget(QLabel("Cutoff (Hz):"))
        row_lp.addWidget(self.lowpass_cutoff)
        row_lp.addStretch()
        form.addRow(row_lp)

        # Status label
        self._status_label = QLabel("Waiting for data...")

        # Layout ---------------------------------------------------------------
        layout = QVBoxLayout(self)
        layout.addWidget(controls_group)
        layout.addWidget(self._canvas)
        layout.addWidget(self._status_label)

        # Timer to recompute FFT periodically
        self._timer = QTimer(self)
        default_fft_interval = self._clamp_fft_interval(
            self._plot_perf_config.fft_refresh_interval_ms()
        )
        self._timer.setInterval(default_fft_interval)
        self._timer.timeout.connect(self._on_fft_timer)
        self._debug_fft_ema_ms: float = 0.0
        self._debug_fft_last_log: float = time.perf_counter()

        # Wiring
        self.view_mode_combo.currentIndexChanged.connect(self._on_controls_changed)
        self.window_spin.valueChanged.connect(self._on_controls_changed)
        self.window_spin.valueChanged.connect(self._update_fft_timer_interval)
        self.detrend_check.toggled.connect(self._on_controls_changed)
        self.lowpass_check.toggled.connect(self._on_controls_changed)
        self.lowpass_cutoff.valueChanged.connect(self._on_controls_changed)
        self._update_fft_timer_interval()
        self._draw_waiting()

    def _make_key(self, sensor_id: int, channel: str) -> SampleKey:
        return int(sensor_id), str(channel)

    def _clamp_fft_interval(self, interval_ms: float | int) -> int:
        try:
            interval = int(round(float(interval_ms)))
        except (TypeError, ValueError):
            interval = DEFAULT_FFT_UPDATE_MS
        return max(MIN_FFT_UPDATE_MS, min(interval, MAX_FFT_UPDATE_MS))

    def _active_stream_buffer(self) -> StreamingDataBuffer:
        """Return the current streaming buffer (prepping for shared LiveDataStore)."""
        return self._recorder_tab.data_buffer()

    def _get_buffer_window(
        self,
        key: SampleKey,
        *,
        window_s: float,
        data_buffer: StreamingDataBuffer | None = None,
    ) -> tuple[Sequence[float], Sequence[float]]:
        # Prefer reusing the time-windowed data held by SignalsTab; if that
        # is unavailable, fall back to querying the shared StreamingDataBuffer.
        sensor_id, channel = key
        window = self._window_from_signals_tab(sensor_id, channel, window_s)
        if window is not None:
            return window

        buffer = data_buffer or self._active_stream_buffer()
        return buffer.get_axis_series(sensor_id, channel, seconds=window_s)

    def _on_controls_changed(self, *args: object) -> None:
        """Trigger an FFT refresh when the user changes view/filter controls."""
        self._request_full_refresh()
        self._update_fft()

    def _request_full_refresh(self) -> None:
        self._force_next_update = True

    def _update_fft_timer_interval(self, *_: object) -> None:
        """Adjust the FFT refresh cadence based on the selected window length."""
        timer = getattr(self, "_timer", None)
        if timer is None:
            return
        try:
            window_s = float(self.window_spin.value())
        except (TypeError, ValueError):
            window_s = DEFAULT_FFT_WINDOW_S
        window_s = max(MIN_FFT_WINDOW_S, min(window_s, MAX_FFT_WINDOW_S))
        desired_period_s = window_s / 2.0
        interval_ms = int(desired_period_s * 1000.0)
        timer.setInterval(self._clamp_fft_interval(interval_ms))

    def set_refresh_interval_ms(self, interval_ms: int) -> None:
        """Public setter so other tabs can tune how often the FFT updates."""
        clamped = self._clamp_fft_interval(interval_ms)
        self._timer.setInterval(clamped)

    def set_max_fft_samples(self, n: int) -> None:
        """Public setter mainly for tests / tuning of the FFT sample cap."""
        self._max_fft_samples = max(256, int(n))

    def set_signals_tab(self, signals_tab: "SignalsTab | None") -> None:
        """Inject the SignalsTab reference so we can reuse its ring buffers."""
        self._signals_tab = signals_tab

    @Slot(str, float)
    def update_stream_rate(self, sensor_type: str, hz: float) -> None:
        """Receive stream-rate updates so FFT windows know how much data to expect."""
        if sensor_type != "mpu6050":
            return
        self._stream_rate_hz = float(hz) if hz > 0.0 else 0.0
        self._ensure_fft_frequency_axis(self._stream_rate_hz)
        self._request_full_refresh()

    @Slot()
    def on_stream_started(self) -> None:
        self._clear_layout()
        self._draw_waiting()
        self._status_label.setText("Streaming...")
        self._last_rendered_latest_ts = None
        self._request_full_refresh()
        if not self._timer.isActive():
            self._timer.start()

    @Slot()
    def on_stream_stopped(self) -> None:
        # Keep last spectrum visible but update status.
        self._status_label.setText("Stream stopped.")
        if self._timer.isActive():
            self._timer.stop()

    # --------------------------------------------------------------- internals
    @staticmethod
    def _channel_units(channel: str) -> str:
        ch = channel.lower()
        if ch in {"ax", "ay", "az"}:
            return "m/sÂ²"
        if ch in {"gx", "gy", "gz"}:
            return "deg/s"
        return ""

    def _min_samples_required(self, window_s: float) -> int:
        """
        Return the minimum number of samples needed before running the FFT.

        We require at least half of the expected samples for the window, or
        a modest constant so short windows still work.
        """
        if self._stream_rate_hz > 0.0:
            expected = self._stream_rate_hz * window_s
            return max(8, int(expected * 0.5))
        return 8

    def _decimate_signal_for_fft(
        self,
        times: np.ndarray,
        values: np.ndarray,
        target_points: int,
    ) -> tuple[np.ndarray, np.ndarray]:
        """Downsample arrays to ~target_points to cap FFT cost."""
        if target_points <= 0 or values.size <= target_points:
            return times, values

        step = max(2, values.size // target_points)
        indices = np.arange(0, values.size, step, dtype=int)
        if indices[-1] != values.size - 1:
            indices = np.append(indices, values.size - 1)
        return times[indices], values[indices]

    def _window_signal(
        self,
        buf: Sequence[Tuple[float, float]] | RingBuffer[Tuple[float, float]],
        window_s: float,
    ) -> tuple[np.ndarray, np.ndarray, float] | None:
        if buf is None:
            return None
        points = list(buf)
        if len(points) < 4:
            return None
        t_latest = points[-1][0]
        t_min = t_latest - window_s

        times = [t for (t, _v) in points if t >= t_min]
        values = [v for (t, v) in points if t >= t_min]
        if len(values) < 4 or times[-1] <= times[0]:
            return None

        times_arr = np.asarray(times, dtype=float)
        values_arr = np.asarray(values, dtype=float)

        if values_arr.size > self._max_fft_samples:
            times_arr = times_arr[-self._max_fft_samples :]
            values_arr = values_arr[-self._max_fft_samples :]

        if values_arr.size > self._fft_decimation_target:
            times_arr, values_arr = self._decimate_signal_for_fft(
                times_arr, values_arr, self._fft_decimation_target
            )
            if times_arr.size < 2 or values_arr.size < 2:
                return None

        dt = times_arr[-1] - times_arr[0]
        if dt <= 0.0:
            return None
        sample_rate_hz = (len(times_arr) - 1) / dt if dt > 0 else 1.0
        if sample_rate_hz <= 0.0:
            sample_rate_hz = self._stream_rate_hz if self._stream_rate_hz > 0.0 else 1.0
        return times_arr, values_arr, sample_rate_hz

    def _preprocess_signal(
        self,
        values: np.ndarray,
        sample_rate_hz: float,
    ) -> np.ndarray:
        signal = values.copy()
        if self.detrend_check.isChecked():
            signal = filters.detrend(signal)
        if self.lowpass_check.isChecked():
            cutoff = float(self.lowpass_cutoff.value())
            nyquist = 0.5 * sample_rate_hz
            if 0.0 < cutoff < nyquist:
                signal = filters.butter_lowpass(
                    signal,
                    cutoff_hz=cutoff,
                    sample_rate_hz=sample_rate_hz,
                )
        return signal

    def _ensure_fft_frequency_axis(self, sample_rate_hz: float | None = None) -> None:
        """Ensure the cached frequency axis matches the latest sampling rate."""
        if sample_rate_hz is None or sample_rate_hz <= 0.0:
            sample_rate_hz = self._stream_rate_hz
        sample_rate_hz = float(sample_rate_hz) if sample_rate_hz and sample_rate_hz > 0 else 1.0
        if np.isclose(sample_rate_hz, self._fft_sample_rate_hz, rtol=1e-3):
            return
        self._fft_sample_rate_hz = sample_rate_hz
        self._fft_freqs = np.fft.rfftfreq(self._fft_size, 1.0 / self._fft_sample_rate_hz)
        self._fft_window = np.hanning(self._fft_size)
        zero_line = np.zeros_like(self._fft_freqs)
        for line in self._fft_lines.values():
            line.set_xdata(self._fft_freqs)
            line.set_ydata(zero_line.copy())
        for ax in self._fft_axes.values():
            self._apply_frequency_limits(ax)

    def _apply_frequency_limits(self, ax: Axes) -> None:
        max_freq = float(DEFAULT_MAX_FREQUENCY_HZ)
        if self._fft_freqs.size > 0:
            max_freq = min(max_freq, float(self._fft_freqs[-1]))
        if max_freq <= 0.0 or not np.isfinite(max_freq):
            max_freq = 1.0
        ax.set_xlim(0.0, max_freq)
        ax.set_ylim(*self._default_ylim)

    def _compute_fft_magnitude(self, signal: np.ndarray) -> np.ndarray:
        """Return FFT magnitudes for the most recent fft_size samples."""
        if signal.size == 0:
            return np.zeros_like(self._fft_freqs)
        window = signal[-self._fft_size :]
        if window.size < self._fft_size:
            padded = np.zeros(self._fft_size, dtype=float)
            if window.size > 0:
                padded[-window.size :] = window
            window = padded
        windowed = window * self._fft_window
        fft_vals = np.fft.rfft(windowed)
        return np.abs(fft_vals)

    def _on_fft_timer(self) -> None:
        if not debug_enabled():
            self._update_fft()
            return

        start = time.perf_counter()
        self._update_fft()
        elapsed_ms = (time.perf_counter() - start) * 1000.0
        alpha = 0.2
        if self._debug_fft_ema_ms <= 0.0:
            self._debug_fft_ema_ms = elapsed_ms
        else:
            self._debug_fft_ema_ms = (
                alpha * elapsed_ms + (1.0 - alpha) * self._debug_fft_ema_ms
            )
        now_perf = time.perf_counter()
        if now_perf - self._debug_fft_last_log >= 5.0:
            interval = self._timer.interval() if hasattr(self, "_timer") else 0
            print(
                f"[DEBUG] fft_redraw interval={interval} ms emaâ‰ˆ{self._debug_fft_ema_ms:.2f} ms",
                flush=True,
            )
            self._debug_fft_last_log = now_perf

    def _update_fft(self) -> None:
        self._update_mpu6050_fft()

    def _update_mpu6050_fft(self) -> None:
        data_buffer = self._active_stream_buffer()

        sensor_ids = self._resolve_sensor_ids(data_buffer)
        if not sensor_ids:
            self._draw_waiting()
            return

        latest_ts = data_buffer.latest_timestamp()
        if latest_ts is None:
            self._draw_waiting()
            return

        if (
            not self._force_next_update
            and self._last_rendered_latest_ts is not None
            and latest_ts <= self._last_rendered_latest_ts
        ):
            return

        view_mode = self.view_mode_combo.currentData()
        if view_mode == "default3":
            channels = ["ax", "ay", "gz"]
        else:
            channels = ["ax", "ay", "az", "gx", "gy", "gz"]

        window_s = float(self.window_spin.value())
        min_samples = self._min_samples_required(window_s)

        if not self._ensure_fft_layout(sensor_ids, channels):
            self._draw_waiting()
            return

        stats_samples = None
        stats_fs = None
        have_data = False

        for sensor_id in sensor_ids:
            for ch in channels:
                key = self._make_key(sensor_id, ch)
                timestamps, values = self._get_buffer_window(
                    key,
                    window_s=window_s,
                    data_buffer=data_buffer,
                )
                if (
                    self._sequence_length(values) < min_samples
                    or self._sequence_length(timestamps) < 2
                ):
                    self._clear_line(sensor_id, ch)
                    continue

                points = list(zip(timestamps, values))
                prepared = self._window_signal(points, window_s)
                if prepared is None:
                    self._clear_line(sensor_id, ch)
                    continue

                _times_arr, values_arr, sample_rate_hz = prepared
                signal = self._preprocess_signal(values_arr, sample_rate_hz)

                axis_sample_rate = self._stream_rate_hz if self._stream_rate_hz > 0.0 else sample_rate_hz
                self._ensure_fft_frequency_axis(axis_sample_rate)
                magnitude = self._compute_fft_magnitude(signal)
                if magnitude.size == 0:
                    self._clear_line(sensor_id, ch)
                    continue

                self._update_fft_line(key, magnitude)
                have_data = True
                if stats_samples is None:
                    stats_samples = self._fft_size
                    stats_fs = axis_sample_rate

        if not have_data:
            self._status_label.setText("Waiting for data...")
            self._last_rendered_latest_ts = latest_ts
            self._force_next_update = False
            self._canvas.draw_idle()
            return

        self._canvas.draw_idle()
        self._last_rendered_latest_ts = latest_ts
        self._force_next_update = False
        if stats_samples is not None and stats_fs is not None:
            self._status_label.setText(
                f"Window: {window_s:.1f} s, FFT samples: {stats_samples}, fsâ‰ˆ{stats_fs:.1f} Hz"
            )

    def _apply_subplot_limits(
        self,
        sensor_ids: Sequence[int],
        channels: Sequence[str],
    ) -> tuple[list[int], list[str], bool, bool]:
        limited_sensors = list(sensor_ids)
        limited_channels = list(channels)
        if not limited_sensors or not limited_channels:
            return limited_sensors, limited_channels, False, False
        limit = self._max_subplots
        if not limit or limit <= 0:
            return limited_sensors, limited_channels, False, False
        total = len(limited_sensors) * len(limited_channels)
        if total <= limit:
            return limited_sensors, limited_channels, False, False

        trimmed_channels = False
        max_channels = max(1, limit // len(limited_sensors))
        if len(limited_channels) > max_channels:
            limited_channels = limited_channels[:max_channels]
            trimmed_channels = True
        trimmed_sensors = False
        max_sensors = max(1, limit // len(limited_channels))
        if len(limited_sensors) > max_sensors:
            limited_sensors = limited_sensors[:max_sensors]
            trimmed_sensors = True
        return limited_sensors, limited_channels, trimmed_channels, trimmed_sensors

    def _ensure_fft_layout(self, sensor_ids: Sequence[int], channels: Sequence[str]) -> bool:
        sensor_list = [int(s) for s in sensor_ids]
        channel_list = [str(ch) for ch in channels]
        if not sensor_list or not channel_list:
            return False

        original_sensor_count = len(sensor_list)
        original_channel_count = len(channel_list)
        (
            sensor_list,
            channel_list,
            trimmed_channels,
            trimmed_sensors,
        ) = self._apply_subplot_limits(sensor_list, channel_list)
        if not sensor_list or not channel_list:
            return False

        signature = (tuple(sensor_list), tuple(channel_list))
        should_log_limits = (
            (trimmed_channels or trimmed_sensors)
            and signature != self._current_layout
        )
        if should_log_limits:
            limit = self._max_subplots
            if trimmed_channels and original_channel_count > len(channel_list):
                logger.warning(
                    "FFT tab: reducing visible channels from %d to %d to honor max subplot limit (%s).",
                    original_channel_count,
                    len(channel_list),
                    limit,
                )
            if trimmed_sensors and original_sensor_count > len(sensor_list):
                logger.warning(
                    "FFT tab: reducing visible sensors from %d to %d to honor max subplot limit (%s).",
                    original_sensor_count,
                    len(sensor_list),
                    limit,
                )

        if signature == self._current_layout:
            return True

        self._current_layout = signature
        self._fft_axes.clear()
        self._fft_lines.clear()
        self._figure.clear()
        self._ensure_fft_frequency_axis()

        nrows = len(sensor_list)
        ncols = len(channel_list)
        subplot_index = 1
        for row_idx, sensor_id in enumerate(sensor_list):
            for col_idx, ch in enumerate(channel_list):
                ax = self._figure.add_subplot(nrows, ncols, subplot_index)
                subplot_index += 1
                zero_line = np.zeros_like(self._fft_freqs)
                line, = ax.plot(self._fft_freqs, zero_line, lw=0.9)
                key = self._make_key(sensor_id, ch)
                self._fft_axes[key] = ax
                self._fft_lines[key] = line
                if row_idx == nrows - 1:
                    ax.set_xlabel("Frequency [Hz]")
                if col_idx == 0:
                    ax.set_ylabel("Magnitude")
                units = self._channel_units(ch)
                title = f"S{sensor_id} {ch.upper()}"
                if units:
                    title = f"{title} [{units}]"
                ax.set_title(title)
                ax.grid(True)
                self._apply_frequency_limits(ax)

        self._figure.tight_layout()
        self._canvas.draw_idle()
        return True

    def _update_fft_line(self, key: SampleKey, magnitude: np.ndarray) -> None:
        line = self._fft_lines.get(key)
        ax = self._fft_axes.get(key)
        if line is None or ax is None:
            return

        if magnitude.size != self._fft_freqs.size:
            padded = np.zeros_like(self._fft_freqs)
            count = min(len(padded), magnitude.size)
            if count > 0:
                padded[:count] = magnitude[:count]
            magnitude = padded

        line.set_ydata(magnitude)
        self._maybe_expand_ylim(ax, magnitude)

    def _clear_line(self, sensor_id: int, channel: str) -> None:
        key = self._make_key(sensor_id, channel)
        line = self._fft_lines.get(key)
        ax = self._fft_axes.get(key)
        if line is not None:
            line.set_ydata(np.zeros_like(self._fft_freqs))
        if ax is not None:
            ax.set_ylim(*self._default_ylim)

    def _maybe_expand_ylim(self, ax: Axes, magnitude: np.ndarray) -> None:
        if magnitude.size == 0:
            return
        try:
            mag_max = float(np.nanmax(magnitude))
        except ValueError:
            return
        if not np.isfinite(mag_max) or mag_max <= 0.0:
            return

        _current_min, current_max = ax.get_ylim()
        if current_max <= 0.0 or mag_max > current_max * 0.95:
            new_max = max(mag_max * 1.1, self._default_ylim[1])
            ax.set_ylim(self._default_ylim[0], new_max)

    def _clear_layout(self) -> None:
        self._fft_axes.clear()
        self._fft_lines.clear()
        self._current_layout = None
        self._figure.clear()

    def _draw_waiting(self) -> None:
        self._clear_layout()
        ax = self._figure.add_subplot(111)
        ax.set_xlabel("Frequency [Hz]")
        ax.set_ylabel("Magnitude")
        ax.set_title("Waiting for data...")
        self._canvas.draw_idle()
        self._status_label.setText("Waiting for data...")

    def _window_from_signals_tab(
        self,
        sensor_id: int,
        channel: str,
        window_s: float,
    ) -> tuple[Sequence[float], Sequence[float]] | None:
        signals_tab = self._signals_tab
        if signals_tab is None:
            return None
        getter = getattr(signals_tab, "get_time_series_window", None)
        if getter is None:
            return None
        try:
            times, values = getter(sensor_id, channel, window_s)
        except Exception:
            return None
        if times is None or values is None:
            return None
        if self._sequence_length(times) < 2 or self._sequence_length(values) < 2:
            return None
        return times, values

    def _sequence_length(self, seq: object) -> int:
        """Return len(seq) while tolerating numpy arrays."""
        if seq is None:
            return 0
        try:
            return len(seq)  # type: ignore[arg-type]
        except TypeError:
            size = getattr(seq, "size", None)
            if size is None:
                return 0
            try:
                return int(size)
            except (TypeError, ValueError):
                return 0

    def _sensor_ids_from_signals_tab(self) -> list[int]:
        signals_tab = self._signals_tab
        if signals_tab is None:
            return []
        getter = getattr(signals_tab, "live_sensor_ids", None)
        if getter is None:
            return []
        try:
            ids = list(getter())
        except Exception:
            return []
        normalized: list[int] = []
        for sensor_id in ids:
            try:
                normalized.append(int(sensor_id))
            except (TypeError, ValueError):
                continue

        deduped: list[int] = []
        for sensor_id in normalized:
            if sensor_id not in deduped:
                deduped.append(sensor_id)
        return sorted(deduped)

    def _resolve_sensor_ids(self, data_buffer: StreamingDataBuffer) -> list[int]:
        sensor_ids = self._sensor_ids_from_signals_tab()
        if sensor_ids:
            return sensor_ids
        raw_ids = data_buffer.get_sensor_ids()
        return sorted(raw_ids, key=str)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_logs.py
# File: tab_logs.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 4637 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_logs.py
from __future__ import annotations

from pathlib import Path
from typing import List, Optional

from PySide6.QtCore import Slot
from PySide6.QtGui import QTextCursor
from PySide6.QtWidgets import (
    QCheckBox,
    QComboBox,
    QHBoxLayout,
    QLabel,
    QPlainTextEdit,
    QPushButton,
    QVBoxLayout,
    QWidget,
)

from ...config.app_config import AppPaths


class LogsTab(QWidget):
    """Simple viewer for application log files stored under AppPaths.logs."""

    _MAX_READ_BYTES = 250_000

    def __init__(
        self, app_paths: AppPaths | None = None, parent: Optional[QWidget] = None
    ) -> None:
        super().__init__(parent)
        self._paths = app_paths or AppPaths()
        self._paths.ensure()
        self._log_files: list[Path] = []

        layout = QVBoxLayout(self)

        control_row = QHBoxLayout()
        control_row.addWidget(QLabel("Log file:"))
        self._file_combo = QComboBox(self)
        control_row.addWidget(self._file_combo, stretch=1)
        self._refresh_button = QPushButton("Refresh", self)
        self._follow_check = QCheckBox("Follow tail", self)
        self._follow_check.setChecked(True)
        control_row.addWidget(self._follow_check)
        control_row.addWidget(self._refresh_button)
        layout.addLayout(control_row)

        self._view = QPlainTextEdit(self)
        self._view.setReadOnly(True)
        self._view.setLineWrapMode(QPlainTextEdit.NoWrap)
        layout.addWidget(self._view, stretch=1)

        self._status_label = QLabel("Select a log file to view.", self)
        layout.addWidget(self._status_label)

        self._refresh_button.clicked.connect(self._refresh_log_list)
        self._file_combo.currentIndexChanged.connect(
            self._on_selection_changed
        )

        self._refresh_log_list()

    def _log_dir(self) -> Path:
        return self._paths.logs

    def _collect_log_files(self) -> List[Path]:
        log_dir = self._log_dir()
        if not log_dir.exists():
            return []
        patterns = ("*.log", "*.txt")
        files: dict[Path, float] = {}
        for pattern in patterns:
            for path in log_dir.glob(pattern):
                try:
                    files[path] = path.stat().st_mtime
                except FileNotFoundError:
                    continue
        return sorted(files, key=files.get, reverse=True)

    @Slot()
    def _refresh_log_list(self, _checked: bool = False) -> None:
        files = self._collect_log_files()
        self._log_files = files
        current_text = self._file_combo.currentText()
        was_blocked = self._file_combo.blockSignals(True)
        self._file_combo.clear()
        for path in files:
            self._file_combo.addItem(path.name, userData=str(path))
        if files:
            index = 0
            if current_text:
                idx = self._file_combo.findText(current_text)
                if idx >= 0:
                    index = idx
            self._file_combo.setCurrentIndex(index)
            self._file_combo.blockSignals(was_blocked)
            self._load_log_file(files[index])
        else:
            self._file_combo.blockSignals(was_blocked)
            self._view.clear()
            self._status_label.setText("No log files found.")

    @Slot(int)
    def _on_selection_changed(self, index: int) -> None:
        if index < 0 or index >= len(self._log_files):
            self._view.clear()
            return
        self._load_log_file(self._log_files[index])

    def _load_log_file(self, path: Path) -> None:
        if not path.exists():
            self._status_label.setText(f"File not found: {path.name}")
            return
        try:
            text = self._read_tail(path)
        except Exception as exc:
            self._view.setPlainText("")
            self._status_label.setText(f"Failed to read {path.name}: {exc}")
            return

        self._view.setPlainText(text)
        if self._follow_check.isChecked():
            self._view.moveCursor(QTextCursor.End)
        self._status_label.setText(str(path))

    def _read_tail(self, path: Path) -> str:
        size = path.stat().st_size
        prefix = ""
        read_bytes = self._MAX_READ_BYTES
        if size > read_bytes:
            prefix = (
                f"... showing last {read_bytes // 1024} KB "
                "- file truncated for display ...\n"
            )
        with path.open("rb") as handle:
            if size > read_bytes:
                handle.seek(size - read_bytes)
            data = handle.read()
        text = data.decode("utf-8", errors="replace")
        return prefix + text

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_offline.py
# File: tab_offline.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 14136 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_offline.py
from __future__ import annotations

from collections import Counter
import stat
from pathlib import Path, PurePosixPath
from typing import Iterable, Optional, TYPE_CHECKING

from PySide6.QtCore import Qt, Slot
from PySide6.QtWidgets import (
    QApplication,
    QFileDialog,
    QHBoxLayout,
    QLabel,
    QListWidget,
    QMessageBox,
    QPushButton,
    QVBoxLayout,
    QWidget,
)
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg

from ...config.app_config import AppPaths
from ...config.log_paths import LOG_SUBDIR_MPU, build_pc_session_root
from ...remote.ssh_client import SSHClient
# Decimation helper used to downsample long recordings for plotting
from ...tools.plotter import Plotter

if TYPE_CHECKING:
    from .tab_recorder import RecorderTab


class OfflineTab(QWidget):
    """Offline log viewer built on the shared plotter helpers."""

    def __init__(
        self,
        app_paths: AppPaths,
        recorder_tab: "RecorderTab | None" = None,
        parent: Optional[QWidget] = None,
    ) -> None:
        super().__init__(parent)
        self._paths = app_paths
        self._paths.ensure()
        self._recorder_tab = recorder_tab
        self._plotter = Plotter()

        self._canvas: FigureCanvasQTAgg | None = None

        layout = QVBoxLayout()

        top_row = QHBoxLayout()
        top_row.addWidget(QLabel("Offline log files:"))
        self.btn_refresh = QPushButton("Refresh")
        self.btn_sync = QPushButton("Sync logs from Pi")
        self.btn_sync_open = QPushButton("Sync && open latest")
        self.btn_browse = QPushButton("Browseâ€¦")
        top_row.addWidget(self.btn_refresh)
        top_row.addWidget(self.btn_sync)
        top_row.addWidget(self.btn_sync_open)
        top_row.addWidget(self.btn_browse)
        top_row.addStretch()
        layout.addLayout(top_row)
        self.help_label = QLabel(
            "Offline workflow:\n"
            "1. Record data on the Pi via the Device tab.\n"
            "2. Click 'Sync logs from Pi' to download new log files.\n"
            "3. Select a log file below and double-click it to open.",
            self,
        )
        self.help_label.setWordWrap(True)
        layout.addWidget(self.help_label)

        self.file_list = QListWidget(self)
        layout.addWidget(self.file_list)

        self.status_label = QLabel(
            "No logs synced yet. Sync from the Pi to download runs, then "
            "select a file to open it.",
            self,
        )
        layout.addWidget(self.status_label)

        self.setLayout(layout)

        self.btn_refresh.clicked.connect(self._populate_files)
        self.btn_browse.clicked.connect(self._on_browse)
        self.btn_sync.clicked.connect(self._on_sync_from_pi_clicked)
        self.btn_sync_open.clicked.connect(self._on_sync_and_open_latest_clicked)
        self.file_list.itemDoubleClicked.connect(self._on_open_selected)

        self._populate_files()

    def _populate_files(self, update_status: bool = True) -> int:
        self.file_list.clear()
        count = 0
        for path in self._candidate_logs():
            self.file_list.addItem(str(path))
            count += 1

        if update_status:
            if count == 0:
                self.status_label.setText(
                    "No local logs found. Record on the Pi, then use "
                    "'Sync logs from Pi' to download them here."
                )
            else:
                self.status_label.setText(
                    f"Found {count} log file(s). Select one to view or sync "
                    "new logs from the Pi."
                )
        return count

    def _resolve_remote_context(self):
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is None:
            return None
        try:
            details = recorder.current_host_details()
        except AttributeError:
            return None
        if not details:
            return None
        host, cfg = details
        remote_dir = cfg.data_dir.expanduser().as_posix()
        host_label = cfg.name or host.name or host.host
        return host, remote_dir, host_label

    def _download_remote_logs(
        self,
        client: SSHClient,
        remote_root: str,
        host_label: str,
    ) -> tuple[int, Counter[str | None], list[Path]]:
        remote_root = remote_root.rstrip("/") or "/"
        allowed_ext = {".csv", ".jsonl", ".json"}
        sensor_prefix = LOG_SUBDIR_MPU
        downloaded = 0
        per_session: Counter[str | None] = Counter()
        new_files: list[Path] = []
        slug_source = host_label or getattr(client.host, "name", "") or client.host.host

        with client.sftp() as sftp:
            try:
                sftp.listdir(remote_root)
            except IOError as exc:
                raise RuntimeError(
                    f"Remote directory {remote_root} not found"
                ) from exc

            sensor_root = PurePosixPath(remote_root)
            if sensor_root.name != sensor_prefix:
                candidate = sensor_root / sensor_prefix
                try:
                    sftp.listdir(candidate.as_posix())
                except IOError:
                    pass
                else:
                    sensor_root = candidate

            stack: list[tuple[PurePosixPath, PurePosixPath]] = [
                (sensor_root, PurePosixPath())
            ]
            treats_first_part_as_session = sensor_root.name == sensor_prefix

            while stack:
                remote_dir, rel_dir = stack.pop()
                remote_dir_str = remote_dir.as_posix()
                try:
                    entries = sftp.listdir_attr(remote_dir_str)
                except IOError:
                    continue
                for entry in entries:
                    remote_path = remote_dir / entry.filename
                    rel_path = rel_dir / entry.filename
                    mode = entry.st_mode
                    if stat.S_ISDIR(mode):
                        stack.append((remote_path, rel_path))
                        continue
                    if not stat.S_ISREG(mode):
                        continue
                    if Path(entry.filename).suffix.lower() not in allowed_ext:
                        continue

                    rel_parts = rel_path.parts
                    if not rel_parts:
                        continue
                    session_name: str | None = None
                    rel_after_session = rel_parts
                    if treats_first_part_as_session and len(rel_parts) >= 2:
                        session_name = rel_parts[0]
                        rel_after_session = rel_parts[1:]
                    if not rel_after_session:
                        continue

                    local_rel = Path(*rel_after_session)
                    target_root = build_pc_session_root(
                        raw_root=self._paths.raw_data,
                        host_slug=slug_source,
                        session_name=session_name,
                        sensor_prefix=sensor_prefix,
                    )
                    local_path = target_root / local_rel
                    local_path.parent.mkdir(parents=True, exist_ok=True)

                    should_skip = False
                    if local_path.exists():
                        try:
                            should_skip = local_path.stat().st_size == entry.st_size
                        except FileNotFoundError:
                            should_skip = False
                    if should_skip:
                        continue

                    sftp.get(remote_path.as_posix(), str(local_path))
                    downloaded += 1
                    per_session[session_name] += 1
                    new_files.append(local_path)

        return downloaded, per_session, new_files

    def _format_sync_message(
        self,
        total: int,
        per_session: Counter[str | None],
        host_label: str,
    ) -> str:
        session_names = sorted(name for name in per_session if name)
        unnamed_count = per_session.get(None, 0)
        host_display = host_label or "host"

        if session_names and not unnamed_count:
            if len(session_names) == 1:
                return f"Synced {total} log file(s) for session '{session_names[0]}'."
            listed = ", ".join(session_names[:3])
            if len(session_names) > 3:
                listed += f", +{len(session_names) - 3} more"
            return (
                f"Synced {total} log file(s) across {len(session_names)} sessions: "
                f"{listed}."
            )

        if unnamed_count and not session_names:
            return f"Synced {total} log file(s) from host '{host_display}'."

        if session_names:
            return (
                f"Synced {total} log file(s) from host '{host_display}' "
                f"(sessions: {', '.join(session_names)})."
            )

        return f"Synced {total} log file(s)."

    def _candidate_logs(self) -> Iterable[Path]:
        roots = [
            self._paths.raw_data,
            self._paths.processed_data,
            self._paths.logs,
        ]
        candidates: dict[Path, float] = {}
        patterns = ("*.csv", "*.jsonl")
        for root in roots:
            if not root.exists():
                continue
            for pattern in patterns:
                for path in root.glob(pattern):
                    try:
                        candidates[path] = path.stat().st_mtime
                    except FileNotFoundError:
                        continue
        for path in sorted(candidates, key=candidates.get, reverse=True):
            yield path

    @Slot()
    def _on_browse(self) -> None:
        path, _ = QFileDialog.getOpenFileName(
            self,
            "Select log file",
            str(self._paths.raw_data),
            "Logs (*.csv *.jsonl)",
        )
        if path:
            self.load_file(Path(path))

    @Slot()
    def _on_open_selected(self) -> None:
        item = self.file_list.currentItem()
        if not item:
            return
        self.load_file(Path(item.text()))

    @Slot()
    def _on_sync_from_pi_clicked(self) -> None:
        """Download new log files from the Pi (offline workflow step 2).

        Workflow reminder:
        1. Record data on the Pi from the Device tab.
        2. Sync logs from the Pi to the laptop (this method).
        3. Open downloaded logs for offline plotting.
        """
        context = self._resolve_remote_context()
        if context is None:
            QMessageBox.information(
                self,
                "Sync logs",
                "Select a Raspberry Pi host in the Device tab first.",
            )
            return
        host, remote_dir, host_label = context
        if not remote_dir:
            QMessageBox.warning(
                self,
                "Sync logs",
                "The selected host does not define a data directory.",
            )
            return

        client = SSHClient(host)
        host_display = host_label or host.name or host.host
        self.status_label.setText(f"Syncing logs from {remote_dir} â€¦")
        QApplication.setOverrideCursor(Qt.WaitCursor)
        try:
            downloaded, per_session, new_files = self._download_remote_logs(
                client, remote_dir, host_display
            )
        except Exception as exc:
            QMessageBox.critical(
                self,
                "Sync failed",
                f"Unable to sync logs from {remote_dir}: {exc}",
            )
            self.status_label.setText(f"Sync failed: {exc}")
        else:
            self._populate_files(update_status=False)
            self._highlight_new_files(new_files)
            if downloaded:
                message = self._format_sync_message(
                    downloaded, per_session, host_display
                )
            else:
                message = "No new log files to sync."
            self.status_label.setText(message)
        finally:
            QApplication.restoreOverrideCursor()
            try:
                client.close()
            except Exception:
                pass

    def _highlight_new_files(self, new_files: Iterable[Path]) -> None:
        """Select the first newly downloaded log to draw attention to it."""
        targets = {str(path) for path in new_files}
        if not targets:
            return
        for row in range(self.file_list.count()):
            item = self.file_list.item(row)
            if item is None:
                continue
            if item.text() in targets:
                self.file_list.setCurrentRow(row)
                break

    @Slot()
    def _on_sync_and_open_latest_clicked(self) -> None:
        """Sync logs and immediately open the newest entry."""
        self._on_sync_from_pi_clicked()

        if self.file_list.count() <= 0:
            return

        first_item = self.file_list.item(0)
        if first_item is None:
            return

        newest_path = Path(first_item.text())
        self.file_list.setCurrentItem(first_item)
        self._on_open_selected()
        self.status_label.setText(
            f"Synced and opened latest log: {newest_path.name}"
        )

    def load_file(self, path: Path) -> None:
        if not path.exists():
            self.status_label.setText(f"File does not exist: {path}")
            return

        try:
            fig, _axes, _lines = self._plotter.build_figure(path)
        except Exception as exc:
            self.status_label.setText(f"Failed to load {path.name}: {exc}")
            return

        if self._canvas is not None:
            self.layout().removeWidget(self._canvas)
            self._canvas.setParent(None)
            self._canvas.deleteLater()

        self._canvas = FigureCanvasQTAgg(fig)
        self.layout().addWidget(self._canvas)
        fig.canvas.manager.set_window_title(f"SensePi offline â€” {path.name}")
        self.status_label.setText(f"Loaded {path}")

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_recorder.py
# File: tab_recorder.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 29948 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_recorder.py
"""Recorder tab for starting/stopping Raspberry Pi loggers."""

from __future__ import annotations

import logging
import math
import queue
from dataclasses import dataclass
from pathlib import Path
import shlex
from typing import Dict, Iterable, Mapping, Optional

from PySide6.QtCore import QMetaObject, QThread, Qt, Signal, Slot
from PySide6.QtWidgets import (
    QCheckBox,
    QComboBox,
    QDoubleSpinBox,
    QFormLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QPushButton,
    QVBoxLayout,
    QWidget,
)

from ..widgets import AcquisitionSettings, CollapsibleSection
from ...analysis.rate import RateController
from ...config.app_config import HostConfig, HostInventory, SensorDefaults
from ...config.pi_logger_config import (
    PiLoggerConfig,
    build_logger_args,
    build_logger_command,
)
from ...config.sampling import GuiSamplingDisplay, SamplingConfig
from ...core.live_stream import select_parser
from ...data import BufferConfig, StreamingDataBuffer
from ...remote.pi_recorder import PiRecorder
from ...remote.sensor_ingest_worker import SensorIngestWorker
from ...remote.ssh_client import Host
from ...sensors.mpu6050 import MpuSample


logger = logging.getLogger(__name__)


@dataclass
class MpuGuiConfig:
    enabled: bool = True
    rate_hz: float = 100.0
    sensors: str = "1,2,3"
    channels: str = "default"
    include_temp: bool = False
    limit_duration: bool = False
    duration_s: float = 0.0


class RecorderTab(QWidget):
    """
    Recorder tab for starting/stopping Raspberry Pi loggers.

    It emits parsed sample objects to other tabs (e.g. Signals and FFT).
    """

    #: Emitted for every parsed sample object (MpuSample, generic LiveSample, ...).
    sample_received = Signal(object)
    streaming_started = Signal()
    streaming_stopped = Signal()
    error_reported = Signal(str)
    rate_updated = Signal(str, float)
    sampling_config_changed = Signal(object)
    recording_started = Signal()
    recording_stopped = Signal()
    recording_error = Signal(str)

    def __init__(
        self,
        host_inventory: HostInventory | None = None,
        parent: Optional[QWidget] = None,
    ) -> None:
        super().__init__(parent)
        self._host_inventory = host_inventory or HostInventory()
        self._sensor_defaults = SensorDefaults()
        self._sampling_config = self._sensor_defaults.load_sampling_config()

        self._hosts: Dict[str, Dict[str, object]] = {}
        self._pi_recorder: Optional[PiRecorder] = None

        self._ingest_thread: Optional[QThread] = None
        self._ingest_worker: Optional[SensorIngestWorker] = None
        self._rate_controllers: Dict[str, RateController] = {
            "mpu6050": RateController(window_size=500, default_hz=0.0),
        }
        self._recording_mode: bool = False
        self._stop_requested: bool = False
        self._ingest_batch_size = 50
        self._ingest_max_latency_ms = 100
        self._ingest_had_error = False
        self._last_session_name: str = ""
        decimation = self._sampling_config.compute_decimation()
        self._data_buffer = StreamingDataBuffer(
            BufferConfig(
                max_seconds=6.0,
                sample_rate_hz=decimation["stream_rate_hz"],
            )
        )
        self._active_stream: Iterable[str] | None = None
        self._sample_queue: queue.Queue[object] = queue.Queue(maxsize=10_000)

        self._build_ui()
        self._load_hosts()
        self._refresh_sampling_labels()

        self.error_reported.connect(self._show_error)
        self.rate_updated.connect(self._on_rate_updated)

    # --------------------------------------------------------------- UI setup
    def _build_ui(self) -> None:
        layout = QVBoxLayout(self)

        # Host selection
        self.host_group = QGroupBox("Raspberry Pi host", self)
        host_form = QFormLayout(self.host_group)

        self.host_combo = QComboBox(self.host_group)
        host_form.addRow("Host:", self.host_combo)

        self.host_status_label = QLabel("No hosts configured.", self.host_group)
        host_form.addRow("Status:", self.host_status_label)

        host_section = CollapsibleSection("Raspberry Pi host", self)
        host_layout = QVBoxLayout()
        host_layout.setContentsMargins(0, 0, 0, 0)
        host_layout.addWidget(self.host_group)
        host_section.setContentLayout(host_layout)
        layout.addWidget(host_section)

        # MPU6050 settings
        self.mpu_group = QGroupBox("MPU6050 settings", self)
        mpu_layout = QHBoxLayout(self.mpu_group)

        self.mpu_enable_chk = QCheckBox("Enable MPU6050", self.mpu_group)
        self.mpu_enable_chk.setChecked(True)
        mpu_layout.addWidget(self.mpu_enable_chk)

        mpu_layout.addWidget(QLabel("Sensors:", self.mpu_group))
        self.mpu_sensors_edit = QLineEdit("1,2,3", self.mpu_group)
        self.mpu_sensors_edit.setToolTip("Comma-separated sensor IDs (1..3)")
        mpu_layout.addWidget(self.mpu_sensors_edit)

        mpu_layout.addWidget(QLabel("Channels:", self.mpu_group))
        self.mpu_channels_combo = QComboBox(self.mpu_group)
        self.mpu_channels_combo.addItem("Default (AX, AY, GZ)", userData="default")
        self.mpu_channels_combo.addItem("Accel only (AX, AY, AZ)", userData="acc")
        self.mpu_channels_combo.addItem("Gyro only (GX, GY, GZ)", userData="gyro")
        self.mpu_channels_combo.addItem("Both (acc + gyro)", userData="both")
        self.mpu_channels_combo.setToolTip(
            "Select which axes are streamed from the MPU6050.\n"
            "Default (AX, AY, GZ): streams only AX, AY and GZ (no AZ, GX, GY).\n"
            "Both (acc + gyro): streams all six axes (AX, AY, AZ, GX, GY, GZ), "
            "so the Signals page can show every axis."
        )
        mpu_layout.addWidget(self.mpu_channels_combo)

        self.mpu_temp_chk = QCheckBox("Include temperature", self.mpu_group)
        self.mpu_temp_chk.setChecked(False)
        mpu_layout.addWidget(self.mpu_temp_chk)

        rate_form = QFormLayout()
        self.device_rate_label = QLabel("â€”", self.mpu_group)
        self.record_rate_label = QLabel("â€”", self.mpu_group)
        self.stream_rate_label = QLabel("â€”", self.mpu_group)
        self.mode_label = QLabel("â€”", self.mpu_group)

        rate_form.addRow("Device rate [Hz]:", self.device_rate_label)
        rate_form.addRow("Recording rate [Hz]:", self.record_rate_label)
        rate_form.addRow("GUI stream [Hz]:", self.stream_rate_label)
        rate_form.addRow("Mode:", self.mode_label)

        mpu_layout.addLayout(rate_form)

        duration_row = QHBoxLayout()
        self.mpu_limit_duration_chk = QCheckBox(
            "Limit duration (s):", self.mpu_group
        )
        self.mpu_duration_spin = QDoubleSpinBox(self.mpu_group)
        self.mpu_duration_spin.setRange(0.1, 3600.0)
        self.mpu_duration_spin.setDecimals(1)
        self.mpu_duration_spin.setSingleStep(1.0)
        self.mpu_duration_spin.setValue(10.0)
        self.mpu_duration_spin.setEnabled(False)
        self.mpu_limit_duration_chk.toggled.connect(
            self.mpu_duration_spin.setEnabled
        )
        duration_row.addWidget(self.mpu_limit_duration_chk)
        duration_row.addWidget(self.mpu_duration_spin)
        duration_row.addStretch(1)
        mpu_layout.addLayout(duration_row)

        mpu_section = CollapsibleSection("MPU6050 settings", self)
        mpu_container = QVBoxLayout()
        mpu_container.setContentsMargins(0, 0, 0, 0)
        mpu_container.addWidget(self.mpu_group)
        mpu_section.setContentLayout(mpu_container)
        mpu_section.setCollapsed(True)
        layout.addWidget(mpu_section)

        # Status + rate
        self.overall_status = QLabel("Idle.", self)
        layout.addWidget(self.overall_status)

        self.mpu_rate_label = QLabel("GUI stream rate: --", self)
        self.mpu_rate_label.setToolTip(
            "Estimated sample rate of data arriving in this GUI tab."
        )
        layout.addWidget(self.mpu_rate_label)

        layout.addStretch()

        # Hidden start/stop buttons (driven programmatically by MainWindow)
        button_box = QGroupBox(self)
        button_box.setVisible(False)
        btn_row = QHBoxLayout()
        self.start_btn = QPushButton("Start streaming", button_box)
        self.stop_btn = QPushButton("Stop", button_box)
        self.stop_btn.setEnabled(False)
        btn_row.addWidget(self.start_btn)
        btn_row.addWidget(self.stop_btn)
        button_box.setLayout(btn_row)
        layout.addWidget(button_box)

        # Signal wiring
        self.start_btn.clicked.connect(self._on_start_clicked)
        self.stop_btn.clicked.connect(self._on_stop_clicked)

    def _load_hosts(self) -> None:
        """Populate the host combo from config/hosts.yaml."""
        try:
            inventory = self._host_inventory.load()
        except Exception as exc:
            self.host_status_label.setText(f"Error loading hosts.yaml: {exc}")
            return

        entries = inventory.get("pis") or []
        self._apply_host_entries(entries, preserve_selection=False)

    def _apply_host_entries(
        self,
        host_entries: Iterable[Mapping[str, object]],
        *,
        preserve_selection: bool,
    ) -> None:
        previous = self.host_combo.currentText() if preserve_selection else None

        self._hosts.clear()
        self.host_combo.clear()

        for entry in host_entries:
            if not isinstance(entry, Mapping):
                logger.warning("Skipping invalid host entry (not a mapping): %r", entry)
                continue
            try:
                host_cfg = self._host_inventory.to_host_config(entry)
            except Exception as exc:
                logger.warning("Skipping invalid host entry %r: %s", entry, exc)
                continue

            host = Host(
                name=host_cfg.name,
                host=host_cfg.host,
                user=host_cfg.user,
                password=host_cfg.password,
                port=host_cfg.port,
            )
            self._hosts[host_cfg.name] = {
                "host": host,
                "config": host_cfg,
            }
            self.host_combo.addItem(host_cfg.name)

        if previous and previous in self._hosts:
            idx = self.host_combo.findText(previous)
            if idx >= 0:
                self.host_combo.setCurrentIndex(idx)
        elif self.host_combo.count():
            self.host_combo.setCurrentIndex(0)

        if self._hosts:
            self.host_status_label.setText("Ready.")
        else:
            self.host_status_label.setText(
                "No hosts configured. Add one in Settings â†’ Raspberry Pi hosts."
            )

    # --------------------------------------------------------------- helpers
    def _ensure_recorder(self) -> PiRecorder:
        if self._pi_recorder is not None:
            return self._pi_recorder

        details = self.current_host_details()
        if details is None:
            raise RuntimeError("No Raspberry Pi host selected.")

        host, cfg = details
        recorder = PiRecorder(host, cfg.base_path)
        recorder.connect()

        self._pi_recorder = recorder
        self.host_status_label.setText(f"Connected to {host.name}")
        return recorder

    def current_host_details(self) -> tuple[Host, HostConfig] | None:
        """Return the active host credentials and config if available."""
        name = self.host_combo.currentText()
        if not name:
            return None
        entry = self._hosts.get(name)
        if not entry:
            return None
        host = entry.get("host")
        cfg = entry.get("config")
        if not isinstance(host, Host) or not isinstance(cfg, HostConfig):
            return None
        return host, cfg

    def current_remote_data_dir(self) -> Path | None:
        """Remote filesystem root used for recording on the selected Pi."""
        details = self.current_host_details()
        if details is None:
            return None
        _, cfg = details
        return cfg.data_dir

    def last_session_name(self) -> str:
        """Return the last session label requested by the user."""
        return self._last_session_name

    def _load_sampling_config(self) -> SamplingConfig:
        try:
            config = self._sensor_defaults.load()
            sampling = SamplingConfig.from_mapping(config)
        except Exception:
            sampling = SamplingConfig(device_rate_hz=200.0)
        self._sampling_config = sampling
        return sampling

    def _get_default_mpu_dlpf(self) -> int | None:
        """Return the MPU6050 DLPF setting from sensors.yaml."""
        try:
            config = self._sensor_defaults.load()
        except Exception:
            return 3
        sensors = config.get("sensors", {}) if isinstance(config, dict) else {}
        mpu_cfg = dict(sensors.get("mpu6050", {}) or {})
        dlpf = mpu_cfg.get("dlpf")
        try:
            return int(dlpf)
        except (TypeError, ValueError):
            return None

    def _current_sampling(self) -> SamplingConfig:
        if not isinstance(self._sampling_config, SamplingConfig):
            return self._load_sampling_config()
        return self._sampling_config

    def sampling_config(self) -> SamplingConfig:
        """Return the current SamplingConfig used by the recorder UI."""
        return self._current_sampling()

    def set_sampling_config(self, sampling: SamplingConfig) -> None:
        """Replace the active SamplingConfig and refresh dependent labels."""
        self._apply_sampling_config(sampling, notify=True)

    def _apply_sampling_config(self, sampling: SamplingConfig, *, notify: bool = True) -> None:
        previous = self._current_sampling()
        normalized = SamplingConfig(
            device_rate_hz=float(sampling.device_rate_hz),
            mode_key=str(sampling.mode_key),
        )
        changed = (
            not math.isclose(previous.device_rate_hz, normalized.device_rate_hz, rel_tol=1e-6, abs_tol=1e-6)
            or previous.mode_key != normalized.mode_key
        )
        self._sampling_config = normalized
        self._refresh_sampling_labels()
        if notify and changed:
            self.sampling_config_changed.emit(normalized)

    def _refresh_sampling_labels(self) -> None:
        sampling = self._current_sampling()
        display = GuiSamplingDisplay.from_sampling(sampling)
        self.device_rate_label.setText(f"{display.device_rate_hz:.1f} Hz")
        self.record_rate_label.setText(f"{display.record_rate_hz:.1f} Hz")
        self.stream_rate_label.setText(f"{display.stream_rate_hz:.1f} Hz")
        self.mode_label.setText(display.mode_label)
        if hasattr(self._data_buffer, "config"):
            try:
                self._data_buffer.config.sample_rate_hz = display.stream_rate_hz
            except Exception:
                pass

    def target_stream_rate_hz(self) -> float:
        """Return the expected GUI stream rate derived from SamplingConfig."""
        display = GuiSamplingDisplay.from_sampling(self._current_sampling())
        return float(display.stream_rate_hz)

    def compute_stream_every(
        self,
        sample_rate_hz: float | None = None,
        *,
        fallback_every: int | None = None,
        **_legacy_kwargs: object,
    ) -> int:
        """
        Compatibility shim for older code that expects RecorderTab.compute_stream_every().

        Modern code should call :meth:`SamplingConfig.compute_decimation` directly, but
        this keeps historical call sites working by routing everything through the
        shared SamplingConfig state.
        """
        # Swallow legacy keyword arguments such as "recording".
        if _legacy_kwargs:
            _legacy_kwargs.pop("recording", None)

        sampling = getattr(self, "_sampling_config", None)
        if isinstance(sampling, SamplingConfig):
            try:
                decimation = sampling.compute_decimation()
                stream_every = int(decimation.get("stream_decimate", 0))
            except Exception:
                stream_every = 0
            else:
                stream_every = max(stream_every, 1)
            if stream_every >= 1:
                return stream_every

        if fallback_every is not None and fallback_every >= 1:
            return int(fallback_every)

        if sample_rate_hz and sample_rate_hz > 0:
            return max(1, int(round(float(sample_rate_hz) / 25.0)))

        return 1

    def request_coarser_streaming(self) -> None:
        """Adaptive tuning hook (no-op with unified sampling)."""
        logger.info("RecorderTab: sampling decimation is derived from mode; ignoring request")

    def request_finer_streaming(self) -> None:
        """Adaptive tuning hook (no-op with unified sampling)."""
        logger.info("RecorderTab: sampling decimation is derived from mode; ignoring request")

    def current_mpu_gui_config(self) -> MpuGuiConfig:
        sampling = self._current_sampling()
        return MpuGuiConfig(
            enabled=self.mpu_enable_chk.isChecked(),
            rate_hz=sampling.device_rate_hz,
            sensors=self.mpu_sensors_edit.text().strip() or "1,2,3",
            channels=self.mpu_channels_combo.currentData(),
            include_temp=self.mpu_temp_chk.isChecked(),
            limit_duration=self.mpu_limit_duration_chk.isChecked(),
            duration_s=float(self.mpu_duration_spin.value())
            if self.mpu_limit_duration_chk.isChecked()
            else 0.0,
        )

    def _build_pi_logger_config(
        self,
        acquisition: AcquisitionSettings | None = None,
        *,
        session_name: str | None = None,
    ) -> PiLoggerConfig:
        """
        Construct the PiLoggerConfig passed down to ``mpu6050_multi_logger.py``.

        The logger always uses :class:`SamplingConfig` as the single source of
        truth. Decimation for recording and streaming is derived from the
        selected recording mode; no other code path computes ``--stream-every``.
        """

        sampling = self._current_sampling()
        if acquisition is not None:
            sampling = acquisition.sampling

        extra: dict[str, object] = {}
        sensors = self.mpu_sensors_edit.text().strip()
        if sensors:
            extra["sensors"] = sensors

        channels = self.mpu_channels_combo.currentData() or "default"
        extra["channels"] = channels

        dlpf = self._get_default_mpu_dlpf()
        if dlpf is not None:
            extra["dlpf"] = dlpf

        if self.mpu_temp_chk.isChecked():
            extra["temp"] = True

        if self.mpu_limit_duration_chk.isChecked():
            duration_s = float(self.mpu_duration_spin.value())
            if duration_s > 0:
                extra["duration"] = f"{duration_s:.3f}"

        if session_name:
            extra["session_name"] = session_name

        return PiLoggerConfig.from_sampling(sampling, extra_cli=extra)

    # --------------------------------------------------------------- slots
    @Slot()
    def _on_start_clicked(self) -> None:
        try:
            self.start_live_stream(recording=False)
        except Exception as exc:
            self._emit_error(str(exc))

    @Slot()
    def _on_stop_clicked(self) -> None:
        self.stop_live_stream()

    def start_live_stream(
        self,
        recording: bool,
        acquisition: AcquisitionSettings | None = None,
        *,
        session_name: str | None = None,
    ) -> None:
        """
        Called by MainWindow when the live stream should start.

        Uses the current GUI configuration and forwards `recording` down to
        the Pi logger.
        """
        self._recording_mode = bool(recording)
        normalized_session = (session_name or "").strip()
        self._last_session_name = normalized_session

        if acquisition is not None:
            self._apply_sampling_config(acquisition.sampling, notify=True)

        mpu_cfg = self.current_mpu_gui_config()

        if not mpu_cfg.enabled:
            raise RuntimeError("Enable the MPU6050 sensor to start streaming.")

        recorder = self._ensure_recorder()

        if self._ingest_worker is not None:
            raise RuntimeError("MPU6050 streaming is already running.")

        pi_logger_cfg = self._build_pi_logger_config(
            acquisition, session_name=normalized_session or None
        )
        logger_cmd = build_logger_command(pi_logger_cfg)
        logger.debug("mpu6050 command: %s", shlex.join(logger_cmd))
        if len(logger_cmd) > 3:
            cli_tokens = logger_cmd[3:]
        else:
            cli_tokens = build_logger_args(pi_logger_cfg)
        mpu_extra_args = " ".join(shlex.quote(a) for a in cli_tokens)

        self._start_stream(recorder, sensor_type="mpu6050", extra_args=mpu_extra_args)

    def stop_live_stream(
        self,
        *,
        wait: bool = False,
        wait_timeout_ms: int | None = 5000,
    ) -> None:
        """Called by MainWindow when the Live Signals tab requests stop."""

        thread = self._ingest_thread
        self._stop_stream()

        if wait and thread is not None:
            if wait_timeout_ms is None:
                thread.wait()
            else:
                thread.wait(max(0, int(wait_timeout_ms)))

    def _stop_stream(self) -> None:
        worker = self._ingest_worker
        if worker is not None:
            self._stop_requested = True
            QMetaObject.invokeMethod(worker, "stop", Qt.QueuedConnection)
            self.start_btn.setEnabled(True)
            self.stop_btn.setEnabled(False)
            self.overall_status.setText("Stopping stream.")
            self.streaming_stopped.emit()
            self.recording_stopped.emit()
            self.mpu_rate_label.setText("MPU6050 rate: --")
            self._close_active_stream()
        else:
            # No worker running; ensure recorder is closed.
            self._stop_requested = False
            self._close_active_stream()
            if self._pi_recorder is not None:
                try:
                    self._pi_recorder.close()
                except Exception:
                    pass
                self._pi_recorder = None
                self.host_status_label.setText("Disconnected.")
            self.start_btn.setEnabled(True)
            self.stop_btn.setEnabled(False)
            self.overall_status.setText("Idle.")
            self._clear_sample_queue()

    def _close_active_stream(self) -> None:
        stream = self._active_stream
        self._active_stream = None
        if stream is None:
            return

        close = getattr(stream, "close", None)
        if callable(close):
            try:
                close()
            except Exception:
                logger.exception("Failed to close active stream")

    def _start_stream(
        self,
        recorder: PiRecorder,
        sensor_type: str,
        extra_args: str,
    ) -> None:
        if sensor_type != "mpu6050":
            raise ValueError(
                f"RecorderTab only supports 'mpu6050' streams, got {sensor_type!r}"
            )

        self._close_active_stream()
        self._clear_sample_queue()
        parser = select_parser(sensor_type)
        self._stop_requested = False
        rc = self._rate_controllers[sensor_type]
        rc.reset()

        def _stderr_callback(line: str) -> None:
            self._emit_error(f"{sensor_type} stderr: {line}")

        try:
            stream = recorder.stream_mpu6050(
                extra_args=extra_args,
                recording=self._recording_mode,
                on_stderr=_stderr_callback,
            )
        except Exception as exc:
            self._emit_error(f"Failed to start sensor stream: {exc}")
            return

        self._active_stream = stream

        def _stream_factory():
            return stream

        thread = QThread(self)
        worker = SensorIngestWorker(
            recorder=recorder,
            stream_factory=_stream_factory,
            parser=parser,
            batch_size=self._ingest_batch_size,
            max_latency_ms=self._ingest_max_latency_ms,
            stream_label=sensor_type,
        )
        worker.moveToThread(thread)
        thread.started.connect(worker.start)
        worker.samples_batch.connect(self._on_samples_batch)
        worker.error.connect(self._on_ingest_error)
        worker.finished.connect(self._on_ingest_finished)
        worker.finished.connect(worker.deleteLater)
        worker.finished.connect(thread.quit)
        thread.finished.connect(thread.deleteLater)
        thread.start()

        self._ingest_thread = thread
        self._ingest_worker = worker

        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.overall_status.setText("Streaming.")
        self.recording_started.emit()
        self.streaming_started.emit()

    @Slot(list)
    def _on_samples_batch(self, samples: list[MpuSample]) -> None:
        if not samples:
            return

        self._data_buffer.add_samples(samples)
        # Store the batch in the shared StreamingDataBuffer (for Signals/FFT)
        # and also push individual samples into the GUI queue for live plots.

        rc = self._rate_controllers.get("mpu6050")
        updated_rate = False

        for sample in samples:
            if sample is None:
                continue
            if rc is not None:
                t = self._sample_time_seconds(sample)
                if t is not None:
                    rc.add_sample_time(t)
                    updated_rate = True
            self._enqueue_sample(sample)

        if rc is not None and updated_rate:
            self.rate_updated.emit("mpu6050", rc.estimated_hz)

    @Slot(str)
    def _on_ingest_error(self, message: str) -> None:
        self._ingest_had_error = True
        if message:
            self._emit_error(message)

    @Slot()
    def _on_ingest_finished(self) -> None:
        self._ingest_worker = None
        self._ingest_thread = None
        self._close_active_stream()

        if self._pi_recorder is not None:
            try:
                self._pi_recorder.close()
            except Exception:
                pass
            self._pi_recorder = None
            self.host_status_label.setText("Disconnected.")

        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)

        if self._stop_requested:
            self.overall_status.setText("Idle.")
        else:
            if not self._ingest_had_error:
                self._emit_error(
                    "Stream for mpu6050 ended unexpectedly (remote process exited)."
                )
            self.overall_status.setText("Stream ended.")
            self.streaming_stopped.emit()
            self.recording_stopped.emit()
            self.mpu_rate_label.setText("MPU6050 rate: --")

        self._stop_requested = False
        self._ingest_had_error = False
        self._clear_sample_queue()

    def _sample_time_seconds(self, sample: object) -> Optional[float]:
        if isinstance(sample, MpuSample):
            if sample.t_s is not None:
                return float(sample.t_s)
            return sample.timestamp_ns * 1e-9
        return None

    def report_error(self, message: str) -> None:
        """Expose the error-reporting pipeline to other widgets."""
        self._emit_error(message)

    def _emit_error(self, message: str) -> None:
        if not message:
            return
        self.error_reported.emit(message)
        self.recording_error.emit(message)

    @Slot(str)
    def _show_error(self, message: str) -> None:
        # Always reflect the latest remote message in the status bar
        self.overall_status.setText(message)

    @Slot(str, float)
    def _on_rate_updated(self, sensor_type: str, hz: float) -> None:
        if sensor_type == "mpu6050":
            self.mpu_rate_label.setText(f"GUI stream rate: {hz:.1f} Hz")

    @Slot(dict)
    def on_sensors_updated(self, sensors: dict) -> None:
        """
        Slot connected from SettingsTab.sensorsUpdated to keep sampling labels
        in sync with sensors.yaml.
        """
        try:
            sampling = SamplingConfig.from_mapping(sensors)
        except Exception:
            return
        self.set_sampling_config(sampling)

    @Slot(list)
    def on_hosts_updated(self, host_list: list[dict]) -> None:
        """
        Slot connected from SettingsTab.hostsUpdated to refresh the host combo.

        Parameters
        ----------
        host_list:
            Sequence of dictionaries mirroring hosts.yaml["pis"] entries.
        """
        entries: Iterable[Mapping[str, object]] = list(host_list or [])
        self._apply_host_entries(entries, preserve_selection=True)

    def data_buffer(self) -> StreamingDataBuffer:
        """Return the streaming data buffer for other tabs to query."""
        return self._data_buffer

    @property
    def sample_queue(self) -> queue.Queue[object]:
        """Return the queue carrying recent samples for GUI ingestion."""
        return self._sample_queue

    def _enqueue_sample(self, sample: object) -> None:
        """Push a parsed sample into the GUI queue without blocking."""
        try:
            self._sample_queue.put_nowait(sample)
        except queue.Full:
            logger.warning("Sample queue is full; dropping sample.")

    def _clear_sample_queue(self) -> None:
        """Best-effort drain of the GUI-facing sample queue."""
        try:
            while True:
                self._sample_queue.get_nowait()
        except queue.Empty:
            pass

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_settings.py
# File: tab_settings.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 23099 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_settings.py
"""Settings tab for SSH hosts and sensor defaults.

This widget is backed by the YAML files in :mod:`sensepi.config`:

* ``hosts.yaml``      via :class:`sensepi.config.app_config.HostInventory`
* ``sensors.yaml``    via :class:`sensepi.config.app_config.SensorDefaults`

It exposes two signals so other tabs (e.g. RecorderTab) can keep in sync:

* ``hostsUpdated(list[dict])``   â€“ emitted after saving hosts.yaml
* ``sensorsUpdated(dict)``       â€“ emitted after saving sensors.yaml

RecorderTab can either:

* Call :meth:`current_host_config` / :meth:`all_hosts` /
  :meth:`sensor_defaults` directly, or
* Connect to the signals to be notified when the user changes settings.

Example usage in RecorderTab (pseudo-code)::

    settings_tab: SettingsTab = ...

    def on_hosts_updated(hosts: list[dict]) -> None:
        self.populate_host_combo(hosts)

    settings_tab.hostsUpdated.connect(on_hosts_updated)

    # When starting a recording:
    host_cfg = settings_tab.current_host_config()
    sensor_cfg = settings_tab.sensor_defaults()
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

from PySide6.QtCore import Qt, Signal, Slot
from PySide6.QtWidgets import (
    QCheckBox,
    QFileDialog,
    QComboBox,
    QDoubleSpinBox,
    QFormLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QListWidget,
    QMessageBox,
    QPushButton,
    QSpinBox,
    QVBoxLayout,
    QLineEdit,
    QWidget,
)

from ...config.app_config import (
    AppConfig,
    HostConfig,
    HostInventory,
    SensorDefaults,
    build_pi_config_for_host,
)
from ...config.sampling import RECORDING_MODES, SamplingConfig
from ...remote.ssh_client import SSHClient


class SettingsTab(QWidget):
    """GUI tab for managing Pi hosts and sensor defaults."""

    # Emitted after a successful save of the corresponding YAML file
    hostsUpdated = Signal(list)   # list[dict] â€“ entries from hosts.yaml["pis"]
    sensorsUpdated = Signal(dict) # dict      â€“ full sensors.yaml mapping

    def __init__(self, parent: Optional[QWidget] = None) -> None:
        super().__init__(parent)

        self._host_inventory = HostInventory()
        self._sensor_defaults = SensorDefaults()

        # In-memory models mirroring the YAML content
        self._hosts: List[Dict[str, Any]] = []
        self._sensors: Dict[str, Any] = {}

        self._current_host_index: Optional[int] = None

        self._build_ui()
        self._load_from_disk()

    # ------------------------------------------------------------------
    # UI construction
    # ------------------------------------------------------------------
    def _build_ui(self) -> None:
        root = QVBoxLayout(self)

        # ----- Host configuration --------------------------------------
        hosts_group = QGroupBox("Raspberry Pi hosts", self)
        hosts_layout = QHBoxLayout(hosts_group)

        # Left column: host list + add/remove buttons
        host_list_col = QVBoxLayout()
        host_list_col.addWidget(QLabel("Configured Pis:"))

        self.host_list = QListWidget(hosts_group)
        host_list_col.addWidget(self.host_list)

        host_btn_row = QHBoxLayout()
        self.btn_add_host = QPushButton("Add")
        self.btn_remove_host = QPushButton("Remove")
        host_btn_row.addWidget(self.btn_add_host)
        host_btn_row.addWidget(self.btn_remove_host)
        host_list_col.addLayout(host_btn_row)

        hosts_layout.addLayout(host_list_col, 1)

        # Right column: host detail form
        host_form_col = QVBoxLayout()
        form = QFormLayout()

        self.edit_host_name = QLineEdit(hosts_group)
        self.edit_host_address = QLineEdit(hosts_group)
        self.edit_host_user = QLineEdit(hosts_group)

        self.edit_host_port = QSpinBox(hosts_group)
        self.edit_host_port.setRange(1, 65535)
        self.edit_host_port.setValue(22)

        self.edit_password = QLineEdit(hosts_group)
        self.edit_password.setEchoMode(QLineEdit.Password)

        # Base path row: line edit + "Browse..."
        base_row = QHBoxLayout()
        self.edit_base_path = QLineEdit(hosts_group)
        self.btn_browse_base = QPushButton("Browseâ€¦", hosts_group)
        base_row.addWidget(self.edit_base_path)
        base_row.addWidget(self.btn_browse_base)

        self.edit_data_dir = QLineEdit(hosts_group)
        self.edit_pi_config = QLineEdit(hosts_group)

        form.addRow("Name:", self.edit_host_name)
        form.addRow("Host / IP:", self.edit_host_address)
        form.addRow("User:", self.edit_host_user)
        form.addRow("SSH port:", self.edit_host_port)
        form.addRow("Password:", self.edit_password)
        form.addRow("Scripts base path:", base_row)
        form.addRow("Data directory:", self.edit_data_dir)
        form.addRow("Pi config path:", self.edit_pi_config)

        host_form_col.addLayout(form)

        self.btn_save_hosts = QPushButton("Save hosts.yaml", hosts_group)
        self.btn_sync_pi = QPushButton("Sync config to Pi", hosts_group)

        buttons_row = QHBoxLayout()
        buttons_row.addStretch()
        buttons_row.addWidget(self.btn_sync_pi)
        buttons_row.addWidget(self.btn_save_hosts)
        host_form_col.addLayout(buttons_row)

        hosts_layout.addLayout(host_form_col, 2)
        root.addWidget(hosts_group)

        # ----- Sensor defaults ----------------------------------------
        sensors_group = QGroupBox("Sensor defaults", self)
        sensors_layout = QVBoxLayout(sensors_group)

        # Sampling (single source of truth)
        sampling_group = QGroupBox("Sampling", sensors_group)
        sampling_form = QFormLayout(sampling_group)

        self.device_rate_spin = QDoubleSpinBox(sampling_group)
        self.device_rate_spin.setRange(1.0, 4000.0)
        self.device_rate_spin.setDecimals(1)
        self.device_rate_spin.setValue(200.0)

        self.mode_combo = QComboBox(sampling_group)
        for key, mode in RECORDING_MODES.items():
            self.mode_combo.addItem(mode.label, userData=key)

        sampling_form.addRow("Device rate [Hz]:", self.device_rate_spin)
        sampling_form.addRow("Mode:", self.mode_combo)

        # MPU6050 defaults
        mpu_group = QGroupBox("MPU6050", sensors_group)
        mpu_form = QFormLayout(mpu_group)

        self.mpu_channels = QComboBox(mpu_group)
        # Same choices as the logger
        self.mpu_channels.addItems(["default", "acc", "gyro", "both"])

        self.mpu_dlpf = QSpinBox(mpu_group)
        self.mpu_dlpf.setRange(0, 6)
        self.mpu_dlpf.setValue(3)

        self.mpu_include_temp = QCheckBox("Include on-die temperature", mpu_group)

        mpu_form.addRow("Channels:", self.mpu_channels)
        mpu_form.addRow("DLPF:", self.mpu_dlpf)
        mpu_form.addRow("", self.mpu_include_temp)

        sensors_layout.addWidget(sampling_group)
        sensors_layout.addWidget(mpu_group)

        self.btn_save_sensors = QPushButton("Save sensors.yaml", sensors_group)
        sensors_layout.addWidget(self.btn_save_sensors, alignment=Qt.AlignRight)

        root.addWidget(sensors_group)

        # ----- signal wiring ------------------------------------------
        self.host_list.currentRowChanged.connect(self._on_host_row_changed)
        self.btn_add_host.clicked.connect(self._on_add_host)
        self.btn_remove_host.clicked.connect(self._on_remove_host)
        self.btn_browse_base.clicked.connect(self._on_browse_base)
        self.btn_sync_pi.clicked.connect(self._on_sync_to_pi)
        self.btn_save_hosts.clicked.connect(self._on_save_hosts_clicked)
        self.btn_save_sensors.clicked.connect(self._on_save_sensors_clicked)

        self._set_host_fields_enabled(False)

    # ------------------------------------------------------------------
    # Host helpers
    # ------------------------------------------------------------------
    def _set_host_fields_enabled(self, enabled: bool) -> None:
        widgets = [
            self.edit_host_name,
            self.edit_host_address,
            self.edit_host_user,
            self.edit_host_port,
            self.edit_password,
            self.edit_base_path,
            self.btn_browse_base,
            self.btn_remove_host,
            self.edit_data_dir,
            self.edit_pi_config,
            self.btn_sync_pi,
        ]
        for w in widgets:
            w.setEnabled(enabled)

    def _clear_host_fields(self) -> None:
        self.edit_host_name.clear()
        self.edit_host_address.clear()
        self.edit_host_user.clear()
        self.edit_password.clear()
        self.edit_base_path.clear()
        self.edit_data_dir.clear()
        self.edit_pi_config.clear()
        self.edit_host_port.setValue(22)

    def _load_from_disk(self) -> None:
        # --- Hosts -----------------------------------------------------
        try:
            data = self._host_inventory.load()
        except Exception as exc:
            QMessageBox.warning(self, "Config error", f"Could not load hosts.yaml:\n{exc}")
            data = {}

        self._hosts = list(data.get("pis", [])) if isinstance(data, dict) else []
        self._refresh_host_list()

        # --- Sensors ---------------------------------------------------
        try:
            self._sensors = self._sensor_defaults.load()
        except Exception as exc:
            QMessageBox.warning(self, "Config error", f"Could not load sensors.yaml:\n{exc}")
            self._sensors = {}

        self._load_sensor_widgets_from_model()

    def _refresh_host_list(self) -> None:
        self.host_list.blockSignals(True)
        self.host_list.clear()
        for host in self._hosts:
            label = host.get("name") or host.get("host") or "<unnamed>"
            self.host_list.addItem(label)
        self.host_list.blockSignals(False)

        if self._hosts:
            self.host_list.setCurrentRow(0)
        else:
            self._current_host_index = None
            self._set_host_fields_enabled(False)
            self._clear_host_fields()

    @Slot(int)
    def _on_host_row_changed(self, row: int) -> None:
        # Persist edits from previous host
        if self._current_host_index is not None:
            self._update_model_from_host_fields(self._current_host_index)

        if row < 0 or row >= len(self._hosts):
            self._current_host_index = None
            self._set_host_fields_enabled(False)
            self._clear_host_fields()
            return

        self._current_host_index = row
        self._set_host_fields_enabled(True)

        host = self._hosts[row]
        self.edit_host_name.setText(str(host.get("name", "")))
        self.edit_host_address.setText(str(host.get("host", "")))
        self.edit_host_user.setText(str(host.get("user", "")))
        self.edit_password.setText(str(host.get("password", "")))
        self.edit_base_path.setText(str(host.get("base_path", host.get("scripts_dir", ""))))
        self.edit_data_dir.setText(str(host.get("data_dir", "")))
        self.edit_pi_config.setText(str(host.get("pi_config_path", "")))
        self.edit_host_port.setValue(int(host.get("port", 22)))

    def _update_model_from_host_fields(self, index: int) -> None:
        if index < 0 or index >= len(self._hosts):
            return

        original = dict(self._hosts[index])  # keep unknown keys
        name = self.edit_host_name.text().strip()
        host = self.edit_host_address.text().strip()
        user = self.edit_host_user.text().strip()
        password = self.edit_password.text()
        base_path = self.edit_base_path.text().strip()
        data_dir = self.edit_data_dir.text().strip()
        pi_config = self.edit_pi_config.text().strip()
        port = int(self.edit_host_port.value())

        if name:
            original["name"] = name
        else:
            original.pop("name", None)

        if host:
            original["host"] = host
        else:
            original.pop("host", None)

        if user:
            original["user"] = user
        else:
            original.pop("user", None)

        if password:
            # TODO: This stores the password in plain text. Consider a keyring.
            original["password"] = password
        else:
            original.pop("password", None)

        if base_path:
            original["base_path"] = base_path
        else:
            original.pop("base_path", None)

        if data_dir:
            original["data_dir"] = data_dir
        else:
            original.pop("data_dir", None)

        if pi_config:
            original["pi_config_path"] = pi_config
        else:
            original.pop("pi_config_path", None)

        # store port even if default
        original["port"] = port

        self._hosts[index] = original

        item = self.host_list.item(index)
        if item is not None:
            label = original.get("name") or original.get("host") or "<unnamed>"
            item.setText(label)

    @Slot()
    def _on_add_host(self) -> None:
        if self._current_host_index is not None:
            self._update_model_from_host_fields(self._current_host_index)

        new = {
            "name": f"pi-{len(self._hosts) + 1}",
            "host": "raspberrypi.local",
            "user": "pi",
            "password": "",
            "base_path": "~/sensor",
            "data_dir": "~/logs",
            "pi_config_path": "~/sensor/pi_config.yaml",
            "port": 22,
        }
        self._hosts.append(new)
        self._refresh_host_list()
        self.host_list.setCurrentRow(len(self._hosts) - 1)

    @Slot()
    def _on_remove_host(self) -> None:
        row = self.host_list.currentRow()
        if row < 0 or row >= len(self._hosts):
            return
        del self._hosts[row]
        self._refresh_host_list()

    @Slot()
    def _on_browse_base(self) -> None:
        path = QFileDialog.getExistingDirectory(
            self,
            "Select remote scripts directory",
            self.edit_base_path.text() or "",
        )
        if path:
            self.edit_base_path.setText(path)

    @Slot()
    def _on_save_hosts_clicked(self) -> None:
        if self._current_host_index is not None:
            self._update_model_from_host_fields(self._current_host_index)

        try:
            existing = self._host_inventory.load()
        except Exception as exc:
            QMessageBox.warning(self, "Config error", f"Could not reload hosts.yaml:\n{exc}")
            existing = {}

        if not isinstance(existing, dict):
            existing = {}
        existing["pis"] = self._hosts

        try:
            self._host_inventory.save(existing)
        except Exception as exc:
            QMessageBox.critical(self, "Save error", f"Failed to write hosts.yaml:\n{exc}")
            return

        QMessageBox.information(self, "Saved", "Host configuration saved to hosts.yaml.")
        self.hostsUpdated.emit([dict(h) for h in self._hosts])

    @Slot()
    def _on_sync_to_pi(self) -> None:
        host_dict = self.current_host_config()
        if host_dict is None:
            QMessageBox.information(self, "No host", "Select a host to sync.")
            return

        host_cfg = self._host_inventory.to_host_config(host_dict)
        sensor_defaults, sampling_cfg = self._build_sensor_defaults_payload()
        app_cfg = AppConfig(
            sensor_defaults=sensor_defaults,
            sampling_config=sampling_cfg,
        )
        pi_cfg = build_pi_config_for_host(host_cfg, app_cfg)
        contents = pi_cfg.render_pi_config_yaml()

        remote_host = self._host_inventory.to_remote_host(host_dict)
        client = SSHClient(remote_host)
        try:
            client.connect()
        except Exception as exc:
            QMessageBox.critical(self, "SSH error", f"Could not connect: {exc}")
            return

        try:
            if not client.path_exists(str(host_cfg.data_dir)):
                QMessageBox.critical(
                    self,
                    "Validation failed",
                    f"Remote data directory does not exist: {host_cfg.data_dir}",
                )
                return
            if not client.path_exists(str(host_cfg.base_path)):
                QMessageBox.critical(
                    self,
                    "Validation failed",
                    f"Remote scripts directory does not exist: {host_cfg.base_path}",
                )
                return

            with client.sftp() as sftp:
                with sftp.open(str(host_cfg.pi_config_path), "w") as fh:
                    fh.write(contents)
        except Exception as exc:
            QMessageBox.critical(
                self,
                "Sync error",
                f"Failed to upload config to {host_cfg.pi_config_path}:\n{exc}",
            )
            return
        finally:
            client.close()

        QMessageBox.information(
            self,
            "Config synced",
            f"Uploaded configuration to {host_cfg.pi_config_path}.",
        )

    # ------------------------------------------------------------------
    # Sensor defaults helpers
    # ------------------------------------------------------------------
    def _load_sensor_widgets_from_model(self) -> None:
        sampling_cfg = SamplingConfig.from_mapping(self._sensors)
        self.device_rate_spin.setValue(float(sampling_cfg.device_rate_hz))
        idx_mode = self.mode_combo.findData(sampling_cfg.mode_key)
        if idx_mode < 0:
            idx_mode = self.mode_combo.findData("high_fidelity")
        self.mode_combo.setCurrentIndex(max(0, idx_mode))

        sensors = self._sensors.get("sensors", {}) if isinstance(self._sensors, dict) else {}
        mpu_cfg = dict(sensors.get("mpu6050", {}) or {})

        mpu_ch = str(mpu_cfg.get("channels", "default"))
        idx = self.mpu_channels.findText(mpu_ch)
        if idx < 0:
            idx = self.mpu_channels.findText("default")
        self.mpu_channels.setCurrentIndex(idx)

        self.mpu_dlpf.setValue(int(mpu_cfg.get("dlpf", 3)))
        self.mpu_include_temp.setChecked(bool(mpu_cfg.get("include_temperature", False)))

    def _current_sampling_from_widgets(self) -> SamplingConfig:
        mode_key = self.mode_combo.currentData()
        return SamplingConfig(
            device_rate_hz=float(self.device_rate_spin.value()),
            mode_key=str(mode_key or "high_fidelity"),
        )

    def _build_sensor_defaults_payload(self) -> tuple[Dict[str, Any], SamplingConfig]:
        sensors_model = dict(self._sensors) if isinstance(self._sensors, dict) else {}
        sampling_cfg = self._current_sampling_from_widgets()

        sensors_block = dict(sensors_model.get("sensors", {}) or {})
        mpu_cfg = dict(sensors_block.get("mpu6050", {}) or {})
        mpu_cfg.update(
            {
                "channels": str(self.mpu_channels.currentText()),
                "dlpf": int(self.mpu_dlpf.value()),
                "include_temperature": bool(self.mpu_include_temp.isChecked()),
            }
        )
        mpu_cfg.pop("sample_rate_hz", None)
        sensors_block["mpu6050"] = mpu_cfg

        sensors_model["sampling"] = sampling_cfg.to_mapping()["sampling"]
        sensors_model["sensors"] = sensors_block
        sensors_model.pop("mpu6050", None)
        sensors_model.pop("adxl203_ads1115", None)
        return sensors_model, sampling_cfg

    @Slot()
    def _on_save_sensors_clicked(self) -> None:
        sensors, _ = self._build_sensor_defaults_payload()

        try:
            self._sensor_defaults.save(sensors)
        except Exception as exc:
            QMessageBox.critical(self, "Save error", f"Failed to write sensors.yaml:\n{exc}")
            return

        self._sensors = self._sensor_defaults.load()
        QMessageBox.information(self, "Saved", "Sensor defaults saved to sensors.yaml.")
        self.sensorsUpdated.emit(dict(self._sensors))

    # ------------------------------------------------------------------
    # Public helpers for RecorderTab (API points)
    # ------------------------------------------------------------------
    def current_host_config(self) -> Optional[Dict[str, Any]]:
        """
        Return the currently selected host dictionary (as in ``hosts.yaml``).

        A shallow copy is returned so callers can mutate it freely.
        """
        row = self.host_list.currentRow()
        if row < 0 or row >= len(self._hosts):
            return None
        if row == self._current_host_index:
            self._update_model_from_host_fields(row)
        return dict(self._hosts[row])

    def all_hosts(self) -> List[Dict[str, Any]]:
        """Return a list of host dictionaries (copied from the in-memory model)."""
        if self._current_host_index is not None:
            self._update_model_from_host_fields(self._current_host_index)
        return [dict(h) for h in self._hosts]

    def sensor_defaults(self) -> Dict[str, Any]:
        """Return the full sensor-defaults mapping."""
        sensors, _ = self._build_sensor_defaults_payload()
        return dict(sensors)


"""
What this gives you:

A full host editor (list/add/remove, edit name/host/user/password/base_path/port).

Per-sensor defaults UI that directly mirrors sensors.yaml.

Safe load/save that preserves unknown keys in both YAML files.

API surface + signals for RecorderTab:

current_host_config(), all_hosts(), sensor_defaults()

hostsUpdated and sensorsUpdated signals.

Using HostInventory + SensorDefaults in RecorderTab

Once you wire your RecorderTab to know about the SettingsTab, you can:

from ...config.app_config import HostInventory, SensorDefaults

# build Pi host + scripts dir without re-parsing YAML schema
inventory = HostInventory()
hosts = inventory.list_hosts()
host_cfg = hosts[0]
remote_host = inventory.to_remote_host(host_cfg)
scripts_dir = inventory.scripts_dir_for(host_cfg)

# build CLI args from sensor defaults
sensors = SensorDefaults()
mpu_args = sensors.build_mpu6050_cli_args()          # ['--rate', '200', '--channels', 'both', '--dlpf', '3']


You can also combine this with the live SettingsTab:

settings_tab: SettingsTab = ...

host_cfg = settings_tab.current_host_config()
sensor_cfg = settings_tab.sensor_defaults()

inventory = HostInventory()
remote_host = inventory.to_remote_host(host_cfg)
scripts_dir = inventory.scripts_dir_for(host_cfg)

mpu_defaults = sensor_cfg.get("mpu6050", {})

from ...config.app_config import build_mpu6050_cli_args

mpu_args = build_mpu6050_cli_args(mpu_defaults)


All ~ expansion happens right before use, not in the YAML itself.

"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\tabs\tab_signals.py
# File: tab_signals.py (ext: .py
# Dir : src\sensepi\gui\tabs\
# Size: 104952 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\tabs\tab_signals.py
"""Signals tab and live plotting widgets for SensePi."""

from __future__ import annotations

import logging
import math
import queue
import time
from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple, TYPE_CHECKING, cast

from PySide6.QtCore import Signal, Slot, QTimer, Qt, QSignalBlocker
from PySide6.QtWidgets import (
    QCheckBox,
    QComboBox,
    QFormLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QPushButton,
    QSizePolicy,
    QVBoxLayout,
    QWidget,
)
import pyqtgraph as pg
import numpy as np
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg
from matplotlib.axes import Axes
from matplotlib.figure import Figure
from matplotlib.lines import Line2D

from ..perf_metrics import PlotPerfStats
from ..widgets import (
    AcquisitionSettings,
    AcquisitionSettingsWidget,
    CollapsibleSection,
)
from ...config.app_config import AppConfig, PlotPerformanceConfig
from ...config.constants import ENABLE_PLOT_PERF_METRICS
from ...core.timeseries_buffer import (
    NS_PER_SECOND,
    TimeSeriesBuffer,
    calculate_capacity,
    initialize_buffers_for_channels,
    ns_to_seconds,
)
from ...data import BufferConfig, StreamingDataBuffer
from ...sensors.mpu6050 import MpuSample
from ...perf_system import get_process_cpu_percent
from ...tools.debug import debug_enabled
from . import SampleKey

if TYPE_CHECKING:
    from .tab_recorder import RecorderTab

DEFAULT_REFRESH_MODE = "fixed"
DEFAULT_REFRESH_INTERVAL_MS = 50  # 20 Hz default for live traces
# Hard lower bound on the GUI timer interval. Updating more often than ~50 Hz
# brings little perceptual benefit but can chew CPU when many traces are shown.
MIN_REFRESH_INTERVAL_MS = 20
REFRESH_PROFILE_CUSTOM_LABEL = "Custom"
REFRESH_PRESETS: list[tuple[str, int]] = [
    ("Low CPU", 250),
    ("Balanced", DEFAULT_REFRESH_INTERVAL_MS),
    ("High fidelity", 20),
]
STREAM_STALL_THRESHOLD_S = 2.0
DEFAULT_DISPLAY_SLACK_NS = int(0.05 * NS_PER_SECOND)
MANUAL_STATUS_HOLD_S = 1.5

logger = logging.getLogger(__name__)

DEFAULT_CHANNEL_Y_LIMITS: dict[str, tuple[float, float]] = {
    "ax": (-20.0, 20.0),
    "ay": (-20.0, 20.0),
    "az": (-20.0, 20.0),
    "gx": (-500.0, 500.0),
    "gy": (-500.0, 500.0),
    "gz": (-500.0, 500.0),
}
DEFAULT_FALLBACK_Y_LIMITS: tuple[float, float] = (-10.0, 10.0)


class SignalPlotWidgetBase(QWidget):
    """Shared data management and rendering helpers for signal plot widgets."""

    def __init__(self, parent: Optional[QWidget] = None, max_seconds: float = 10.0) -> None:
        super().__init__(parent)

        self._max_seconds = float(max_seconds)
        self._max_rate_hz: float = 500.0
        self._buffer_margin: float = 1.2
        self._buffer_capacity = calculate_capacity(
            self._max_seconds,
            self._max_rate_hz,
            margin=self._buffer_margin,
        )
        self._buffers: Dict[SampleKey, TimeSeriesBuffer] = self._create_buffer_store()
        self._sensor_ids = self._extract_sensor_ids()

        self._visible_channels: Set[str] = set()
        self._channel_order: list[str] = []

        # Per-line visibility filter (keeps backend objects alive while hiding traces)
        self._visible_line_keys: Set[SampleKey] = set()
        self._visible_channel_filter: Set[str] = set()
        self._visibility_filter_active: bool = False

        self._line_width: float = 0.8
        self._max_points_per_trace: int = 2000

        self._base_correction_enabled: bool = False
        self._baseline_offsets: Dict[SampleKey, float] = {}
        self._display_slack_ns: int = 0
        self._latest_timestamp_ns: Optional[int] = None

        self._perf: Optional[PlotPerfStats] = PlotPerfStats() if ENABLE_PLOT_PERF_METRICS else None
        self._latest_gui_receive_ts: Optional[float] = None
        self._target_refresh_hz: Optional[float] = None

        self._lines: Dict[SampleKey, Any] = {}
        self._layout_signature: tuple[tuple[int, ...], tuple[str, ...]] | tuple[()] = ()
        self._needs_layout: bool = True

        self._nominal_sample_rate_hz: float = 200.0
        self._plot_window_samples: int = self._compute_window_samples(
            self._nominal_sample_rate_hz
        )
        self._time_axis: np.ndarray = self._compute_time_axis(
            self._plot_window_samples,
            self._nominal_sample_rate_hz,
        )
        self._plot_buffers: Dict[SampleKey, np.ndarray] = {}
        self._plot_write_counts: Dict[SampleKey, int] = {}
        self._max_subplots: int | None = None
        self._max_lines_per_subplot: int | None = None

    def _create_buffer_store(self) -> Dict[SampleKey, TimeSeriesBuffer]:
        # Only physical sensors 1, 2, 3 exist. Using 0 here created a phantom S0 row.
        return initialize_buffers_for_channels(
            sensor_ids=(1, 2, 3),
            channels=("ax", "ay", "az", "gx", "gy", "gz"),
            window_seconds=self._max_seconds,
            max_rate_hz=self._max_rate_hz,
            margin=self._buffer_margin,
        )

    def _extract_sensor_ids(self) -> list[int]:
        return sorted({sensor_id for sensor_id, _ in self._buffers.keys()})

    def _make_key(self, sensor_id: int, channel: str) -> SampleKey:
        return int(sensor_id), str(channel)

    def _get_buffer(self, key: SampleKey) -> TimeSeriesBuffer | None:
        return self._buffers.get(key)

    def _ensure_buffer(self, key: SampleKey) -> TimeSeriesBuffer:
        buf = self._buffers.get(key)
        if buf is None:
            buf = TimeSeriesBuffer(self._buffer_capacity)
            self._buffers[key] = buf
            sensor_id = int(key[0])
            if sensor_id not in self._sensor_ids:
                self._sensor_ids.append(sensor_id)
                self._sensor_ids.sort()
                self._needs_layout = True
        return buf

    @property
    def window_seconds(self) -> float:
        """Length of the sliding time window shown in the plots."""
        return self._max_seconds

    def _compute_window_samples(self, sample_rate_hz: float) -> int:
        rate = max(1.0, float(sample_rate_hz))
        samples = int(math.ceil(rate * self._max_seconds))
        return max(1, samples)

    def _compute_time_axis(self, window_samples: int, sample_rate_hz: float) -> np.ndarray:
        rate = max(1.0, float(sample_rate_hz))
        count = max(1, int(window_samples))
        return np.arange(count, dtype=np.float64) / rate

    def _time_axis_domain(self) -> tuple[float, float]:
        """Return the (xmin, xmax) bounds in seconds for the rolling window."""

        time_axis = getattr(self, "_time_axis", None)
        if isinstance(time_axis, np.ndarray) and time_axis.size:
            return float(time_axis[0]), float(time_axis[-1])
        return 0.0, self._max_seconds

    @staticmethod
    def _channel_units(channel: str) -> str:
        """Return a human-readable unit for a channel name."""
        ch = channel.lower()
        if ch in {"ax", "ay", "az"}:
            return "m/sÂ²"
        if ch in {"gx", "gy", "gz"}:
            return "deg/s"
        return ""

    # --------------------------------------------------------------- public API
    def clear(self) -> None:
        """Clear all buffered data and reset the plot."""
        self._buffers = self._create_buffer_store()
        self._sensor_ids = self._extract_sensor_ids()
        self._reset_plot_buffers()
        self._baseline_offsets.clear()
        self._latest_timestamp_ns = None
        self._latest_gui_receive_ts = None
        self._clear_canvas()

    def set_display_slack_ns(self, slack_ns: int) -> None:
        """Configure how much to lag the display behind new data to absorb jitter."""
        self._display_slack_ns = max(0, int(slack_ns))

    def set_target_refresh_rate(self, hz: Optional[float]) -> None:
        """Configure the intended refresh rate for FPS/drop metrics."""
        if hz is None:
            self._target_refresh_hz = None
            return
        try:
            value = float(hz)
        except (TypeError, ValueError):
            self._target_refresh_hz = None
            return
        if value <= 0.0:
            self._target_refresh_hz = None
            return
        self._target_refresh_hz = value

    def set_max_points_per_trace(self, max_points: int) -> None:
        """Cap how many samples are rendered per line in the live plots."""
        try:
            value = int(max_points)
        except (TypeError, ValueError):
            return
        self._max_points_per_trace = max(100, value)

    def set_nominal_sample_rate(self, hz: Optional[float]) -> None:
        """Update the nominal sample rate used for the rolling plot buffers."""
        try:
            value = float(hz) if hz is not None else float("nan")
        except (TypeError, ValueError):
            value = float("nan")
        if not math.isfinite(value) or value <= 0.0:
            value = 200.0
        value = max(1.0, value)
        if math.isclose(value, self._nominal_sample_rate_hz, rel_tol=0.05, abs_tol=0.5):
            return
        self._nominal_sample_rate_hz = value
        self._plot_window_samples = self._compute_window_samples(value)
        self._time_axis = self._compute_time_axis(self._plot_window_samples, value)
        self._reset_plot_buffers()
        self._refresh_axes_limits()

    def latest_timestamp_ns(self) -> Optional[int]:
        """Return the last timestamp appended to any buffer."""
        if self._latest_timestamp_ns is not None:
            return self._latest_timestamp_ns
        latest: Optional[int] = None
        for buf in self._buffers.values():
            candidate = buf.latest_timestamp_ns()
            if candidate is None:
                continue
            if latest is None or candidate > latest:
                latest = candidate
        self._latest_timestamp_ns = latest
        return latest

    def live_sensor_ids(self) -> list[int]:
        """
        Return sensor IDs that currently have buffered samples.

        This allows other widgets (e.g. FFT) to know which sensors are active
        when sharing the same ring buffers.
        """
        sensor_ids: Set[int] = set()
        for (sensor_id, _), buf in self._buffers.items():
            if len(buf) > 0:
                sensor_ids.add(sensor_id)
        return sorted(sensor_ids)

    def get_time_series_window(
        self,
        sensor_id: int,
        channel: str,
        window_seconds: float,
    ) -> tuple[np.ndarray, np.ndarray]:
        """
        Return ``(times_s, values)`` arrays for the requested channel.

        The timestamps are converted to floating-point seconds so downstream
        consumers (FFT tab) can compute sample spacing without duplicating the
        ring-buffer plumbing.
        """
        key = self._make_key(sensor_id, channel)
        buf = self._get_buffer(key)
        if buf is None or len(buf) == 0:
            return self._empty_series()

        latest_ns = buf.latest_timestamp_ns()
        if latest_ns is None:
            return self._empty_series()

        try:
            window = float(window_seconds)
        except (TypeError, ValueError):
            window = self._max_seconds
        if window <= 0.0:
            window = self._max_seconds

        window_ns = int(window * NS_PER_SECOND)
        start_ns = max(0, latest_ns - window_ns)
        ts_ns, values = buf.get_window(start_ns, latest_ns)
        if ts_ns.size == 0 or values.size == 0:
            return self._empty_series()

        times_s = ns_to_seconds(ts_ns)
        return times_s, values.copy()

    @staticmethod
    def _empty_series() -> tuple[np.ndarray, np.ndarray]:
        """Return empty arrays for shared buffer queries."""
        return (
            np.empty(0, dtype=np.float64),
            np.empty(0, dtype=np.float64),
        )

    def _reset_plot_buffers(self) -> None:
        """Reset cached rolling buffers used for display rendering."""
        self._plot_buffers.clear()
        self._plot_write_counts.clear()

    def get_perf_snapshot(self) -> dict[str, float]:
        """Return current performance metrics if tracking is enabled."""
        target = float(self._target_refresh_hz) if self._target_refresh_hz else 0.0
        perf = self._perf
        snapshot = {
            "fps": 0.0,
            "achieved_fps": 0.0,
            "avg_frame_ms": 0.0,
            "avg_latency_ms": 0.0,
            "max_latency_ms": 0.0,
            "target_fps": target,
            "approx_dropped_frames_per_sec": 0.0,
        }
        if not (ENABLE_PLOT_PERF_METRICS and perf):
            return snapshot

        fps = perf.compute_fps()
        approx_drop = 0.0
        if target > 0.0:
            approx_drop = max(0.0, target - fps)
        snapshot.update(
            {
                "fps": fps,
                "achieved_fps": fps,
                "avg_frame_ms": perf.avg_frame_ms(),
                "avg_latency_ms": perf.avg_latency_ms(),
                "max_latency_ms": perf.max_latency_ms(),
                "target_fps": target,
                "approx_dropped_frames_per_sec": approx_drop,
            }
        )
        return snapshot

    def set_channel_layout(self, channels: Iterable[str]) -> None:
        """Configure which channel columns should be present in the grid."""
        channels_list = [str(ch) for ch in channels]
        self._visible_channels = set(channels_list)
        self._channel_order = channels_list
        self._needs_layout = True
        self._ensure_layout(self._get_visible_channels())

    def set_visible_channels(
        self,
        visible: Optional[Iterable[SampleKey | str]],
    ) -> None:
        """
        Hide or show individual sensor/channel traces without rebuilding plots.

        ``visible`` may contain ``(sensor_id, channel)`` tuples for explicit
        control or raw channel strings to apply visibility across all sensors.
        Pass ``None`` to reset the filter and show all traces.
        """
        if visible is None:
            self._visibility_filter_active = False
            self._visible_line_keys.clear()
            self._visible_channel_filter.clear()
            self._apply_visibility_to_all_lines()
            return

        explicit_keys: Set[SampleKey] = set()
        channel_filter: Set[str] = set()
        for item in visible:
            if isinstance(item, (tuple, list)) and len(item) == 2:
                sensor_id, channel = item
                explicit_keys.add(self._make_key(sensor_id, channel))
            else:
                channel_filter.add(str(item))

        self._visible_line_keys = explicit_keys
        self._visible_channel_filter = channel_filter
        self._visibility_filter_active = True
        self._apply_visibility_to_all_lines()

    def set_subplot_limits(
        self,
        *,
        max_subplots: Optional[int] = None,
        max_lines_per_subplot: Optional[int] = None,
    ) -> None:
        """Clamp how many subplot slots and traces may be created."""
        normalized_subplots = self._normalize_positive_int(max_subplots)
        normalized_lines = self._normalize_positive_int(max_lines_per_subplot)
        if normalized_lines is None:
            normalized_lines = None
        if normalized_subplots == self._max_subplots and normalized_lines == self._max_lines_per_subplot:
            return
        self._max_subplots = normalized_subplots
        self._max_lines_per_subplot = normalized_lines
        self._needs_layout = True

    @staticmethod
    def _normalize_positive_int(value: Optional[int]) -> int | None:
        if value is None:
            return None
        try:
            normalized = int(value)
        except (TypeError, ValueError):
            return None
        if normalized <= 0:
            return None
        return normalized

    def add_sample(self, sample: MpuSample) -> None:
        """Append a sample from the MPU6050 sensor."""
        if ENABLE_PLOT_PERF_METRICS:
            gui_ts = getattr(sample, "gui_receive_ts", None)
            if gui_ts is not None:
                try:
                    self._latest_gui_receive_ts = float(gui_ts)
                except (TypeError, ValueError):
                    pass

        # Use sensor_id as row index; default to 1 if missing
        sensor_id = int(sample.sensor_id) if sample.sensor_id is not None else 1
        if sample.t_s is not None:
            t_ns = int(round(float(sample.t_s) * NS_PER_SECOND))
        else:
            t_ns = int(sample.timestamp_ns)
        for ch in ("ax", "ay", "az", "gx", "gy", "gz"):
            val = getattr(sample, ch, None)
            if val is None:
                continue
            try:
                v = float(val)
            except (TypeError, ValueError):
                continue
            if math.isnan(v):
                continue
            self._append_point(sensor_id, ch, t_ns, v)

    def redraw(self) -> None:
        """Refresh the live plots (intended to be driven by a QTimer)."""
        perf = self._perf if ENABLE_PLOT_PERF_METRICS else None
        start_ts = time.perf_counter() if perf is not None else None
        try:
            visible_channels = self._get_visible_channels()
            if not visible_channels:
                self._clear_canvas()
                return

            self._ensure_layout(visible_channels)
            if not self._lines:
                return

            slack_samples = self._display_slack_samples()
            time_axis = self._time_axis
            for key in self._lines.keys():
                window_values = self._get_plot_window(key, slack_samples)
                if window_values.size == 0:
                    self._clear_line_data(key)
                    continue
                if self._base_correction_enabled:
                    offset = self._baseline_offsets.get(key, 0.0)
                    if offset:
                        window_values = window_values - offset

                finite_mask = np.isfinite(window_values)
                if not finite_mask.any():
                    self._clear_line_data(key)
                    continue

                times = time_axis[finite_mask]
                values = window_values[finite_mask]

                times_decimated, values_decimated = self._decimate_for_plot(
                    times,
                    values,
                    self._max_points_per_trace,
                )
                if not times_decimated:
                    self._clear_line_data(key)
                    continue
                self._set_line_data(key, times_decimated, values_decimated)

            self._refresh_axes_limits()
        finally:
            if perf is not None and start_ts is not None:
                end_ts = time.perf_counter()
                perf.record_frame(start_ts, end_ts)
                gui_ts = self._latest_gui_receive_ts
                if gui_ts is not None:
                    latency_s = end_ts - gui_ts
                    if 0.0 <= latency_s < 60.0:
                        perf.record_latency(latency_s)

        self._finalize_redraw()

    def _decimate_for_plot(
        self,
        times: Sequence[float],
        values: Sequence[float],
        max_points: int,
    ) -> Tuple[list[float], list[float]]:
        """
        Downsample ``times``/``values`` to at most ``max_points`` samples.

        Uses a simple envelope approach (per-chunk min/max) to preserve spikes
        without incurring heavy computation.
        """
        n = min(len(times), len(values))
        if n == 0:
            return [], []

        try:
            limit = int(max_points)
        except (TypeError, ValueError):
            limit = self._max_points_per_trace
        if limit <= 0:
            limit = 1

        if limit == 1:
            return [float(times[0])], [float(values[0])]

        if n <= limit:
            return (
                [float(times[i]) for i in range(n)],
                [float(values[i]) for i in range(n)],
            )

        last_index = n - 1
        mid_budget = max(0, limit - 2)
        selected_indices: list[int] = [0]

        if mid_budget == 0:
            if last_index != 0:
                selected_indices.append(last_index)
            return (
                [float(times[idx]) for idx in selected_indices],
                [float(values[idx]) for idx in selected_indices],
            )

        chunk_budget = max(1, math.ceil(mid_budget / 2))
        step = max(1, math.ceil(n / chunk_budget))
        mid_added = 0

        for start in range(0, n, step):
            if mid_added >= mid_budget:
                break
            end = min(n, start + step)
            if end - start <= 0:
                continue

            min_idx = start
            max_idx = start
            min_val = float(values[start])
            max_val = min_val

            for idx in range(start + 1, end):
                v = float(values[idx])
                if v < min_val:
                    min_val = v
                    min_idx = idx
                if v > max_val:
                    max_val = v
                    max_idx = idx

            for idx in sorted({min_idx, max_idx}):
                if idx == 0 or idx == last_index:
                    continue
                if idx <= selected_indices[-1]:
                    continue
                selected_indices.append(idx)
                mid_added += 1
                if mid_added >= mid_budget:
                    break

        if selected_indices[-1] != last_index:
            selected_indices.append(last_index)

        times_out = [float(times[idx]) for idx in selected_indices]
        values_out = [float(values[idx]) for idx in selected_indices]
        return times_out, values_out

    def _get_visible_channels(self) -> list[str]:
        return [ch for ch in self._channel_order if ch in self._visible_channels]

    def _apply_subplot_limits(
        self,
        visible_channels: list[str],
    ) -> tuple[list[int], list[str], bool, bool]:
        sensors = list(self._sensor_ids)
        channels = [str(ch) for ch in visible_channels]
        if not sensors or not channels:
            return sensors, channels, False, False
        max_subplots = self._max_subplots
        if not max_subplots or max_subplots <= 0:
            return sensors, channels, False, False
        total = len(sensors) * len(channels)
        if total <= max_subplots:
            return sensors, channels, False, False

        limited_channels = list(channels)
        max_channels = max(1, max_subplots // len(sensors))
        trimmed_channels = False
        if len(limited_channels) > max_channels:
            limited_channels = limited_channels[:max_channels]
            trimmed_channels = True
        if not limited_channels and channels:
            limited_channels = [channels[0]]

        limited_sensors = list(sensors)
        max_sensors = max(1, max_subplots // len(limited_channels))
        trimmed_sensors = False
        if len(limited_sensors) > max_sensors:
            limited_sensors = limited_sensors[:max_sensors]
            trimmed_sensors = True
        return limited_sensors, limited_channels, trimmed_channels, trimmed_sensors

    def _apply_visibility_to_all_lines(self) -> None:
        if not self._lines:
            return
        for key, line in self._lines.items():
            self._line_set_visible(line, self._is_key_visible(key))

    def _is_key_visible(self, key: SampleKey) -> bool:
        if not self._visibility_filter_active:
            return True
        if key in self._visible_line_keys:
            return True
        channel = key[1]
        if channel in self._visible_channel_filter:
            return True
        return False

    def _clear_canvas(self) -> None:
        self._backend_clear()
        self._lines.clear()
        self._layout_signature = ()
        self._needs_layout = True

    def _refresh_axes_limits(self) -> None:
        self._backend_refresh_axes_limits()

    def _ensure_layout(self, visible_channels: list[str]) -> None:
        if not visible_channels:
            self._clear_canvas()
            return
        if not self._sensor_ids:
            self._needs_layout = True
            return
        original_sensor_count = len(self._sensor_ids)
        original_channel_count = len(visible_channels)
        (
            sensor_ids,
            limited_channels,
            trimmed_channels,
            trimmed_sensors,
        ) = self._apply_subplot_limits(visible_channels)
        if not sensor_ids or not limited_channels:
            self._clear_canvas()
            return
        signature: tuple[tuple[int, ...], tuple[str, ...]] = (
            tuple(sensor_ids),
            tuple(limited_channels),
        )
        should_log_limits = (
            (trimmed_channels or trimmed_sensors)
            and (self._needs_layout or signature != self._layout_signature)
        )
        if should_log_limits:
            limit = self._max_subplots
            if trimmed_channels and original_channel_count > len(limited_channels):
                logger.warning(
                    "SignalsTab: reducing visible channels from %d to %d to honor max subplot limit (%s).",
                    original_channel_count,
                    len(limited_channels),
                    limit,
                )
            if trimmed_sensors and original_sensor_count > len(sensor_ids):
                logger.warning(
                    "SignalsTab: reducing visible sensor rows from %d to %d to honor max subplot limit (%s).",
                    original_sensor_count,
                    len(sensor_ids),
                    limit,
                )
        if not self._needs_layout and signature == self._layout_signature:
            return
        self._lines.clear()
        self._backend_rebuild_layout(sensor_ids, limited_channels)
        self._layout_signature = signature
        self._needs_layout = False
        self._apply_visibility_to_all_lines()

    # --------------------------------------------------------------- base correction API
    def enable_base_correction(self, enabled: bool) -> None:
        """Enable or disable baseline subtraction."""
        self._base_correction_enabled = bool(enabled)

    def reset_calibration(self) -> None:
        """Clear all stored baseline offsets."""
        self._baseline_offsets.clear()

    def calibrate_from_buffer(self) -> None:
        """
        Compute per-channel baseline from the most recent time window.

        For each (sensor_id, channel) we take the mean over the same sliding
        window that is used for plotting (self._max_seconds).
        """
        if not self._buffers:
            return

        latest_times = [
            ts
            for buf in self._buffers.values()
            if len(buf) > 0 and (ts := buf.latest_timestamp_ns()) is not None
        ]
        if not latest_times:
            return

        window_ns = int(self._max_seconds * NS_PER_SECOND)
        latest_ns = max(latest_times)
        cutoff_ns = max(0, latest_ns - window_ns)

        new_offsets: Dict[SampleKey, float] = {}
        for key, buf in self._buffers.items():
            if not buf:
                continue
            _, values = buf.get_window(cutoff_ns, latest_ns)
            if values.size == 0:
                continue
            new_offsets[key] = float(values.mean())

        self._baseline_offsets = new_offsets

    # --------------------------------------------------------------- internals
    def _append_point(self, sensor_id: int, channel: str, t_ns: int, value: float) -> None:
        key = self._make_key(sensor_id, channel)
        buf = self._ensure_buffer(key)
        buf.append(t_ns, value)
        self._append_plot_value(key, value)
        if self._latest_timestamp_ns is None or t_ns > self._latest_timestamp_ns:
            self._latest_timestamp_ns = t_ns

    def _append_plot_value(self, key: SampleKey, value: float) -> None:
        """Append a sample to the rolling buffer used for live rendering."""
        window = self._plot_window_samples
        if window <= 0:
            return
        buf = self._ensure_plot_buffer(key)
        write_count = self._plot_write_counts.get(key, 0)
        idx = write_count % window
        buf[idx] = float(value)
        self._plot_write_counts[key] = write_count + 1

    def _ensure_plot_buffer(self, key: SampleKey) -> np.ndarray:
        buf = self._plot_buffers.get(key)
        if buf is not None and buf.shape[0] == self._plot_window_samples:
            return buf
        new_buf = np.full(self._plot_window_samples, np.nan, dtype=np.float64)
        self._plot_buffers[key] = new_buf
        self._plot_write_counts[key] = 0
        return new_buf

    def _display_slack_samples(self) -> int:
        if self._display_slack_ns <= 0:
            return 0
        slack_seconds = self._display_slack_ns / float(NS_PER_SECOND)
        samples = int(round(slack_seconds * self._nominal_sample_rate_hz))
        if samples <= 0:
            return 0
        return min(samples, self._plot_window_samples)

    def _get_plot_window(self, key: SampleKey, slack_samples: int) -> np.ndarray:
        buf = self._plot_buffers.get(key)
        if buf is None:
            return np.empty(0, dtype=np.float64)
        window = self._plot_window_samples
        if window <= 0:
            return np.empty(0, dtype=np.float64)
        write_count = self._plot_write_counts.get(key, 0)
        if write_count <= 0:
            return np.empty(0, dtype=np.float64)
        idx = write_count % window
        window_values = np.roll(buf, -idx).copy()
        if write_count < window:
            invalid = window - int(write_count)
            if invalid > 0:
                window_values[:invalid] = np.nan
        if slack_samples > 0:
            shift = min(slack_samples, window)
            window_values = np.roll(window_values, -shift)
            window_values[-shift:] = np.nan
        return window_values

    def _get_time_axis_domain(self) -> tuple[float, float]:
        return self._time_axis_domain()

    # ------------------------------------------------------------------ backend hooks
    def _set_line_data(self, key: SampleKey, times: Sequence[float], values: Sequence[float]) -> None:
        line = self._lines.get(key)
        if line is None:
            return
        self._line_set_data(line, times, values)

    def _clear_line_data(self, key: SampleKey) -> None:
        line = self._lines.get(key)
        if line is None:
            return
        self._line_clear_data(line)

    def _line_set_data(self, line: Any, times: Sequence[float], values: Sequence[float]) -> None:
        raise NotImplementedError

    def _line_clear_data(self, line: Any) -> None:
        raise NotImplementedError

    def _line_set_visible(self, line: Any, visible: bool) -> None:
        raise NotImplementedError

    def _backend_clear(self) -> None:
        raise NotImplementedError

    def _backend_refresh_axes_limits(self) -> None:
        raise NotImplementedError

    def _backend_rebuild_layout(self, sensor_ids: list[int], visible_channels: list[str]) -> None:
        raise NotImplementedError

    def _finalize_redraw(self) -> None:
        """Hook for subclasses that need to trigger an explicit redraw."""
        return


class SignalPlotWidgetPyQtGraph(SignalPlotWidgetBase):
    """PyQtGraph implementation of the signal plot widget."""

    def __init__(self, parent: Optional[QWidget] = None, max_seconds: float = 10.0) -> None:
        super().__init__(parent=parent, max_seconds=max_seconds)
        layout = QVBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        self._glw = pg.GraphicsLayoutWidget(self)
        layout.addWidget(self._glw)

        self._plots: Dict[SampleKey, pg.PlotItem] = {}
        self._line_pen = pg.mkPen(width=max(1.0, float(self._line_width)))

    def _time_axis_domain(self) -> tuple[float, float]:
        """Expose the base-class time axis helper for Qt's attribute lookup."""
        return super()._time_axis_domain()

    def _line_set_data(self, line: pg.PlotDataItem, times: Sequence[float], values: Sequence[float]) -> None:
        line.setData(times, values)

    def _line_clear_data(self, line: pg.PlotDataItem) -> None:
        line.setData([], [])

    def _line_set_visible(self, line: pg.PlotDataItem, visible: bool) -> None:
        line.setVisible(visible)

    def _backend_clear(self) -> None:
        self._glw.clear()
        self._plots.clear()

    def _backend_refresh_axes_limits(self) -> None:
        xmin, xmax = self._time_axis_domain()
        for plot in self._plots.values():
            plot.setXRange(xmin, xmax, padding=0.0)

    def _backend_rebuild_layout(self, sensor_ids: list[int], visible_channels: list[str]) -> None:
        self._glw.clear()
        self._plots.clear()

        nrows = len(sensor_ids)
        ncols = len(visible_channels)
        xmin, xmax = self._time_axis_domain()

        for row_idx, sid in enumerate(sensor_ids):
            for col_idx, ch in enumerate(visible_channels):
                plot = self._glw.addPlot(row=row_idx, col=col_idx)
                plot.setMenuEnabled(False)
                plot.hideButtons()
                plot.showGrid(x=True, y=True, alpha=0.3)
                plot.setXRange(xmin, xmax, padding=0.0)
                plot.enableAutoRange(x=False, y=True)

                if row_idx == nrows - 1:
                    plot.setLabel("bottom", "Time", units="s")

                unit = self._channel_units(ch)
                base_label = ch.upper()
                if unit:
                    base_label = f"{base_label} [{unit}]"

                if col_idx == 0:
                    # Label rows as S0, S1, S2... regardless of underlying sensor_id
                    plot.setLabel("left", f"S{row_idx}\n{base_label}")
                else:
                    plot.setLabel("left", base_label)

                line = plot.plot([], [], pen=self._line_pen)
                key = self._make_key(sid, ch)
                self._plots[key] = plot
                self._lines[key] = line


class SignalPlotWidgetMatplotlib(SignalPlotWidgetBase):
    """Matplotlib implementation of the signal plot widget."""

    def __init__(self, parent: Optional[QWidget] = None, max_seconds: float = 10.0) -> None:
        super().__init__(parent=parent, max_seconds=max_seconds)
        layout = QVBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        self._figure = Figure(figsize=(6, 6))
        self._canvas = FigureCanvasQTAgg(self._figure)
        layout.addWidget(self._canvas)
        self.use_blit: bool = True
        self._bg_cache: Any | None = None
        self._full_redraw_requested = False
        self._canvas.mpl_connect("draw_event", self._on_canvas_draw)
        self._axes_map: Dict[SampleKey, Axes] = {}
        self._lines_map: Dict[SampleKey, Line2D] = cast(Dict[SampleKey, Line2D], self._lines)
        self._axes_cache: Dict[SampleKey, Axes] = {}
        self._lines_cache: Dict[SampleKey, Line2D] = {}
        self._axis_y_limits: Dict[SampleKey, tuple[float, float]] = {}
        self._channel_initial_ylim: Dict[str, tuple[float, float]] = dict(DEFAULT_CHANNEL_Y_LIMITS)
        self._default_initial_ylim: tuple[float, float] = DEFAULT_FALLBACK_Y_LIMITS
        self._autoscale_margin = 0.1
        self._channel_superset = self._infer_available_channels()
        for sensor_id in self._sensor_ids:
            self._ensure_cached_axes_for_sensor(sensor_id)

    def _on_canvas_draw(self, event: Any) -> None:
        if event is not None and getattr(event, "canvas", None) is not self._canvas:
            return
        if not self.use_blit:
            self._bg_cache = None
            self._full_redraw_requested = False
            return
        self._bg_cache = self._canvas.copy_from_bbox(self._figure.bbox)
        self._full_redraw_requested = False

    def _request_full_redraw(self) -> None:
        self._bg_cache = None
        self._full_redraw_requested = True

    def _set_line_data(self, key: SampleKey, times: Sequence[float], values: Sequence[float]) -> None:
        line = self._lines_map.get(key)
        if line is None:
            return
        self._line_set_data(line, times, values)
        limits_changed = self._update_axis_limits_from_data(key, values)
        if limits_changed:
            self._request_full_redraw()

    def _line_set_data(self, line: Line2D, times: Sequence[float], values: Sequence[float]) -> None:
        line.set_data(times, values)

    def _line_clear_data(self, line: Line2D) -> None:
        line.set_data([], [])

    def _line_set_visible(self, line: Line2D, visible: bool) -> None:
        line.set_visible(visible)

    def _backend_clear(self) -> None:
        for line in self._lines_cache.values():
            line.set_data([], [])
            line.set_visible(False)
        for ax in self._axes_cache.values():
            ax.set_visible(False)
        self._axes_map.clear()
        self._lines_map.clear()
        self._request_full_redraw()
        self._canvas.draw_idle()

    def _backend_refresh_axes_limits(self) -> None:
        xmin, xmax = self._get_time_axis_domain()
        for ax in self._axes_map.values():
            current_xmin, current_xmax = ax.get_xlim()
            if not (
                math.isclose(current_xmin, xmin, rel_tol=1e-9, abs_tol=1e-9)
                and math.isclose(current_xmax, xmax, rel_tol=1e-9, abs_tol=1e-9)
            ):
                ax.set_xlim(xmin, xmax)
                self._request_full_redraw()

    def _backend_rebuild_layout(self, sensor_ids: list[int], visible_channels: list[str]) -> None:
        for channel in visible_channels:
            self._ensure_channel_slot(channel)
        for sensor_id in sensor_ids:
            self._ensure_cached_axes_for_sensor(sensor_id)

        self._axes_map.clear()
        self._lines_map.clear()

        active_sensor_ids = list(sensor_ids)
        if not active_sensor_ids or not visible_channels:
            for ax in self._axes_cache.values():
                ax.set_visible(False)
            for line in self._lines_cache.values():
                line.set_visible(False)
            self._request_full_redraw()
            self._canvas.draw_idle()
            return

        sensor_index = {sid: idx for idx, sid in enumerate(active_sensor_ids)}
        channel_index = {ch: idx for idx, ch in enumerate(visible_channels)}
        nrows = len(active_sensor_ids)
        ncols = len(visible_channels)

        for key, ax in self._axes_cache.items():
            sensor_id, channel = key
            if sensor_id not in sensor_index or channel not in channel_index:
                ax.set_visible(False)
                self._lines_cache[key].set_visible(False)
                continue

            row_idx = sensor_index[sensor_id]
            col_idx = channel_index[channel]
            ax.set_position(self._compute_axes_position(row_idx, col_idx, nrows, ncols))
            ax.set_visible(True)
            self._configure_axis_labels(ax, sensor_id, channel, row_idx, col_idx, nrows, ncols)

            line = self._lines_cache[key]
            line.set_visible(True)
            self._axes_map[key] = ax
            self._lines_map[key] = line

        self._request_full_redraw()
        self._canvas.draw_idle()

    def _finalize_redraw(self) -> None:
        if not self.use_blit or not self._lines_map:
            self._canvas.draw_idle()
            return

        if self._full_redraw_requested or self._bg_cache is None:
            self._full_redraw_requested = False
            self._canvas.draw()
            return

        try:
            self._canvas.restore_region(self._bg_cache)
        except Exception:
            # If restoring fails (e.g., after a resize), do a full redraw.
            self._request_full_redraw()
            self._canvas.draw()
            return

        for line in self._lines_map.values():
            if not line.get_visible():
                continue
            ax = line.axes
            if ax is None or not ax.get_visible():
                continue
            ax.draw_artist(line)

        self._canvas.blit(self._figure.bbox)

    def _infer_available_channels(self) -> list[str]:
        channels = sorted({channel for _, channel in self._buffers.keys()})
        if channels:
            return channels
        return ["ax", "ay", "az", "gx", "gy", "gz"]

    def _ensure_channel_slot(self, channel: str) -> None:
        if channel in self._channel_superset:
            return
        self._channel_superset.append(channel)
        for sensor_id in self._sensor_ids:
            self._ensure_cached_axis(sensor_id, channel)

    def _ensure_cached_axes_for_sensor(self, sensor_id: int) -> None:
        for channel in self._channel_superset:
            self._ensure_cached_axis(sensor_id, channel)

    def _ensure_cached_axis(self, sensor_id: int, channel: str) -> None:
        key = self._make_key(sensor_id, channel)
        if key in self._axes_cache:
            if key not in self._axis_y_limits:
                ax = self._axes_cache[key]
                self._axis_y_limits[key] = ax.get_ylim()
            return
        ax = self._figure.add_axes([0.0, 0.0, 1.0, 1.0])
        ax.set_visible(False)
        ax.grid(True, which="both", alpha=0.3)
        xmin, xmax = self._get_time_axis_domain()
        ax.set_xlim(xmin, xmax)
        initial_limits = self._initial_limits_for_channel(channel)
        ax.set_ylim(*initial_limits)
        line, = ax.plot([], [], linewidth=self._line_width)
        line.set_visible(False)
        self._axes_cache[key] = ax
        self._lines_cache[key] = line
        self._axis_y_limits[key] = initial_limits

    def _configure_axis_labels(
        self,
        ax: Axes,
        sensor_id: int,
        channel: str,
        row_idx: int,
        col_idx: int,
        nrows: int,
        ncols: int,
    ) -> None:
        if row_idx == nrows - 1:
            ax.set_xlabel("Time [s]")
            ax.tick_params(labelbottom=True)
        else:
            ax.set_xlabel("")
            ax.tick_params(labelbottom=False)

        unit = self._channel_units(channel)
        base_label = channel.upper()
        if unit:
            base_label = f"{base_label} [{unit}]"

        if col_idx == 0:
            ax.set_ylabel(f"S{row_idx}\n{base_label}")
        else:
            ax.set_ylabel(base_label)

    def _compute_axes_position(
        self,
        row_idx: int,
        col_idx: int,
        nrows: int,
        ncols: int,
    ) -> list[float]:
        left = 0.08
        right = 0.98
        bottom = 0.07
        top = 0.98
        hpad = 0.03
        vpad = 0.03

        total_width = max(0.0, right - left)
        total_height = max(0.0, top - bottom)
        width = total_width if ncols <= 1 else (total_width - hpad * (ncols - 1)) / ncols
        height = total_height if nrows <= 1 else (total_height - vpad * (nrows - 1)) / nrows

        x = left + col_idx * (width + (0 if ncols <= 1 else hpad))
        y = bottom + (nrows - 1 - row_idx) * (height + (0 if nrows <= 1 else vpad))
        return [x, y, width, height]

    def _initial_limits_for_channel(self, channel: str) -> tuple[float, float]:
        return self._channel_initial_ylim.get(channel, self._default_initial_ylim)

    def _update_axis_limits_from_data(self, key: SampleKey, values: Sequence[float]) -> bool:
        if not values:
            return False
        arr = np.asarray(values, dtype=np.float64)
        if arr.size == 0:
            return False
        finite = arr[np.isfinite(arr)]
        if finite.size == 0:
            return False

        data_min = float(finite.min())
        data_max = float(finite.max())
        ax = self._axes_cache.get(key)
        if ax is None:
            return False

        current_limits = self._axis_y_limits.get(key)
        if current_limits is None:
            current_limits = ax.get_ylim()
            self._axis_y_limits[key] = current_limits

        ymin, ymax = current_limits
        span = max(ymax - ymin, 1e-6)
        new_ymin, new_ymax = ymin, ymax
        changed = False

        if data_max > ymax:
            new_ymax = data_max + span * self._autoscale_margin
            changed = True
        if data_min < ymin:
            new_ymin = data_min - span * self._autoscale_margin
            changed = True

        if changed:
            ax.set_ylim(new_ymin, new_ymax)
            self._axis_y_limits[key] = (new_ymin, new_ymax)
            return True
        return False


# Backwards compatibility alias: existing imports that expect ``SignalPlotWidget``
# will receive the PyQtGraph backend unless explicitly overridden via config.
SignalPlotWidget = SignalPlotWidgetPyQtGraph


def _normalize_signal_backend(name: str | None) -> str:
    if not name:
        return "pyqtgraph"
    normalized = str(name).strip().lower()
    if normalized in {"matplotlib", "mpl"}:
        return "matplotlib"
    if normalized in {"pyqtgraph", "pg", "pyqt"}:
        return "pyqtgraph"
    return "pyqtgraph"


def create_signal_plot_widget(
    parent: Optional[QWidget],
    backend: str | None,
    *,
    max_seconds: float = 10.0,
) -> SignalPlotWidgetBase:
    """Factory that returns a signal-plot widget for the requested backend."""
    normalized = _normalize_signal_backend(backend)
    if normalized == "matplotlib":
        widget = SignalPlotWidgetMatplotlib(parent=parent, max_seconds=max_seconds)
    else:
        if backend is not None and normalized != str(backend).strip().lower():
            logger.warning(
                "Unknown signal plot backend '%s'; falling back to PyQtGraph.",
                backend,
            )
        widget = SignalPlotWidgetPyQtGraph(parent=parent, max_seconds=max_seconds)
    return widget


class SignalsTab(QWidget):
    """
    Tab that embeds a :class:`SignalPlotWidget` and exposes a small
    configuration UI for selecting sensor type and channels.

    Layout: one row per sensor, one column per selected channel.
    """

    start_stream_requested = Signal(bool)  # bool = recording mode
    stop_stream_requested = Signal()
    fft_refresh_interval_changed = Signal(int)

    def __init__(
        self,
        recorder_tab: "RecorderTab | None" = None,
        parent: Optional[QWidget] = None,
        plot_widget: SignalPlotWidgetBase | None = None,
        app_config: AppConfig | None = None,
    ) -> None:
        super().__init__(parent)

        self._recorder_tab: RecorderTab | None = recorder_tab
        self._app_config: AppConfig = app_config or AppConfig()
        plot_perf = getattr(self._app_config, "plot_performance", None)
        if not isinstance(plot_perf, PlotPerformanceConfig):
            plot_perf = PlotPerformanceConfig()
        self._plot_perf_config: PlotPerformanceConfig = plot_perf
        default_refresh_ms = max(
            MIN_REFRESH_INTERVAL_MS,
            int(self._plot_perf_config.signal_refresh_interval_ms()),
        )

        # Refresh configuration
        self.refresh_mode: str = DEFAULT_REFRESH_MODE
        self.refresh_interval_ms: int = default_refresh_ms
        self._sampling_rate_hz: Optional[float] = None
        self._last_follow_interval_ms: Optional[int] = None

        max_window_seconds = self._plot_perf_config.normalized_time_window_s()
        if plot_widget is None:
            plot_widget = SignalPlotWidgetPyQtGraph(max_seconds=max_window_seconds)
        if plot_widget.parent() is None:
            plot_widget.setParent(self)
        self._plot: SignalPlotWidgetBase = plot_widget
        self._plot.set_subplot_limits(
            max_subplots=self._plot_perf_config.normalized_max_subplots(),
            max_lines_per_subplot=self._plot_perf_config.normalized_max_lines(),
        )
        self._plot.set_max_points_per_trace(
            self._plot_perf_config.normalized_max_points()
        )
        self._fallback_data_buffer = StreamingDataBuffer(
            BufferConfig(
                max_seconds=self._plot.window_seconds,
                sample_rate_hz=self._sampling_rate_hz or 200.0,
            )
        )
        self._data_buffer: StreamingDataBuffer | None = None
        self._buffer_cursors: Dict[int | str, float] = {}
        self._synthetic_active = False
        if recorder_tab is not None:
            try:
                self.set_data_buffer(recorder_tab.data_buffer())
            except AttributeError:
                self._data_buffer = None
            host_combo = getattr(recorder_tab, "host_combo", None)
            if host_combo is not None:
                host_combo.currentIndexChanged.connect(self._refresh_mode_hint)
        self._target_refresh_hz: Optional[float] = None
        self._last_refresh_timestamp: float | None = None
        self._smoothed_refresh_hz: float | None = None
        self._timer_tick_counter = 0
        self._timer_measure_window_start = time.perf_counter()
        self._timer_stats_hz = 0.0
        self._channel_checkboxes: Dict[str, QCheckBox] = {}
        self._recording_section: CollapsibleSection | None = None
        self._host_section: CollapsibleSection | None = None
        self._mpu_section: CollapsibleSection | None = None
        self._acquisition_section: CollapsibleSection | None = None
        self._acquisition_widget = AcquisitionSettingsWidget(self)
        self._acquisition_widget.signalsModeChanged.connect(
            self._on_acquisition_mode_changed
        )
        self._acquisition_widget.signalsRefreshChanged.connect(
            self._on_acquisition_refresh_changed
        )
        self._acquisition_widget.fftRefreshChanged.connect(
            self._emit_fft_refresh_interval_changed
        )
        self._acquisition_widget.set_signals_refresh_interval(self.refresh_interval_ms)
        fft_spin_min = int(self._acquisition_widget.fft_refresh_spin.minimum())
        default_fft_interval = max(
            fft_spin_min,
            int(self._plot_perf_config.fft_refresh_interval_ms()),
        )
        self._acquisition_widget.set_fft_refresh_interval(default_fft_interval)
        self._plot.set_display_slack_ns(DEFAULT_DISPLAY_SLACK_NS)
        self._perf_hud_label = QLabel(self._plot)
        self._perf_hud_label.setText("")
        self._perf_hud_label.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        self._perf_hud_label.setMargin(6)
        self._perf_hud_label.setStyleSheet(
            "QLabel {"
            "background-color: rgba(0, 0, 0, 150);"
            "color: white;"
            "font-family: monospace;"
            "font-size: 9pt;"
            "}"
        )
        self._perf_hud_label.setAttribute(Qt.WA_TransparentForMouseEvents, True)
        self._perf_hud_label.move(8, 8)
        self._perf_hud_label.setVisible(False)

        self._stream_active = False
        self._awaiting_first_sample = False
        self._stream_stalled = False
        self._stall_threshold_s = STREAM_STALL_THRESHOLD_S
        self._last_data_monotonic: float = 0.0
        self._manual_status_hold_s = MANUAL_STATUS_HOLD_S
        self._status_source: str = "stream"
        self._last_status_change_monotonic: float = 0.0
        self._synthetic_timer: QTimer | None = None
        self._synthetic_rate_hz: float = 0.0
        self._synthetic_phase: float = 0.0
        self._synthetic_phase_step: float = 0.0
        self._synthetic_sensor_ids: List[int] = [1]
        self._synthetic_interval_ns: int = int(self.refresh_interval_ms * 1_000_000)
        self._synthetic_next_timestamp_ns: int = time.monotonic_ns()
        self._synthetic_start_timestamp_ns: int = self._synthetic_next_timestamp_ns
        self._last_redraw_ms: float = 0.0
        self._redraw_ema_ms: float = 0.0
        self._adaptive_slow_cycles: int = 0
        self._adaptive_fast_cycles: int = 0
        self._queue_ingest_time_acc_ms: float = 0.0
        self._queue_ingest_batches: int = 0
        self._queue_ingest_samples: int = 0
        now = time.perf_counter()
        self._queue_ingest_last_log: float = now
        self._redraw_debug_ema_ms: float = 0.0
        self._redraw_debug_last_log: float = now

        layout = QVBoxLayout(self)

        # Top controls ---------------------------------------------------------
        top_row_group = QGroupBox(self)
        top_row = QHBoxLayout()
        top_row.setSpacing(12)

        sensor_layout = QHBoxLayout()
        sensor_layout.setSpacing(8)
        self.sensor_label = QLabel("Sensor: MPU6050", top_row_group)
        sensor_layout.addWidget(self.sensor_label)
        sensor_layout.addStretch()

        # View preset selector (9 vs 18 charts)
        top_row.addWidget(QLabel("View:", top_row_group))
        self.view_mode_combo = QComboBox(top_row_group)
        self.view_mode_combo.addItem(
            "AX / AY / GZ (9 charts)", userData="default3"
        )
        self.view_mode_combo.addItem(
            "Accel only (AX / AY / AZ)", userData="acc3"
        )
        self.view_mode_combo.addItem(
            "Gyro only (GX / GY / GZ)", userData="gyro3"
        )
        self.view_mode_combo.addItem(
            "All axes (18 charts)", userData="all6"
        )
        top_row.addWidget(self.view_mode_combo)

        self._refresh_profile_custom_index: int | None = None
        self._active_refresh_profile_label: str | None = None
        self._refresh_profile_label = QLabel("Refresh:", top_row_group)
        top_row.addWidget(self._refresh_profile_label)
        self.refresh_profile_combo = QComboBox(top_row_group)
        self.refresh_profile_combo.setToolTip(
            "Preset GUI refresh intervals for the time-domain plots."
        )
        for name, interval in REFRESH_PRESETS:
            hz = int(round(1000.0 / interval)) if interval > 0 else 0
            label = f"{name} ({hz} Hz / {int(interval)} ms)"
            self.refresh_profile_combo.addItem(label, int(interval))
        self._refresh_profile_custom_index = self.refresh_profile_combo.count()
        self.refresh_profile_combo.addItem(
            self._format_custom_refresh_profile_label(self.refresh_interval_ms),
            None,
        )
        self.refresh_profile_combo.currentIndexChanged.connect(
            self._on_refresh_profile_changed
        )
        top_row.addWidget(self.refresh_profile_combo)
        self.adaptive_mode_check = QCheckBox("Adaptive performance", top_row_group)
        self.adaptive_mode_check.setToolTip(
            "Automatically lower plotting/streaming load if redraws become too slow."
        )
        top_row.addWidget(self.adaptive_mode_check)

        self.view_mode_combo.currentIndexChanged.connect(
            self._on_view_mode_changed
        )

        self.recording_check = QCheckBox("Recording", top_row_group)
        self.recording_check.setToolTip(
            "Default is live streaming only. Tick this to also record every "
            "sample on the Pi at the configured rate."
        )
        self._session_name_edit = QLineEdit(top_row_group)
        self._session_name_edit.setPlaceholderText("Session name (optional)")
        self._session_name_edit.setClearButtonEnabled(True)
        self._session_name_edit.setMaximumWidth(200)

        # Base correction controls
        self.base_correction_check = QCheckBox("Base correction", top_row_group)
        self.calibrate_button = QPushButton("Calibrate", top_row_group)

        # Start/stop
        self.start_button = QPushButton("Start", top_row_group)
        self.stop_button = QPushButton("Stop", top_row_group)
        self.stop_button.setEnabled(False)

        # Small info labels: stream rate + calibration window length
        self._stream_rate_label = QLabel("Stream rate: -- Hz", top_row_group)
        self._stream_rate_label.setToolTip(
            "Estimated rate at which samples arrive in this GUI tab after any "
            "Pi-side stream decimation (mpu6050 --stream-every)."
        )
        self._plot_refresh_label = QLabel("Plot refresh: -- Hz", top_row_group)
        self._plot_refresh_label.setToolTip(
            "Approximate refresh/FPS rate achieved by the GUI plot."
        )
        self._base_window_label = QLabel("", top_row_group)

        self.start_button.clicked.connect(self._on_start_clicked)
        self.stop_button.clicked.connect(self._on_stop_clicked)
        self.base_correction_check.stateChanged.connect(
            self._on_base_correction_toggled
        )
        self.calibrate_button.clicked.connect(self._on_calibrate_clicked)
        self.recording_check.stateChanged.connect(self._on_recording_toggled)
        self._session_name_edit.textChanged.connect(self._refresh_mode_hint)

        top_row.addWidget(self.recording_check)
        top_row.addWidget(QLabel("Session:", top_row_group))
        top_row.addWidget(self._session_name_edit)
        top_row.addWidget(self.base_correction_check)
        top_row.addWidget(self.calibrate_button)
        top_row.addWidget(self.start_button)
        top_row.addWidget(self.stop_button)
        top_row.addWidget(self._stream_rate_label)
        top_row.addWidget(self._plot_refresh_label)
        top_row.addWidget(self._base_window_label)
        top_row.addStretch()

        group_layout = QVBoxLayout()
        group_layout.setSpacing(10)
        group_layout.addLayout(sensor_layout)
        group_layout.addLayout(top_row)

        # Short explanatory text under the buttons
        self._mode_hint_label = QLabel("", top_row_group)
        self._mode_hint_label.setWordWrap(True)
        self._mode_hint_label.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        self._mode_hint_label.setSizePolicy(
            QSizePolicy(QSizePolicy.Expanding, QSizePolicy.Maximum)
        )

        hint_row = QHBoxLayout()
        hint_row.setContentsMargins(0, 0, 0, 0)
        hint_row.addWidget(self._mode_hint_label)
        hint_row.addStretch()
        group_layout.addLayout(hint_row)

        top_row_group.setLayout(group_layout)

        self._update_base_window_label()
        self._refresh_mode_hint()

        # Channel selection -----------------------------------------------------
        channel_group = QGroupBox("Channels", self)
        self._channel_layout = QHBoxLayout()
        channel_group.setLayout(self._channel_layout)

        recording_section = CollapsibleSection("Recording / stream controls", self)
        recording_layout = QVBoxLayout()
        recording_layout.setContentsMargins(0, 0, 0, 0)
        recording_layout.addWidget(top_row_group)
        recording_layout.addWidget(channel_group)
        recording_section.setContentLayout(recording_layout)
        layout.addWidget(recording_section)
        self._recording_section = recording_section

        # Acquisition / refresh settings ---------------------------------------
        acquisition_section = CollapsibleSection(
            "Sampling / stream / refresh", self
        )
        acquisition_layout = QVBoxLayout()
        acquisition_layout.setContentsMargins(0, 0, 0, 0)
        acquisition_layout.addWidget(self._acquisition_widget)
        help_label = QLabel(
            "High refresh rates and 'Follow sampling rate' may be heavy on CPU, "
            "especially when many channels are visible.\n"
            "In 'Fixed refresh rate' mode the plot timer runs at the configured "
            "interval regardless of the incoming stream rate.\n"
            "In 'Follow sampling rate' mode the timer interval is derived from "
            "the estimated stream rate reported by the Recorder tab."
        )
        help_label.setWordWrap(True)
        acquisition_layout.addWidget(help_label)
        acquisition_section.setContentLayout(acquisition_layout)
        layout.addWidget(acquisition_section)
        self._acquisition_section = acquisition_section

        # Plot widget -----------------------------------------------------------
        layout.addWidget(self._plot, stretch=1)
        self._perf_hud_label.raise_()

        # Status label ----------------------------------------------------------
        self._status_label = QLabel("", self)
        layout.addWidget(self._status_label)
        self._set_stream_status("Waiting for stream...", force=True)

        self._perf_summary_label = QLabel("", self)
        layout.addWidget(self._perf_summary_label)
        self._update_perf_summary()

        self._rebuild_channel_checkboxes()

        # periodic sample ingestion (decoupled from redraw refresh)
        self._ingest_timer = QTimer(self)
        self._ingest_timer.setInterval(20)
        self._ingest_timer.timeout.connect(self._drain_samples)
        self._ingest_timer.start()

        # periodic redraw of the plot
        self._timer = QTimer(self)
        self._timer.timeout.connect(self._on_redraw_timer)
        self._apply_refresh_settings()
        self._update_refresh_profile_enabled()
        self._perf_hud_timer = QTimer(self)
        self._perf_hud_timer.setInterval(1000)
        self._perf_hud_timer.timeout.connect(self._update_perf_hud)
        self._perf_hud_timer.start()
        self._adaptive_timer = QTimer(self)
        self._adaptive_timer.setInterval(2000)
        self._adaptive_timer.timeout.connect(self._adaptive_step)
        self._adaptive_timer.start()

    # --------------------------------------------------------------- slots
    @Slot()
    def _on_start_clicked(self) -> None:
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self._set_stream_status("Streaming...", force=True)
        self.start_stream_requested.emit(self.recording_check.isChecked())
        if self._recording_section is not None:
            self._recording_section.setCollapsed(True)
        if self._acquisition_section is not None:
            self._acquisition_section.setCollapsed(True)

    @Slot()
    def _on_stop_clicked(self) -> None:
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self._set_manual_status("Stopping...")
        self.stop_stream_requested.emit()
        if self._recording_section is not None:
            self._recording_section.setCollapsed(False)
        if self._acquisition_section is not None:
            self._acquisition_section.setCollapsed(False)
        self._refresh_mode_hint()

    @Slot(int)
    def _on_recording_toggled(self, state: int) -> None:
        self._refresh_mode_hint()

    def current_acquisition_settings(self) -> AcquisitionSettings:
        """Return the current sampling / refresh settings.

        The SamplingConfig inside AcquisitionSettings is now the single
        source of truth for device rate and stream decimation. The
        Recorder tab will derive --stream-every from it, so we don't
        need to tweak anything here.
        """
        return self._acquisition_widget.settings()

    def session_name(self) -> str:
        """Return the normalized session label provided by the user."""
        if not hasattr(self, "_session_name_edit"):
            return ""
        return self._session_name_edit.text().strip()

    def set_data_buffer(
        self, data_buffer: Optional[StreamingDataBuffer]
    ) -> None:
        """Set the StreamingDataBuffer used for timer-driven updates."""
        self._data_buffer = data_buffer
        self._buffer_cursors.clear()

    def live_sensor_ids(self) -> list[int]:
        """
        Expose the sensor IDs with buffered data (shared with FFT tab).
        """
        return self._plot.live_sensor_ids()

    def get_time_series_window(
        self,
        sensor_id: int,
        channel: str,
        window_seconds: float,
    ) -> tuple[np.ndarray, np.ndarray]:
        """
        Return ``(times_s, values)`` arrays for ``sensor_id``/``channel``.

        Delegates to the underlying SignalPlotWidget so other tabs can reuse
        the exact same ring buffers used for the PyQtGraph plots.
        """
        return self._plot.get_time_series_window(sensor_id, channel, window_seconds)

    def _active_data_buffer(self) -> Optional[StreamingDataBuffer]:
        """Return the current data buffer (external or fallback)."""
        return self._data_buffer or self._fallback_data_buffer

    def get_perf_snapshot(self) -> dict[str, float]:
        """
        Combine plot metrics and timer statistics for overlays or logging.
        """
        snap = self._plot.get_perf_snapshot()
        timer_hz = self._timer_stats_hz if ENABLE_PLOT_PERF_METRICS else 0.0
        target = self._target_refresh_hz
        if target is None:
            target = snap.get("target_fps", 0.0) or 0.0
        achieved = snap.get("achieved_fps")
        if achieved is None:
            achieved = snap.get("fps", 0.0)
        if achieved is None:
            achieved = 0.0
        achieved = float(achieved)
        approx_drop = snap.get("approx_dropped_frames_per_sec", 0.0) or 0.0
        if ENABLE_PLOT_PERF_METRICS:
            effective_timer = timer_hz if timer_hz > 0.0 else target
            if effective_timer > 0.0:
                approx_drop = max(0.0, effective_timer - achieved)
        snap.update(
            {
                "target_fps": target,
                "achieved_fps": achieved,
                "timer_hz": timer_hz,
                "approx_dropped_frames_per_sec": approx_drop,
            }
        )
        return snap

    def set_perf_hud_visible(self, visible: bool) -> None:
        """Show or hide the lightweight performance HUD overlay."""
        self._perf_hud_label.setVisible(visible)
        if visible:
            self._update_perf_hud()

    # --------------------------------------------------------------- helpers
    def _set_status_text(self, text: str, *, source: str) -> None:
        self._status_label.setText(text)
        self._status_source = source
        self._last_status_change_monotonic = time.monotonic()

    def _set_stream_status(self, text: str, *, force: bool = False) -> None:
        if (
            not force
            and self._status_source == "manual"
            and (time.monotonic() - self._last_status_change_monotonic)
            < self._manual_status_hold_s
        ):
            return
        self._set_status_text(text, source="stream")

    def _set_manual_status(self, text: str) -> None:
        self._set_status_text(text, source="manual")

    def _remote_destination_text(self) -> str:
        """Return a user-facing description of the remote recording folder."""
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is None:
            return ""
        try:
            remote_dir = recorder.current_remote_data_dir()
        except AttributeError:
            return ""
        if remote_dir is None:
            return ""
        return remote_dir.as_posix()

    def _refresh_mode_hint(self) -> None:
        label = getattr(self, "_mode_hint_label", None)
        if label is None:
            return
        dest = self._remote_destination_text()
        session = " ".join(self.session_name().split())
        if self.recording_check.isChecked():
            hint = "Recording enabled. "
            if dest:
                hint += f"Data is written to {dest} on the Pi."
            else:
                hint += "Data is written to the configured Pi logs directory."
        else:
            hint = "Streaming only (samples are not stored on the Pi). "
            if dest:
                hint += f"Enable recording to write into {dest}."
        if session:
            hint += f" Session name: {session}."
        label.setText(hint.strip())
        self._update_recording_indicator()

    def _update_recording_indicator(self) -> None:
        wants_recording = self.recording_check.isChecked()
        style = ""
        if wants_recording and self._stream_active:
            style = (
                "QPushButton {"
                "background-color: #b3202c;"
                "color: white;"
                "font-weight: bold;"
                "}"
            )
        self.start_button.setStyleSheet(style)

    def _update_base_window_label(self) -> None:
        seconds = self._plot.window_seconds
        self._base_window_label.setText(f"Base/cali window: last {seconds:.1f} s")

    def _extract_sample_timestamp_ns(self, sample: MpuSample) -> Optional[int]:
        if sample.t_s is not None:
            try:
                return int(round(float(sample.t_s) * NS_PER_SECOND))
            except (TypeError, ValueError):
                return None
        try:
            return int(sample.timestamp_ns)
        except (TypeError, ValueError):
            return None

    def _sample_time_seconds(self, sample: MpuSample) -> Optional[float]:
        ts_ns = self._extract_sample_timestamp_ns(sample)
        if ts_ns is None:
            return None
        return ts_ns / float(NS_PER_SECOND)

    @Slot(str, float)
    def update_stream_rate(self, sensor_type: str, hz: float) -> None:
        """Update the GUI-side stream rate shown in the Signals tab.

        ``hz`` reflects the effective rate at which samples arrive in the GUI
        after any Pi-side stream decimation (for example, ``mpu6050 --stream-every N``).
        The Recorder tab emits this signal whenever it refreshes its rate estimate.
        """
        if sensor_type != "mpu6050":
            return
        self._stream_rate_label.setText(f"Stream rate: {hz:.1f} Hz")
        self._sampling_rate_hz = hz
        self._plot.set_nominal_sample_rate(hz)
        if self.refresh_mode == "follow_sampling_rate":
            # Rate updates from RecorderTab may frequently adjust the timer
            # interval when we're following the stream rate.
            self._apply_refresh_settings()
        else:
            self._update_perf_summary()

    @property
    def fixed_interval_ms(self) -> int:
        return self.refresh_interval_ms

    @fixed_interval_ms.setter
    def fixed_interval_ms(self, value: int) -> None:
        interval = max(MIN_REFRESH_INTERVAL_MS, int(value))
        self.refresh_interval_ms = interval
        if self.refresh_mode == "fixed":
            self._apply_refresh_settings()

    @Slot(int)
    def _on_acquisition_refresh_changed(self, interval_ms: int) -> None:
        """Update the timer interval when the acquisition widget spin changes."""
        self.fixed_interval_ms = max(MIN_REFRESH_INTERVAL_MS, int(interval_ms))

    @Slot(str)
    def _on_acquisition_mode_changed(self, mode: str) -> None:
        """Switch between fixed and adaptive refresh modes from the UI."""
        stream_rate = self._get_sampling_rate_hz()
        self.set_refresh_mode(mode, stream_rate)

    @Slot(int)
    def _emit_fft_refresh_interval_changed(self, interval_ms: int) -> None:
        """Relay FFT refresh adjustments so MainWindow can reconfigure FftTab."""
        self.fft_refresh_interval_changed.emit(int(interval_ms))

    @Slot(int)
    def _on_refresh_profile_changed(self, index: int) -> None:
        combo = getattr(self, "refresh_profile_combo", None)
        if combo is None or index < 0:
            return
        interval_data = combo.itemData(index)
        if interval_data is None:
            # Custom entry - editing happens via the acquisition widget spin box.
            return
        interval_ms = max(MIN_REFRESH_INTERVAL_MS, int(interval_data))
        self._acquisition_widget.set_signals_refresh_interval(interval_ms)
        was_fixed = self.refresh_mode == "fixed"
        self.fixed_interval_ms = interval_ms
        if not was_fixed:
            self.set_refresh_mode("fixed")

    def _rebuild_channel_checkboxes(self) -> None:
        # Clear previous
        while self._channel_layout.count():
            item = self._channel_layout.takeAt(0)
            w = item.widget()
            if w:
                w.setParent(None)

        self._channel_checkboxes.clear()

        # Use view preset:
        #   - "default3" => AX, AY, GZ (3 columns; matches Pi --channels default)
        #   - "acc3"     => AX, AY, AZ (accelerometer only)
        #   - "gyro3"    => GX, GY, GZ (gyro only)
        #   - "all6"     => AX, AY, AZ, GX, GY, GZ (6 columns)
        view_mode = (
            self.view_mode_combo.currentData()
            if hasattr(self, "view_mode_combo")
            else "all6"
        )
        if view_mode == "default3":
            channels = ["ax", "ay", "gz"]
        elif view_mode == "acc3":
            channels = ["ax", "ay", "az"]
        elif view_mode == "gyro3":
            channels = ["gx", "gy", "gz"]
        else:
            channels = ["ax", "ay", "az", "gx", "gy", "gz"]

        for ch in channels:
            cb = QCheckBox(ch)
            cb.setChecked(True)
            cb.stateChanged.connect(self._on_channel_toggles_changed)
            self._channel_checkboxes[ch] = cb
            self._channel_layout.addWidget(cb)

        self._plot.set_channel_layout(channels)
        self._channel_layout.addStretch(1)
        self._on_channel_toggles_changed()

    @Slot()
    def _on_view_mode_changed(self) -> None:
        """Called when the view preset combo changes."""
        self._rebuild_channel_checkboxes()

    @Slot()
    def _on_channel_toggles_changed(self) -> None:
        # Preserve the checkbox order as the column order
        visible = [
            ch for ch, cb in self._channel_checkboxes.items() if cb.isChecked()
        ]
        self._plot.set_visible_channels(visible)

    def set_refresh_mode(
        self, mode: str, stream_rate_hz: Optional[float] = None
    ) -> None:
        """
        Configure how often the plot is refreshed.

        ``mode`` accepts ``"fixed"`` (use ``self.refresh_interval_ms``) or
        ``"adaptive"``/``"follow_sampling_rate"`` which uses ``stream_rate_hz``.
        """
        normalized = self._normalize_refresh_mode(mode)
        self.refresh_mode = normalized
        if stream_rate_hz is not None:
            self._sampling_rate_hz = stream_rate_hz
            self._plot.set_nominal_sample_rate(stream_rate_hz)
        self._apply_refresh_settings()
        self._update_refresh_profile_enabled()

    def _ingest_buffer_data(self) -> None:
        """Append new samples from the active StreamingDataBuffer to the plot buffers."""
        data_buffer = self._active_data_buffer()
        if data_buffer is None:
            return

        sensor_ids = data_buffer.get_sensor_ids()
        if not sensor_ids:
            return

        window_s = self._plot.window_seconds
        for sensor_id in sensor_ids:
            samples = data_buffer.get_recent_samples(sensor_id, seconds=window_s)
            if not samples:
                continue
            last_seen = self._buffer_cursors.get(sensor_id)
            updated_last = last_seen
            for sample in samples:
                ts_s = self._sample_time_seconds(sample)
                if ts_s is None:
                    continue
                if updated_last is not None and ts_s <= updated_last:
                    continue
                if ENABLE_PLOT_PERF_METRICS:
                    try:
                        sample.gui_receive_ts = time.perf_counter()
                    except Exception:
                        pass
                self._plot.add_sample(sample)
                updated_last = ts_s
                self._handle_ingested_sample(sample)
            if updated_last is not None:
                self._buffer_cursors[sensor_id] = updated_last

    def _handle_ingested_sample(self, sample: MpuSample) -> None:
        """Update stream state tracking when new data arrives."""
        if not self._stream_active:
            return

        timestamp_ns = self._extract_sample_timestamp_ns(sample)
        if timestamp_ns is None:
            return

        self._last_data_monotonic = time.monotonic()
        if self._awaiting_first_sample:
            self._awaiting_first_sample = False
            self._set_stream_status("Streaming...")
        if self._stream_stalled:
            self._stream_stalled = False
            self._set_stream_status("Streaming...")

    @Slot()
    def _on_redraw_timer(self) -> None:
        start = time.perf_counter()
        self.update_plot()
        elapsed_ms = (time.perf_counter() - start) * 1000.0
        self._last_redraw_ms = elapsed_ms
        alpha = 0.2
        if self._redraw_ema_ms <= 0.0:
            self._redraw_ema_ms = elapsed_ms
        else:
            self._redraw_ema_ms = alpha * elapsed_ms + (1.0 - alpha) * self._redraw_ema_ms

    @Slot()
    def update_plot(self) -> None:
        """Timer slot that refreshes the SignalPlotWidget."""
        # First update the stream status (waiting / stalled / streaming),
        # then ask the plotting backend to redraw the latest buffered data.
        debug_on = debug_enabled()
        if ENABLE_PLOT_PERF_METRICS:
            self._record_timer_tick()
        if self._stream_active:
            if self._awaiting_first_sample:
                self._set_stream_status("Waiting for data...")
            else:
                stalled = False
                if self._last_data_monotonic > 0.0:
                    stalled = (time.monotonic() - self._last_data_monotonic) >= self._stall_threshold_s

                if stalled:
                    if not self._stream_stalled:
                        self._stream_stalled = True
                        self._set_stream_status("No recent data (stream paused?)", force=True)
                elif self._stream_stalled:
                    self._stream_stalled = False
                    self._set_stream_status("Streaming...")
        redraw_start = time.perf_counter() if debug_on else 0.0
        self._plot.redraw()
        if debug_on:
            redraw_ms = (time.perf_counter() - redraw_start) * 1000.0
            alpha = 0.2
            if self._redraw_debug_ema_ms <= 0.0:
                self._redraw_debug_ema_ms = redraw_ms
            else:
                self._redraw_debug_ema_ms = (
                    alpha * redraw_ms + (1.0 - alpha) * self._redraw_debug_ema_ms
                )
            now_perf = time.perf_counter()
            if now_perf - self._redraw_debug_last_log >= 5.0:
                interval_ms = self._timer.interval() if hasattr(self, "_timer") else 0
                print(
                    f"[DEBUG] signals_redraw interval={interval_ms} ms emaâ‰ˆ{self._redraw_debug_ema_ms:.2f} ms",
                    flush=True,
                )
                self._redraw_debug_last_log = now_perf
        self._record_refresh_tick()

    def _adaptive_step(self) -> None:
        """Periodic background check that nudges refresh/stream fidelity."""
        self._update_perf_summary()
        checkbox = getattr(self, "adaptive_mode_check", None)
        if checkbox is None or not checkbox.isChecked():
            self._adaptive_slow_cycles = 0
            self._adaptive_fast_cycles = 0
            return
        if not (self._stream_active or self._synthetic_active):
            return
        redraw_ms = self._redraw_ema_ms
        if redraw_ms <= 0.0:
            return
        target_interval_ms = self._compute_refresh_interval()
        if target_interval_ms <= 0:
            return
        stream_rate = float(self._sampling_rate_hz or 0.0)
        target_stream = float(self._target_stream_rate_from_recorder() or 0.0)
        max_frame_ms = target_interval_ms * 0.9
        redraw_slow = redraw_ms > max_frame_ms
        overloaded_stream = False
        if target_stream > 0.0 and stream_rate > 0.0:
            overloaded_stream = stream_rate > target_stream * 1.15
        comfortable = redraw_ms < target_interval_ms * 0.6
        more_stream_allowed = (
            target_stream > 0.0
            and stream_rate > 0.0
            and stream_rate < target_stream * 0.8
        )

        if redraw_slow or overloaded_stream:
            self._adaptive_slow_cycles += 1
            self._adaptive_fast_cycles = 0
            if self._adaptive_slow_cycles >= 2:
                if self._lower_fidelity():
                    self._adaptive_slow_cycles = 0
            return

        self._adaptive_slow_cycles = 0
        if comfortable:
            self._adaptive_fast_cycles += 1
        else:
            self._adaptive_fast_cycles = 0

        if comfortable and self._adaptive_fast_cycles >= 3:
            if self._maybe_increase_fidelity(allow_stream_tuning=more_stream_allowed):
                self._adaptive_fast_cycles = 0

    def _lower_fidelity(self) -> bool:
        """Try to reduce GUI/stream demand when frames are too slow."""
        adjusted = False
        widget = getattr(self, "_acquisition_widget", None)
        if self.refresh_mode == "fixed" and widget is not None:
            current = int(self.refresh_interval_ms)
            new_interval = min(500, int(max(MIN_REFRESH_INTERVAL_MS, math.ceil(current * 1.5))))
            if new_interval > current:
                widget.set_signals_refresh_interval(int(new_interval))
                self.fixed_interval_ms = int(new_interval)
                adjusted = True
        if adjusted:
            self._update_perf_summary()
            return True
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is not None and hasattr(recorder, "request_coarser_streaming"):
            try:
                recorder.request_coarser_streaming()
                return True
            except Exception:
                logger.exception("SignalsTab: failed to request coarser streaming")
        return False

    def _maybe_increase_fidelity(self, *, allow_stream_tuning: bool) -> bool:
        """If redraws are comfortably fast, cautiously improve fidelity."""
        adjusted = False
        widget = getattr(self, "_acquisition_widget", None)
        if self.refresh_mode == "fixed" and widget is not None:
            current = int(self.refresh_interval_ms)
            if current > MIN_REFRESH_INTERVAL_MS:
                reduced = max(MIN_REFRESH_INTERVAL_MS, int(current * 0.8))
                if reduced < current:
                    widget.set_signals_refresh_interval(int(reduced))
                    self.fixed_interval_ms = int(reduced)
                    adjusted = True
        if adjusted:
            self._update_perf_summary()
            return True
        if not allow_stream_tuning:
            return False
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is not None and hasattr(recorder, "request_finer_streaming"):
            try:
                recorder.request_finer_streaming()
                return True
            except Exception:
                logger.exception("SignalsTab: failed to request finer streaming")
        return False

    def _drain_samples(self) -> None:
        """Drain queued samples from RecorderTab and append them to the plots."""
        if not self._stream_active and not self._synthetic_active:
            return

        queue_obj = self._recorder_sample_queue() if self._stream_active else None
        if queue_obj is None:
            # Fall back to the shared StreamingDataBuffer (synthetic data or tests)
            self._ingest_buffer_data()
            return

        # Pull as many samples as are currently queued by RecorderTab; this keeps
        # ingest work in short bursts driven by the GUI timer instead of per-sample.
        drained: list[object] = []
        try:
            while True:
                drained.append(queue_obj.get_nowait())
        except queue.Empty:
            pass

        if not drained:
            return

        debug_on = debug_enabled()
        ingest_start = time.perf_counter() if debug_on else 0.0
        processed_samples = 0

        for sample in drained:
            if not isinstance(sample, MpuSample):
                continue
            if ENABLE_PLOT_PERF_METRICS:
                try:
                    sample.gui_receive_ts = time.perf_counter()
                except Exception:
                    pass
            self._plot.add_sample(sample)
            self._handle_ingested_sample(sample)
            processed_samples += 1

        if debug_on and processed_samples > 0:
            elapsed_ms = (time.perf_counter() - ingest_start) * 1000.0
            self._queue_ingest_time_acc_ms += elapsed_ms
            self._queue_ingest_batches += 1
            self._queue_ingest_samples += processed_samples
            now_perf = time.perf_counter()
            if now_perf - self._queue_ingest_last_log >= 5.0:
                avg_batch_ms = self._queue_ingest_time_acc_ms / max(
                    1, self._queue_ingest_batches
                )
                avg_us_per_sample = (
                    self._queue_ingest_time_acc_ms
                    / max(1, self._queue_ingest_samples)
                    * 1000.0
                )
                print(
                    "[DEBUG] queue_ingest samples="
                    f"{self._queue_ingest_samples} batches={self._queue_ingest_batches} "
                    f"avgâ‰ˆ{avg_batch_ms:.3f} ms/batch â‰ˆ{avg_us_per_sample:.3f} Âµs/sample",
                    flush=True,
                )
                self._queue_ingest_time_acc_ms = 0.0
                self._queue_ingest_batches = 0
                self._queue_ingest_samples = 0
                self._queue_ingest_last_log = now_perf

    def _recorder_sample_queue(self) -> queue.Queue[object] | None:
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is None:
            return None
        try:
            queue_obj = recorder.sample_queue
        except AttributeError:
            return None
        return queue_obj

    def _normalize_refresh_mode(self, mode: Optional[str]) -> str:
        if mode in {"fixed", "follow_sampling_rate"}:
            return str(mode)
        if mode == "adaptive":
            return "follow_sampling_rate"
        return DEFAULT_REFRESH_MODE

    def _get_sampling_rate_hz(self) -> Optional[float]:
        if self._sampling_rate_hz is not None:
            return self._sampling_rate_hz
        return None

    def _target_stream_rate_from_recorder(self) -> Optional[float]:
        recorder = getattr(self, "_recorder_tab", None)
        if recorder is None:
            return None
        getter = getattr(recorder, "target_stream_rate_hz", None)
        if callable(getter):
            try:
                value = getter()
            except Exception:
                return None
            try:
                return float(value)
            except (TypeError, ValueError):
                return None
        widget = getattr(recorder, "mpu_target_stream_rate", None)
        if widget is not None:
            try:
                return float(widget.value())
            except (TypeError, ValueError):
                return None
        return None

    def _sync_refresh_profile_combo(self, interval_ms: int) -> None:
        combo = getattr(self, "refresh_profile_combo", None)
        if combo is None:
            return
        idx = combo.findData(int(interval_ms))
        with QSignalBlocker(combo):
            if idx >= 0:
                combo.setCurrentIndex(idx)
                interval_data = combo.itemData(idx)
                self._active_refresh_profile_label = self._preset_name_for_interval(
                    int(interval_data)
                )
            elif self._refresh_profile_custom_index is not None:
                self._set_custom_refresh_profile_label(interval_ms)
                combo.setCurrentIndex(self._refresh_profile_custom_index)
                self._active_refresh_profile_label = REFRESH_PROFILE_CUSTOM_LABEL
            else:
                self._active_refresh_profile_label = REFRESH_PROFILE_CUSTOM_LABEL

    def _set_custom_refresh_profile_label(self, interval_ms: int) -> None:
        if self._refresh_profile_custom_index is None:
            return
        combo = getattr(self, "refresh_profile_combo", None)
        if combo is None:
            return
        combo.setItemText(
            self._refresh_profile_custom_index,
            self._format_custom_refresh_profile_label(interval_ms),
        )

    def _format_custom_refresh_profile_label(self, interval_ms: int) -> str:
        if interval_ms <= 0:
            return REFRESH_PROFILE_CUSTOM_LABEL
        return f"{REFRESH_PROFILE_CUSTOM_LABEL} ({interval_ms} ms)"

    def _update_refresh_profile_enabled(self) -> None:
        combo = getattr(self, "refresh_profile_combo", None)
        label = getattr(self, "_refresh_profile_label", None)
        enabled = self.refresh_mode == "fixed"
        tooltip = (
            "Preset refresh rates are available only when using a fixed interval."
            if not enabled
            else "Preset GUI refresh rates for the time-domain plots."
        )
        if combo is not None:
            combo.setEnabled(enabled)
            combo.setToolTip(tooltip)
        if label is not None:
            label.setEnabled(enabled)
        if not enabled:
            self._active_refresh_profile_label = None

    def _refresh_profile_descriptor(self) -> str:
        if self.refresh_mode == "follow_sampling_rate":
            return "Follow stream rate"
        if self._active_refresh_profile_label:
            return self._active_refresh_profile_label
        return REFRESH_PROFILE_CUSTOM_LABEL

    @staticmethod
    def _preset_name_for_interval(interval_ms: int) -> str | None:
        for name, preset_interval in REFRESH_PRESETS:
            if int(preset_interval) == int(interval_ms):
                return name
        return None

    def _compute_refresh_interval(self) -> int:
        if self.refresh_mode == "follow_sampling_rate":
            rate_hz = self._get_sampling_rate_hz()
            if not rate_hz or rate_hz <= 0:
                return DEFAULT_REFRESH_INTERVAL_MS

            interval_ms = int(1000.0 / rate_hz)
            if interval_ms < MIN_REFRESH_INTERVAL_MS:
                # Guard against extremely fast redraws when the device samples
                # faster than humans can meaningfully perceive.
                interval_ms = MIN_REFRESH_INTERVAL_MS
            return interval_ms

        return int(self.refresh_interval_ms)

    def _reset_timer_stats(self) -> None:
        self._timer_tick_counter = 0
        self._timer_measure_window_start = time.perf_counter()
        self._timer_stats_hz = 0.0

    def _record_timer_tick(self) -> None:
        now = time.perf_counter()
        # Initialize the window start if this is the first tick.
        if self._timer_measure_window_start <= 0.0:
            self._timer_measure_window_start = now
        self._timer_tick_counter += 1
        elapsed = now - self._timer_measure_window_start
        if elapsed >= 1.0:
            self._timer_stats_hz = self._timer_tick_counter / elapsed
            self._timer_tick_counter = 0
            self._timer_measure_window_start = now

    def _reset_refresh_measurement(self) -> None:
        self._last_refresh_timestamp = None
        self._smoothed_refresh_hz = None
        self._update_plot_refresh_label()

    def _record_refresh_tick(self) -> None:
        now = time.monotonic()
        last = self._last_refresh_timestamp
        if last is not None:
            dt = now - last
            if dt > 0.0:
                inst_hz = 1.0 / dt
                smoothed = self._smoothed_refresh_hz
                if smoothed is None:
                    self._smoothed_refresh_hz = inst_hz
                else:
                    alpha = 0.2
                    self._smoothed_refresh_hz = alpha * inst_hz + (1.0 - alpha) * smoothed
        self._last_refresh_timestamp = now
        self._update_plot_refresh_label()

    def _update_perf_summary(self) -> None:
        label = getattr(self, "_perf_summary_label", None)
        if label is None:
            return
        stream_hz = float(self._sampling_rate_hz or 0.0)
        redraw_ms = self._redraw_ema_ms if self._redraw_ema_ms > 0.0 else self._last_redraw_ms
        if redraw_ms <= 0.0:
            redraw_ms = 0.0
        interval_ms = 0.0
        timer = getattr(self, "_timer", None)
        if timer is not None:
            try:
                interval_ms = float(timer.interval())
            except (TypeError, ValueError):
                interval_ms = 0.0
        if interval_ms <= 0.0:
            interval_ms = float(self._compute_refresh_interval())
        if interval_ms <= 0.0:
            interval_ms = float(self.refresh_interval_ms)
        label.setText(
            f"Perf: Stream â‰ˆ {stream_hz:4.1f} Hz  |  "
            f"Redraw EMA â‰ˆ {redraw_ms:4.1f} ms  |  "
            f"Refresh interval â‰ˆ {interval_ms:.0f} ms"
        )

    def _update_plot_refresh_label(self) -> None:
        label = getattr(self, "_plot_refresh_label", None)
        if label is None:
            return
        timer = getattr(self, "_timer", None)
        timer_active = True
        interval_ms = int(self.refresh_interval_ms)
        if timer is not None:
            timer_active = timer.isActive()
            try:
                interval_ms = int(timer.interval())
            except TypeError:
                pass
        descriptor = self._refresh_profile_descriptor()
        hz = self._smoothed_refresh_hz
        suffix = ""
        interval_parts: list[str] = []
        if interval_ms > 0:
            interval_parts.append(f"{interval_ms} ms")
        if descriptor:
            interval_parts.append(descriptor)
        if interval_parts:
            suffix = f" ({' â€“ '.join(interval_parts)})"
        if hz is not None:
            label.setText(f"Plot refresh: {hz:4.1f} Hz{suffix}")
            return
        target = self._target_refresh_hz
        if not timer_active:
            label.setText("Plot refresh: paused")
            return
        if target and target > 0.0:
            label.setText(f"Plot refresh (target): {target:4.1f} Hz{suffix}")
        else:
            label.setText(f"Plot refresh: -- Hz{suffix}")

    def _refresh_timer_state(self) -> None:
        """Start/stop the redraw timer based on live or synthetic stream activity."""
        if not hasattr(self, "_timer"):
            return
        # Only tick the GUI timer while we have real or synthetic data;
        # pausing it avoids wasting CPU when no stream is active.
        should_run = self._stream_active or self._synthetic_active
        if should_run:
            if not self._timer.isActive():
                self._timer.start()
                self._reset_refresh_measurement()
        else:
            if self._timer.isActive():
                self._timer.stop()
                self._reset_refresh_measurement()
        self._update_plot_refresh_label()

    def _apply_refresh_settings(self) -> None:
        interval_ms = self._compute_refresh_interval()
        if self.refresh_mode == "follow_sampling_rate":
            previous = self._last_follow_interval_ms
            self._last_follow_interval_ms = interval_ms
            if (
                previous
                and previous > 0
                and abs(interval_ms - previous) / float(previous) > 0.10
            ):
                logger.debug(
                    "SignalsTab: adjusting follow-mode refresh interval from %d ms to %d ms",
                    previous,
                    interval_ms,
                )
        else:
            self._last_follow_interval_ms = None
        target_hz = 1000.0 / interval_ms if interval_ms > 0 else None
        self._target_refresh_hz = target_hz
        self._plot.set_target_refresh_rate(target_hz)
        self._sync_refresh_profile_combo(interval_ms)
        self._reset_refresh_measurement()
        if ENABLE_PLOT_PERF_METRICS:
            self._reset_timer_stats()
        if hasattr(self, "_timer"):
            self._timer.setInterval(interval_ms)
            if self._timer.isActive():
                self._timer.start(interval_ms)
            else:
                self._refresh_timer_state()
        self._update_perf_summary()

    def _update_perf_hud(self) -> None:
        cpu_percent = get_process_cpu_percent()
        if not self._perf_hud_label.isVisible():
            return

        snap = self.get_perf_snapshot() if ENABLE_PLOT_PERF_METRICS else {}
        fps = float(snap.get("fps", 0.0) or 0.0)
        target_fps = float(snap.get("target_fps", 0.0) or 0.0)
        avg_frame_ms = float(snap.get("avg_frame_ms", 0.0) or 0.0)
        avg_latency_ms = float(snap.get("avg_latency_ms", 0.0) or 0.0)
        max_latency_ms = float(snap.get("max_latency_ms", 0.0) or 0.0)
        approx_dropped = snap.get("approx_dropped_fps")
        if approx_dropped is None:
            approx_dropped = snap.get("approx_dropped_frames_per_sec", 0.0)
        approx_dropped = float(approx_dropped or 0.0)
        timer_hz = float(snap.get("timer_hz", 0.0) or 0.0)

        text = (
            f"CPU: {cpu_percent:5.1f}%\n"
            f"FPS: {fps:5.1f} / target {target_fps:4.1f}  (timer: {timer_hz:4.1f} Hz)\n"
            f"Frame: {avg_frame_ms:5.1f} ms\n"
            f"Latency: avg {avg_latency_ms:5.1f} ms, max {max_latency_ms:5.1f} ms\n"
            f"Dropped: ~{approx_dropped:4.1f} fps"
        )
        self._perf_hud_label.setText(text)

    # --------------------------------------------------------------- synthetic data helpers
    def start_synthetic_stream(
        self,
        rate_hz: float,
        sensor_ids: Iterable[int] | None = None,
    ) -> None:
        interval_ms = max(1, int(round(1000.0 / max(1.0, rate_hz))))
        interval_ns = max(1, int(1_000_000_000 / max(1.0, rate_hz)))
        self.stop_synthetic_stream()
        self._synthetic_rate_hz = float(rate_hz)
        self._synthetic_sensor_ids = list(sensor_ids) if sensor_ids else [1]
        self._synthetic_phase = 0.0
        self._synthetic_phase_step = 2.0 * math.pi * (1.0 / max(1.0, rate_hz))
        self._synthetic_interval_ns = interval_ns
        self._synthetic_start_timestamp_ns = time.monotonic_ns()
        self._synthetic_next_timestamp_ns = self._synthetic_start_timestamp_ns
        self._fallback_data_buffer.clear()
        self._buffer_cursors.clear()
        timer = QTimer(self)
        timer.setTimerType(Qt.PreciseTimer)
        timer.setInterval(interval_ms)
        timer.timeout.connect(self._on_synthetic_tick)
        timer.start()
        self._synthetic_timer = timer
        self._synthetic_active = True
        self._refresh_timer_state()

    def stop_synthetic_stream(self) -> None:
        timer = self._synthetic_timer
        if timer is not None:
            timer.stop()
            timer.deleteLater()
        self._synthetic_timer = None
        self._synthetic_active = False
        self._refresh_timer_state()

    def _on_synthetic_tick(self) -> None:
        if self._synthetic_timer is None:
            return
        timestamp_ns = self._synthetic_next_timestamp_ns
        interval_ns = self._synthetic_interval_ns
        start_ns = self._synthetic_start_timestamp_ns
        phase = self._synthetic_phase
        phase_step = self._synthetic_phase_step
        per_sensor_offset_ns = max(1, interval_ns // max(1, len(self._synthetic_sensor_ids)))
        generated: List[MpuSample] = []
        for idx, sensor_id in enumerate(self._synthetic_sensor_ids):
            offset = idx * 0.5
            val_sin = math.sin(phase + offset)
            val_cos = math.cos(phase + offset)
            sample_ns = int(timestamp_ns + idx * per_sensor_offset_ns)
            sample = MpuSample(
                timestamp_ns=sample_ns,
                ax=val_sin,
                ay=math.sin(phase + offset + 0.5),
                az=math.sin(phase + offset + 1.0),
                gx=val_cos,
                gy=math.cos(phase + offset + 0.5),
                gz=math.cos(phase + offset + 1.0),
                sensor_id=int(sensor_id),
                t_s=float((sample_ns - start_ns) / 1_000_000_000),
            )
            generated.append(sample)
        self._synthetic_phase = phase + phase_step
        self._synthetic_next_timestamp_ns = timestamp_ns + interval_ns
        if generated:
            self._fallback_data_buffer.add_samples(generated)

    @Slot()
    def on_stream_started(self) -> None:
        self._plot.clear()
        self._buffer_cursors.clear()
        self._sampling_rate_hz = None
        self._plot.set_nominal_sample_rate(None)
        self._stream_rate_label.setText("Stream rate: -- Hz")
        self._stream_active = True
        self._awaiting_first_sample = True
        self._stream_stalled = False
        self._last_data_monotonic = 0.0
        self._set_stream_status("Streaming...", force=True)
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self._refresh_timer_state()
        self._refresh_mode_hint()

    def set_view_mode_by_channels(self, charts: int) -> None:
        preset = "all6" if charts >= 18 else "default3"
        self.set_view_mode_preset(preset)

    def set_view_mode_preset(self, preset: str) -> None:
        idx = self.view_mode_combo.findData(preset)
        if idx < 0:
            return
        with QSignalBlocker(self.view_mode_combo):
            self.view_mode_combo.setCurrentIndex(idx)
        self._rebuild_channel_checkboxes()

    @Slot()
    def on_stream_stopped(self) -> None:
        self._stream_active = False
        self._awaiting_first_sample = False
        self._stream_stalled = False
        self._last_data_monotonic = 0.0
        self._set_stream_status("Stopped.", force=True)
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self._plot.clear()
        self._buffer_cursors.clear()
        self._refresh_timer_state()
        self._refresh_mode_hint()

    @Slot(str)
    def handle_error(self, message: str) -> None:
        self._set_manual_status(message)

    @Slot(int)
    def _on_base_correction_toggled(self, state: int) -> None:
        enabled = state == Qt.Checked
        self._plot.enable_base_correction(enabled)
        if enabled:
            self._set_manual_status("Base correction enabled.")
        else:
            self._set_manual_status("Base correction disabled.")

    @Slot()
    def _on_calibrate_clicked(self) -> None:
        self._plot.calibrate_from_buffer()
        if self.base_correction_check.isChecked():
            self._set_manual_status(
                "Calibration updated (base correction applied)."
            )
        else:
            self._set_manual_status(
                "Calibration stored (enable 'Base correction' to apply)."
            )

    def attach_recorder_controls(self, recorder_tab: "RecorderTab") -> None:
        """
        Embed the RecorderTab's connection/settings widgets at the top of this tab
        so users can manage the Pi directly from here.

        The RecorderTab itself can stay hidden; we just reuse its widgets.
        """
        # Local import to avoid circular dependency
        from .tab_recorder import RecorderTab as _RecorderTab  # type: ignore

        if not isinstance(recorder_tab, _RecorderTab):
            return

        host_group = getattr(recorder_tab, "host_group", None)
        mpu_group = getattr(recorder_tab, "mpu_group", None)
        if host_group is None or mpu_group is None:
            return

        parent_layout = recorder_tab.layout()
        if parent_layout is not None:
            parent_layout.removeWidget(host_group)
            parent_layout.removeWidget(mpu_group)

        host_group.setParent(self)
        mpu_group.setParent(self)

        layout = self.layout()
        if layout is not None:
            host_group.setTitle("")
            host_section = CollapsibleSection("Raspberry Pi host", self)
            host_layout = QVBoxLayout()
            host_layout.setContentsMargins(0, 0, 0, 0)
            host_layout.addWidget(host_group)
            host_section.setContentLayout(host_layout)
            layout.insertWidget(0, host_section)
            self._host_section = host_section

            mpu_group.setTitle("")
            mpu_section = CollapsibleSection("MPU6050 settings", self)
            mpu_layout = QVBoxLayout()
            mpu_layout.setContentsMargins(0, 0, 0, 0)
            mpu_layout.addWidget(mpu_group)
            mpu_section.setContentLayout(mpu_layout)
            layout.insertWidget(1, mpu_section)
            self._mpu_section = mpu_section

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\widgets\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\gui\widgets\
# Size: 239 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\widgets\__init__.py
from .acquisition_settings import (
    AcquisitionSettings,
    AcquisitionSettingsWidget,
)
from .collapsible import CollapsibleSection

__all__ = [
    "AcquisitionSettings",
    "AcquisitionSettingsWidget",
    "CollapsibleSection",
]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\widgets\acquisition_settings.py
# File: acquisition_settings.py (ext: .py
# Dir : src\sensepi\gui\widgets\
# Size: 8779 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\widgets\acquisition_settings.py
from __future__ import annotations

from dataclasses import dataclass, field

from PySide6.QtCore import Qt, Signal, QSignalBlocker
from PySide6.QtWidgets import (
    QComboBox,
    QDoubleSpinBox,
    QFormLayout,
    QHBoxLayout,
    QLabel,
    QSpinBox,
    QWidget,
)

from ...config.sampling import GuiSamplingDisplay, RECORDING_MODES, SamplingConfig


DEFAULT_DEVICE_RATE_HZ = 200.0
DEFAULT_SIGNALS_REFRESH_MS = 50
DEFAULT_FFT_REFRESH_MS = 750
DEFAULT_MODE_KEY = "high_fidelity"


@dataclass
class AcquisitionSettings:
    sampling: SamplingConfig = field(
        default_factory=lambda: SamplingConfig(
            device_rate_hz=DEFAULT_DEVICE_RATE_HZ,
            mode_key=DEFAULT_MODE_KEY,
        )
    )
    signals_mode: str = "fixed"  # "fixed" or "adaptive"
    signals_refresh_ms: int = DEFAULT_SIGNALS_REFRESH_MS
    fft_refresh_ms: int = DEFAULT_FFT_REFRESH_MS

    @property
    def effective_stream_rate_hz(self) -> float:
        display = GuiSamplingDisplay.from_sampling(self.sampling)
        return float(display.stream_rate_hz)

    def as_dict(self) -> dict:
        """Convenience helper for serialization/testing."""
        return {
            "sampling": self.sampling.to_mapping()["sampling"],
            "signals_mode": str(self.signals_mode),
            "signals_refresh_ms": int(self.signals_refresh_ms),
            "fft_refresh_ms": int(self.fft_refresh_ms),
        }


class AcquisitionSettingsWidget(QWidget):
    """Small form that lets the user tune sampling/stream and refresh rates."""

    signalsModeChanged = Signal(str)
    signalsRefreshChanged = Signal(int)
    fftRefreshChanged = Signal(int)
    samplingChanged = Signal(object)  # emits SamplingConfig

    def __init__(self, parent: QWidget | None = None) -> None:
        super().__init__(parent)

        self._sampling_config = SamplingConfig(
            device_rate_hz=DEFAULT_DEVICE_RATE_HZ,
            mode_key=DEFAULT_MODE_KEY,
        )

        form = QFormLayout(self)
        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)

        # Device sampling controls
        self.device_rate_spin = QDoubleSpinBox(self)
        self.device_rate_spin.setRange(1.0, 4000.0)
        self.device_rate_spin.setDecimals(1)
        self.device_rate_spin.setSingleStep(1.0)
        self.device_rate_spin.setValue(float(self._sampling_config.device_rate_hz))
        form.addRow("Device rate [Hz]:", self.device_rate_spin)

        self.mode_combo = QComboBox(self)
        for key, mode in RECORDING_MODES.items():
            self.mode_combo.addItem(mode.label, key)
        idx = self.mode_combo.findData(self._sampling_config.mode_key)
        if idx >= 0:
            self.mode_combo.setCurrentIndex(idx)
        form.addRow("Mode:", self.mode_combo)

        self._record_rate_label = QLabel("â€”", self)
        self._record_rate_label.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)
        form.addRow("Recording rate [Hz]:", self._record_rate_label)

        self._stream_rate_label = QLabel("â€”", self)
        self._stream_rate_label.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)
        form.addRow("GUI stream [Hz]:", self._stream_rate_label)

        # Signals refresh mode + interval
        mode_row = QHBoxLayout()
        self.signals_mode_combo = QComboBox(self)
        self.signals_mode_combo.addItem("Fixed interval", "fixed")
        self.signals_mode_combo.addItem("Adaptive (follow stream rate)", "adaptive")
        mode_row.addWidget(self.signals_mode_combo)

        self.signals_refresh_spin = QSpinBox(self)
        self.signals_refresh_spin.setRange(10, 1000)
        self.signals_refresh_spin.setSingleStep(5)
        self.signals_refresh_spin.setValue(DEFAULT_SIGNALS_REFRESH_MS)
        self.signals_refresh_spin.setSuffix(" ms")
        mode_row.addWidget(self.signals_refresh_spin)
        self.signals_refresh_spin.valueChanged.connect(
            self._on_signals_refresh_value_changed
        )

        form.addRow("Signals refresh:", mode_row)

        # FFT refresh (independent timer)
        self.fft_refresh_spin = QSpinBox(self)
        self.fft_refresh_spin.setRange(100, 5000)
        self.fft_refresh_spin.setSingleStep(50)
        self.fft_refresh_spin.setValue(DEFAULT_FFT_REFRESH_MS)
        self.fft_refresh_spin.setSuffix(" ms")
        form.addRow("FFT refresh interval:", self.fft_refresh_spin)
        self.fft_refresh_spin.valueChanged.connect(
            self._on_fft_refresh_value_changed
        )

        # Wiring
        self.device_rate_spin.valueChanged.connect(self._on_sampling_control_changed)
        self.mode_combo.currentIndexChanged.connect(self._on_sampling_control_changed)
        self.signals_mode_combo.currentIndexChanged.connect(self._on_mode_changed)

        self._update_sampling_labels()
        self._on_mode_changed()

    # ------------------------------------------------------------------ helpers
    def settings(self) -> AcquisitionSettings:
        sampling = self._build_sampling_config()
        self._sampling_config = sampling
        mode = self.signals_mode_combo.currentData() or "fixed"
        return AcquisitionSettings(
            sampling=sampling,
            signals_mode=str(mode),
            signals_refresh_ms=int(self.signals_refresh_spin.value()),
            fft_refresh_ms=int(self.fft_refresh_spin.value()),
        )

    def set_settings(self, settings: AcquisitionSettings) -> None:
        """Load settings into the widget."""
        self.set_sampling_config(settings.sampling)
        mode = (
            settings.signals_mode
            if settings.signals_mode in {"fixed", "adaptive"}
            else "fixed"
        )
        idx = self.signals_mode_combo.findData(mode)
        if idx >= 0:
            self.signals_mode_combo.setCurrentIndex(idx)
        self.signals_refresh_spin.setValue(int(settings.signals_refresh_ms))
        self.fft_refresh_spin.setValue(int(settings.fft_refresh_ms))
        self._on_mode_changed()

    def set_sampling_config(self, sampling: SamplingConfig) -> None:
        """Update the sampling controls from an external config."""
        sampling = SamplingConfig(
            device_rate_hz=float(sampling.device_rate_hz),
            mode_key=str(sampling.mode_key),
        )
        self._sampling_config = sampling
        with QSignalBlocker(self.device_rate_spin):
            self.device_rate_spin.setValue(float(sampling.device_rate_hz))
        idx = self.mode_combo.findData(sampling.mode_key)
        if idx < 0:
            idx = self.mode_combo.findData(DEFAULT_MODE_KEY)
        if idx < 0:
            idx = 0
        with QSignalBlocker(self.mode_combo):
            self.mode_combo.setCurrentIndex(idx)
        self._update_sampling_labels()

    def _build_sampling_config(self) -> SamplingConfig:
        try:
            rate = float(self.device_rate_spin.value())
        except (TypeError, ValueError):
            rate = DEFAULT_DEVICE_RATE_HZ
        idx = self.mode_combo.currentIndex()
        mode_key = str(self.mode_combo.itemData(idx) or DEFAULT_MODE_KEY)
        if mode_key not in RECORDING_MODES:
            mode_key = DEFAULT_MODE_KEY
        return SamplingConfig(device_rate_hz=rate, mode_key=mode_key)

    def _on_sampling_control_changed(self, *_args) -> None:
        sampling = self._build_sampling_config()
        self._sampling_config = sampling
        self._update_sampling_labels()
        self.samplingChanged.emit(sampling)

    def _update_sampling_labels(self) -> None:
        display = GuiSamplingDisplay.from_sampling(self._sampling_config)
        self._record_rate_label.setText(f"{display.record_rate_hz:.1f} Hz")
        self._stream_rate_label.setText(f"{display.stream_rate_hz:.1f} Hz")

    def _on_mode_changed(self) -> None:
        mode = self.signals_mode_combo.currentData()
        is_fixed = mode == "fixed"
        self.signals_refresh_spin.setEnabled(is_fixed)
        normalized = str(mode or "fixed")
        self.signalsModeChanged.emit(normalized)

    def _on_signals_refresh_value_changed(self, value: int) -> None:
        self.signalsRefreshChanged.emit(int(value))

    def _on_fft_refresh_value_changed(self, value: int) -> None:
        self.fftRefreshChanged.emit(int(value))

    def set_signals_refresh_interval(self, interval_ms: int) -> None:
        """Update the signals refresh spin box without emitting change signals."""
        blocker = QSignalBlocker(self.signals_refresh_spin)
        self.signals_refresh_spin.setValue(int(interval_ms))

    def set_fft_refresh_interval(self, interval_ms: int) -> None:
        """Update the FFT refresh spin box without emitting change signals."""
        blocker = QSignalBlocker(self.fft_refresh_spin)
        self.fft_refresh_spin.setValue(int(interval_ms))

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\gui\widgets\collapsible.py
# File: collapsible.py (ext: .py
# Dir : src\sensepi\gui\widgets\
# Size: 1608 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\gui\widgets\collapsible.py
from __future__ import annotations

from PySide6.QtCore import Qt
from PySide6.QtWidgets import QLayout, QSizePolicy, QToolButton, QVBoxLayout, QWidget


class CollapsibleSection(QWidget):
    """
    Simple collapsible section: a header with an arrow and a content area.
    Use setContentLayout() to put your real widgets inside.
    """

    def __init__(self, title: str, parent: QWidget | None = None) -> None:
        super().__init__(parent)

        self._toggle = QToolButton(text=title, checkable=True, checked=True)
        self._toggle.setToolButtonStyle(Qt.ToolButtonTextBesideIcon)
        self._toggle.setArrowType(Qt.DownArrow)
        self._toggle.toggled.connect(self._on_toggled)

        self._content = QWidget()
        self._content.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)
        self._content.setVisible(True)

        layout = QVBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(0)
        layout.addWidget(self._toggle)
        layout.addWidget(self._content)

    def setContentLayout(self, layout: QLayout) -> None:
        """Put a layout with your controls inside the collapsible content area."""
        self._content.setLayout(layout)

    def setCollapsed(self, collapsed: bool) -> None:
        """Programmatic collapse/expand (e.g. when Start is pressed)."""
        self._toggle.setChecked(not collapsed)

    def _on_toggled(self, checked: bool) -> None:
        self._content.setVisible(checked)
        self._toggle.setArrowType(Qt.DownArrow if checked else Qt.RightArrow)

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\perf_system.py
# File: perf_system.py (ext: .py
# Dir : src\sensepi\
# Size: 577 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\perf_system.py
"""Helpers for querying local process performance metrics."""

from __future__ import annotations

import os
from typing import Final

import psutil

_PROCESS: Final[psutil.Process] = psutil.Process(os.getpid())


def get_process_cpu_percent() -> float:
    """
    Return the current CPU usage of the GUI process.

    psutil's cpu_percent needs to be called periodically; the first call
    may return 0.0 which is acceptable for the lightweight HUD display.
    """
    try:
        return float(_PROCESS.cpu_percent(interval=None))
    except Exception:
        return 0.0

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\remote\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\remote\
# Size: 404 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\remote\__init__.py
"""Remote communication helpers for controlling Raspberry Pi loggers.

Classes such as :class:`SSHClient` and :class:`PiRecorder` establish SSH/SFTP
connections, start sensor scripts on the Pi, and stream stdout/stderr back to
the desktop application.
"""

from .ssh_client import Host, SSHClient, SSHConfig
from .pi_recorder import PiRecorder

__all__ = ["Host", "SSHClient", "SSHConfig", "PiRecorder"]

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\remote\pi_recorder.py
# File: pi_recorder.py (ext: .py
# Dir : src\sensepi\remote\
# Size: 3964 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\remote\pi_recorder.py
"""High-level helpers for starting and streaming logger scripts on the Pi."""

from __future__ import annotations

from pathlib import Path, PurePosixPath
from typing import Callable, Iterable, Optional

from ..config.app_config import DEFAULT_BASE_PATH
from .ssh_client import Host, SSHClient


class PiRecorder:
    """Launches Raspberry Pi logger scripts over SSH."""

    def __init__(self, host: Host, base_path: Optional[Path] = None) -> None:
        self.host = host
        if base_path is None:
            base_path = DEFAULT_BASE_PATH

        # Ensure the remote path is always POSIX-style, even on Windows hosts.
        base_path = Path(base_path).expanduser()
        self.base_path = PurePosixPath(base_path.as_posix())
        self.client = SSHClient(host)

    # ------------------------------------------------------------------ connection
    def connect(self) -> None:
        """Ensure an SSH connection is open."""
        self.client.connect()

    def close(self) -> None:
        """Close the SSH connection (does not kill remote loggers)."""
        self.client.close()

    # ------------------------------------------------------------------ simple runner
    def start_logger(
        self, script_name: str, args: Optional[Iterable[str]] = None
    ):
        """
        Run a logger script and return (stdout, stderr) file-like objects.

        This is mainly useful for short-lived commands or testing; for live
        streaming use :meth:`stream_mpu6050`.
        """
        command = f"python3 {self.base_path / script_name}"
        if args:
            command = " ".join([command, *args])

        self.connect()
        _, stdout, stderr = self.client.run(command)
        return stdout, stderr

    # ------------------------------------------------------------------ streaming
    def _stream_logger(
        self,
        script_name: str,
        extra_args: str = "",
        on_stderr: Optional[Callable[[str], None]] = None,
        *,
        recording: bool = False,
    ) -> Iterable[str]:
        """
        Internal helper: start a logger in ``--stream-stdout`` mode.

        ``extra_args`` is a free-form CLI string; this helper will append
        ``--stream-stdout`` and optionally ``--no-record`` depending on ``recording``.
        """
        self.connect()

        parts = extra_args.strip().split() if extra_args.strip() else []

        if "--stream-stdout" not in parts:
            parts.append("--stream-stdout")

        wants_recording = recording or ("--record" in parts)

        # Ensure we don't accidentally send both flags
        parts = [p for p in parts if p != "--no-record"]

        if not wants_recording and "--no-record" not in parts:
            parts.append("--no-record")

        cmd = f"python3 {script_name}"
        if parts:
            cmd = f"{cmd} {' '.join(parts)}"

        # Use cwd so the script can rely on relative paths.
        cwd = self.base_path.as_posix()
        return self.client.exec_stream(cmd, cwd=cwd, stderr_callback=on_stderr)

    def stream_mpu6050(
        self,
        extra_args: str = "",
        *,
        recording: bool = False,
        on_stderr: Optional[Callable[[str], None]] = None,
    ) -> Iterable[str]:
        """
        Start the MPU6050 logger in streaming mode.

        Internally builds a command like:

            python3 mpu6050_multi_logger.py --rate ... --stream-stdout --no-record ...

        and returns an iterable of JSON lines.
        """
        return self._stream_logger(
            "mpu6050_multi_logger.py",
            extra_args,
            on_stderr=on_stderr,
            recording=recording,
        )

    # ------------------------------------------------------------------ convenience
    def stop(self) -> None:
        """Alias for :meth:`close` to match older code."""
        self.close()

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\remote\sensor_ingest_worker.py
# File: sensor_ingest_worker.py (ext: .py
# Dir : src\sensepi\remote\
# Size: 4760 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\remote\sensor_ingest_worker.py
"""Threaded worker that reads sensor samples and emits batched Qt signals."""

from __future__ import annotations

from typing import Callable, Iterable, Optional
import time

from PySide6.QtCore import QObject, Signal, Slot

from .pi_recorder import PiRecorder
from ..sensors.mpu6050 import MpuSample
from ..tools.debug import debug_enabled


class SensorIngestWorker(QObject):
    """QObject-based worker that pulls data from a PiRecorder stream and batches samples.

    It is meant to live in its own QThread: lines are parsed in the worker
    thread and emitted as small batches to the GUI via the samples_batch signal.
    """

    samples_batch = Signal(list)  # list[MpuSample]
    error = Signal(str)
    finished = Signal()

    def __init__(
        self,
        recorder: PiRecorder,
        stream_factory: Callable[[], Iterable[str]],
        parser: Callable[[str], Optional[MpuSample]],
        *,
        batch_size: int = 50,
        max_latency_ms: int = 100,
        parent: QObject | None = None,
        stream_label: str | None = None,
    ) -> None:
        super().__init__(parent)
        self._recorder = recorder
        self._stream_factory = stream_factory
        self._parser = parser
        self._batch_size = max(1, int(batch_size))
        self._max_latency_ms = max(0.0, float(max_latency_ms))
        self._running = False
        self._stream_label = stream_label or "stream"

    @Slot()
    def start(self) -> None:
        """Entry point for the QThread: consume the remote stream and emit batches."""
        self._running = True
        buffer: list[MpuSample] = []
        last_emit = time.monotonic()
        debug_on = debug_enabled()
        debug_total_samples = 0
        debug_window_samples = 0
        debug_start = time.perf_counter()
        debug_last_log = debug_start

        try:
            try:
                lines = self._stream_factory()
            except Exception as exc:  # pragma: no cover - best-effort guard
                self.error.emit(f"Failed to start sensor stream: {exc}")
                return

            for line in lines:
                if not self._running:
                    break
                if not line:
                    continue

                try:
                    sample = self._parser(line)
                except Exception as exc:  # pragma: no cover - parser errors
                    self.error.emit(f"Failed to parse sensor line: {exc}")
                    continue

                if sample is None:
                    continue

                buffer.append(sample)
                if debug_on:
                    debug_total_samples += 1
                    debug_window_samples += 1

                now = time.monotonic()
                latency_elapsed = (now - last_emit) * 1000.0
                # Emit a batch either when we have enough samples or when the
                # oldest one has been waiting longer than max_latency_ms.
                should_emit = len(buffer) >= self._batch_size
                if not should_emit and self._max_latency_ms > 0.0:
                    should_emit = latency_elapsed >= self._max_latency_ms

                if should_emit and buffer:
                    self.samples_batch.emit(list(buffer))
                    buffer.clear()
                    last_emit = now

                if debug_on:
                    perf_now = time.perf_counter()
                    if perf_now - debug_last_log >= 5.0:
                        elapsed_total = max(1e-9, perf_now - debug_start)
                        elapsed_window = max(1e-9, perf_now - debug_last_log)
                        avg_rate = debug_total_samples / elapsed_total
                        window_rate = debug_window_samples / elapsed_window
                        print(
                            f"[DEBUG] stream={self._stream_label} samples={debug_total_samples} "
                            f"avgâ‰ˆ{avg_rate:.1f} Hz recentâ‰ˆ{window_rate:.1f} Hz",
                            flush=True,
                        )
                        debug_last_log = perf_now
                        debug_window_samples = 0

            if buffer:
                self.samples_batch.emit(list(buffer))
        except Exception as exc:  # pragma: no cover - safety net for stream errors
            self.error.emit(str(exc))
        finally:
            self._running = False
            try:
                self._recorder.close()
            except Exception:
                pass
            self.finished.emit()

    @Slot()
    def stop(self) -> None:
        """Request the reading loop to terminate."""
        self._running = False
        try:
            self._recorder.close()
        except Exception:
            pass

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\remote\ssh_client.py
# File: ssh_client.py (ext: .py
# Dir : src\sensepi\remote\
# Size: 7139 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\remote\ssh_client.py
"""Lightweight SSH client wrapper for Raspberry Pi control."""

from __future__ import annotations

from contextlib import contextmanager
from dataclasses import dataclass
from typing import Iterable, Iterator, Optional

import logging
import paramiko
import shlex
import threading


logger = logging.getLogger(__name__)


@dataclass
class SSHConfig:
    """Simple SSH connection settings for password-based auth only."""

    host: str
    username: str
    port: int = 22
    password: Optional[str] = None


@dataclass
class Host:
    """Connection details for a Raspberry Pi host (password-based auth only)."""

    name: str
    host: str
    user: str
    password: Optional[str] = None
    port: int = 22


class SSHClient:
    """Simple wrapper around ``paramiko`` for running remote commands."""

    def __init__(self, host: Host) -> None:
        self.host = host
        self._client: paramiko.SSHClient = paramiko.SSHClient()
        self._client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    # ------------------------------------------------------------------ internals
    def _ensure_client(self) -> paramiko.SSHClient:
        transport = self._client.get_transport()
        if not (transport and transport.is_active()):
            self.connect()
        return self._client

    # ------------------------------------------------------------------ connection
    def connect(self) -> None:
        transport = self._client.get_transport()
        if transport and transport.is_active():
            return

        logger.info(
            "Connecting to %s@%s:%s", self.host.user, self.host.host, self.host.port
        )

        self._client.connect(
            hostname=self.host.host,
            username=self.host.user,
            port=self.host.port,
            password=self.host.password,
            look_for_keys=False,
            allow_agent=False,
            timeout=10.0,
        )

    def close(self) -> None:
        try:
            self._client.close()
        except Exception:
            pass

    # ------------------------------------------------------------------ commands
    def run(self, command: str):
        """
        Execute a command over SSH and return stdin, stdout, stderr.

        This is a thin wrapper around :meth:`paramiko.SSHClient.exec_command`.
        """
        client = self._ensure_client()
        return client.exec_command(command)

    @contextmanager
    def sftp(self) -> Iterator[paramiko.SFTPClient]:
        """Context manager that yields an SFTP client."""

        client = self._ensure_client()
        sftp = client.open_sftp()
        try:
            yield sftp
        finally:
            try:
                sftp.close()
            except Exception:
                pass

    def path_exists(self, remote_path: str) -> bool:
        """Return True if *remote_path* exists on the Pi."""

        with self.sftp() as sftp:
            try:
                sftp.stat(remote_path)
            except IOError:
                return False
            else:
                return True

    def exec_stream(
        self,
        command: str,
        cwd: Optional[str] = None,
        encoding: str = "utf-8",
        errors: str = "ignore",
        stderr_callback: Optional[callable] = None,
    ) -> Iterable[str]:
        """
        Run a long-lived command and yield stdout lines as they arrive.

        If stderr_callback is provided, stderr lines are forwarded to it.
        The returned iterable exposes ``close()`` to explicitly stop the
        remote process and tear down the SSH channel.
        """
        client = self._ensure_client()

        full_cmd = command
        if cwd:
            full_cmd = f"cd {shlex.quote(cwd)} && {command}"

        stdin, stdout, stderr = client.exec_command(full_cmd)

        class _StreamIterator(Iterator[str]):
            def __init__(self) -> None:
                self._stdout = stdout
                self._stderr = stderr
                self._stdin = stdin
                self._encoding = encoding
                self._errors = errors
                self._stderr_callback = stderr_callback
                self._closed = False
                self._stderr_thread: threading.Thread | None = None
                if self._stderr_callback is not None:
                    self._stderr_thread = threading.Thread(
                        target=self._watch_stderr,
                        name="ssh-stderr",
                        daemon=True,
                    )
                    self._stderr_thread.start()

            def __iter__(self) -> "_StreamIterator":
                return self

            def __next__(self) -> str:
                while True:
                    if self._closed:
                        raise StopIteration
                    try:
                        raw = self._stdout.readline()
                    except Exception:
                        self.close()
                        raise StopIteration
                    if raw == "":
                        self.close()
                        raise StopIteration
                    if isinstance(raw, bytes):
                        text = raw.decode(self._encoding, errors=self._errors)
                    else:
                        text = raw
                    line = text.rstrip("\r\n")
                    if line:
                        return line

            def _watch_stderr(self) -> None:
                assert self._stderr_callback is not None
                try:
                    for raw_err in iter(lambda: self._stderr.readline(), ""):
                        if not raw_err:
                            break
                        if isinstance(raw_err, bytes):
                            text_err = raw_err.decode(
                                self._encoding, errors=self._errors
                            )
                        else:
                            text_err = raw_err
                        text_err = text_err.rstrip("\r\n")
                        if text_err:
                            try:
                                self._stderr_callback(text_err)
                            except Exception:
                                logger.exception("Error handling stderr callback")
                except Exception:
                    logger.exception("Error reading remote stderr")

            def close(self) -> None:
                if self._closed:
                    return
                self._closed = True

                for stream in (self._stdout, self._stdin, self._stderr):
                    try:
                        stream.close()
                    except Exception:
                        pass

                channel = getattr(self._stdout, "channel", None)
                if channel is not None:
                    try:
                        channel.close()
                    except Exception:
                        pass

        return _StreamIterator()

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\sensors\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\sensors\
# Size: 279 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\sensors\__init__.py
"""Sensor-specific data models and parsers.

Each supported sensor exposes typed sample structures plus conversion helpers
that turn raw JSON lines or SSH output into those dataclasses. Currently the
:mod:`mpu6050` module defines :class:`MpuSample` used across the pipeline.
"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\sensors\mpu6050.py
# File: mpu6050.py (ext: .py
# Dir : src\sensepi\sensors\
# Size: 4077 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\sensors\mpu6050.py
"""
The Raspberry Pi logger streams JSON lines with (at least):

  - timestamp_ns : int   monotonic time in nanoseconds
  - t_s          : float seconds since the run started
  - sensor_id    : int   logical sensor index (1, 2, or 3)
  - ax, ay, az   : float linear acceleration in m/sÂ²
  - gx, gy, gz   : float angular rate in deg/s

``parse_line()`` accepts those JSON lines and also supports the legacy
comma-separated format "timestamp_ns,ax,ay,az,gx,gy,gz" for older logs.
"""

from __future__ import annotations

import json
import logging
import math
from dataclasses import dataclass
import time
from typing import Optional, Sequence

from ..tools.debug import debug_enabled

logger = logging.getLogger(__name__)


@dataclass
class MpuSample:
    timestamp_ns: int
    ax: float
    ay: float
    az: float
    gx: float
    gy: float
    gz: float
    sensor_id: Optional[int] = None
    t_s: Optional[float] = None


def _parse_json_line(text: str) -> MpuSample | None:
    try:
        obj = json.loads(text)
    except json.JSONDecodeError as exc:
        logger.warning("Bad JSON from sensor stream: %r (%s)", text, exc)
        return None

    ts_raw = obj.get("timestamp_ns")
    if ts_raw is None:
        logger.warning("Missing field %s in sensor line: %r", "timestamp_ns", obj)
        return None
    timestamp_ns = int(ts_raw)

    sensor_id = obj.get("sensor_id")
    if sensor_id is not None:
        sensor_id = int(sensor_id)

    t_s = obj.get("t_s")
    if t_s is not None:
        t_s = float(t_s)

    def _get_axis(name: str) -> float:
        val = obj.get(name)
        if val is None:
            # Use NaN to indicate "not present" while keeping a float type.
            return math.nan
        return float(val)

    try:
        ax = _get_axis("ax")
        ay = _get_axis("ay")
        az = _get_axis("az")
        gx = float(obj.get("gx", 0.0))
        gy = float(obj.get("gy", 0.0))
        gz = float(obj.get("gz", 0.0))
    except (TypeError, ValueError) as exc:
        logger.warning("Bad field value in sensor line %r (%s)", obj, exc)
        return None

    return MpuSample(
        timestamp_ns=timestamp_ns,
        ax=ax,
        ay=ay,
        az=az,
        gx=gx,
        gy=gy,
        gz=gz,
        sensor_id=sensor_id,
        t_s=t_s,
    )


def _parse_csv_line(text: str) -> MpuSample | None:
    parts: Sequence[str] = text.split(",")
    if len(parts) < 7:
        logger.warning(
            "Expected at least 7 comma-separated values for MPU6050 CSV, got %d: %r",
            len(parts),
            text,
        )
        return None
    try:
        ts, ax, ay, az, gx, gy, gz = map(float, parts[:7])
    except ValueError as exc:
        logger.warning("Bad CSV field in sensor line %r (%s)", text, exc)
        return None
    return MpuSample(
        timestamp_ns=int(ts),
        ax=ax,
        ay=ay,
        az=az,
        gx=gx,
        gy=gy,
        gz=gz,
    )


_parse_time_acc = 0.0
_parse_count = 0


def parse_line(line: str) -> MpuSample | None:
    """
    Parse a single text line from the MPU6050 logger into an :class:`MpuSample`.

    The function understands both the new JSONL streaming format and the
    legacy CSV format. Invalid lines return ``None`` so callers can skip them
    without raising exceptions.
    """
    global _parse_time_acc, _parse_count

    text = line.strip()
    if not text:
        return None

    debug_on = debug_enabled()
    start = time.perf_counter() if debug_on else 0.0

    if text[0] == "{":
        sample = _parse_json_line(text)
    else:
        sample = _parse_csv_line(text)

    if debug_on:
        _parse_time_acc += time.perf_counter() - start
        _parse_count += 1
        if _parse_count % 1000 == 0:
            avg_us = (_parse_time_acc / max(1, _parse_count)) * 1e6
            logger.info(
                "mpu6050.parse_line avg %.1f Âµs over %d samples", avg_us, _parse_count
            )

    return sample

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\tools\__init__.py
# File: __init__.py (ext: .py
# Dir : src\sensepi\tools\
# Size: 303 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\tools\__init__.py
"""Miscellaneous development tools and standalone helpers.

This package contains optional scripts that are helpful during debugging or
teaching sessions, including the Matplotlib log plotter, CLI debug utilities,
and the :mod:`local_plot_runner` process manager reused by the GUI's offline
viewer.
"""

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\tools\debug.py
# File: debug.py (ext: .py
# Dir : src\sensepi\tools\
# Size: 1084 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\tools\debug.py
"""Minimal helpers for opt-in debug/instrumentation hooks."""

from __future__ import annotations

import os
import sys
import time
from contextlib import contextmanager
from typing import Callable, Iterator

DEBUG_SENSEPI = os.getenv("SENSEPI_DEBUG", "").lower() in {"1", "true", "yes", "on"}


def debug_enabled() -> bool:
    """Return True when lightweight instrumentation should run."""
    return DEBUG_SENSEPI


@contextmanager
def time_block(label: str, *, emitter: Callable[[str], None] | None = None) -> Iterator[None]:
    """
    Context manager that emits elapsed time when debugging is enabled.

    The overhead is essentially a couple of perf_counter() calls when disabled.
    """
    if not DEBUG_SENSEPI:
        yield
        return

    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed_ms = (time.perf_counter() - start) * 1000.0
        target = emitter or (lambda msg: print(msg, file=sys.stderr, flush=True))
        try:
            target(f"[DEBUG] {label} took {elapsed_ms:.3f} ms")
        except Exception:
            pass

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\tools\local_plot_runner.py
# File: local_plot_runner.py (ext: .py
# Dir : src\sensepi\tools\
# Size: 2192 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\tools\local_plot_runner.py
"""Legacy helper for spawning the standalone plotter process."""

from __future__ import annotations

import sys
import subprocess
from pathlib import Path
from typing import List, Optional


class LocalPlotRunner:
    """
    Small helper that starts/stops your existing live plotting / FFT script
    (e.g. plotter.py) as a separate process.

    By default it runs:  sys.executable src/sensepi/tools/plotter.py
    in the given project_root.
    """

    def __init__(self, project_root: Path | str, script_name: str = "src/sensepi/tools/plotter.py") -> None:
        self.project_root = Path(project_root).resolve()
        self.script_name = script_name
        self._proc: Optional[subprocess.Popen] = None

    @property
    def is_running(self) -> bool:
        return self._proc is not None and self._proc.poll() is None

    def start(self, extra_args: Optional[List[str]] = None) -> None:
        """
        Start the local plotting / FFT pipeline if it's not already running.
        extra_args can be used to pass CLI options to plotter.py if you add them later.
        """
        if self.is_running:
            # Already running; nothing to do.
            return

        script_path = self.project_root / self.script_name
        if not script_path.exists():
            raise FileNotFoundError(f"Plot script not found: {script_path}")

        cmd = [sys.executable, str(script_path)]
        if extra_args:
            cmd.extend(extra_args)

        # Launch in the project root so relative paths (e.g. logs/) still work
        self._proc = subprocess.Popen(cmd, cwd=str(self.project_root))

    def stop(self) -> None:
        """
        Stop the plotting process if it is running.
        """
        if not self.is_running:
            self._proc = None
            return

        try:
            self._proc.terminate()
            self._proc.wait(timeout=5.0)
        except Exception:
            # If terminate failed or timed out, force kill
            try:
                self._proc.kill()
            except Exception:
                pass
        finally:
            self._proc = None

------------------------------ END OF FILE ------------------------------

============================= src\sensepi\tools\plotter.py
# File: plotter.py (ext: .py
# Dir : src\sensepi\tools\
# Size: 17804 bytes
# Time: 30/11/2025 16:51
============================= src\sensepi\tools\plotter.py
#!/usr/bin/env python3
"""Helpers for decimating/smoothing SensePi time-series data for plotting.

The Offline/Recordings GUI tab imports this module directly so it can reuse
the same NumPy-based downsampling routines that power the standalone CLI
plotter. When executed as a script (see ``LocalPlotRunner``), it opens a
Matplotlib window that can either replay a log once (``--mode replay``) or
periodically reload a log for a faux-live view (``--mode follow``). If
``--file`` is omitted the newest ``*.csv``/``*.jsonl`` under ``data/raw/``,
``logs/``, or the project root is selected automatically.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Optional, Sequence

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.animation import FuncAnimation


REPO_ROOT = Path(__file__).resolve().parents[3]
BASE_TIME_FIELDS = {"timestamp_ns", "t_s", "t_rel_s", "timestamp"}
BASE_IGNORE_FIELDS = BASE_TIME_FIELDS | {"sensor_id"}


# --------------------------------------------------------------------------- # helpers
def find_latest_log(search_roots: Sequence[Path]) -> Optional[Path]:
    candidates: list[Path] = []
    patterns = ("*.csv", "*.jsonl")
    for root in search_roots:
        if not root.exists():
            continue
        for pattern in patterns:
            candidates.extend(root.glob(pattern))

    if not candidates:
        return None

    return max(candidates, key=lambda p: p.stat().st_mtime)


def _load_meta_sidecar(path: Path) -> dict[str, Any] | None:
    meta_path = path.with_suffix(path.suffix + ".meta.json")
    if not meta_path.exists():
        return None
    try:
        with meta_path.open("r", encoding="utf-8") as handle:
            meta = json.load(handle)
    except Exception as exc:
        raise ValueError(f"Failed to parse metadata {meta_path}: {exc}") from exc
    if not isinstance(meta, dict):
        raise ValueError(f"Metadata file {meta_path} must contain a JSON object")
    return meta


def _load_csv_with_meta(path: Path) -> tuple[np.ndarray, list[str], dict[str, Any] | None]:
    data, columns = load_csv(path)
    meta = _load_meta_sidecar(path)
    return data, columns, meta


def _classify_value(value: Any) -> str | None:
    if value is None:
        return None
    if isinstance(value, bool):
        return "bool"
    if isinstance(value, (int, np.integer)) and not isinstance(value, bool):
        return "int"
    if isinstance(value, (float, np.floating)):
        return "float"
    if isinstance(value, str):
        return "str"
    return "object"


def _merge_kinds(existing: str | None, new_kind: str | None) -> str | None:
    if new_kind is None:
        return existing
    if existing is None:
        return new_kind
    if existing == new_kind:
        return existing
    numeric = {"int", "float"}
    if existing in numeric and new_kind in numeric:
        return "float"
    return "object"


def _dtype_for_kind(kind: str | None) -> str:
    if kind == "float":
        return "f8"
    if kind == "int":
        return "i8"
    if kind == "bool":
        return "?"
    # Strings and mixed/unknown types fall back to Python objects
    return "O"


def _records_to_structured_array(
    records: Sequence[dict[str, Any]], meta: dict[str, Any] | None
) -> tuple[np.ndarray, list[str]]:
    order: list[str] = []
    if meta:
        header = meta.get("header")
        if isinstance(header, list):
            for entry in header:
                if isinstance(entry, str) and entry not in order:
                    order.append(entry)

    for record in records:
        for key in record.keys():
            if key not in order:
                order.append(key)

    if not order:
        raise ValueError("JSONL log contains no columns")

    kinds: dict[str, str | None] = {col: None for col in order}
    for record in records:
        for key, value in record.items():
            if key not in kinds:
                kinds[key] = None
            kinds[key] = _merge_kinds(kinds.get(key), _classify_value(value))

    dtype = [(col, _dtype_for_kind(kinds.get(col))) for col in order]
    data = np.zeros(len(records), dtype=dtype)

    for col, kind in ((col_name, kinds.get(col_name)) for col_name in order):
        if kind == "float":
            data[col] = np.nan

    for row_idx, record in enumerate(records):
        for key, value in record.items():
            if value is None or key not in data.dtype.names:
                continue
            data[key][row_idx] = value

    columns = list(data.dtype.names or [])
    return data, columns


def _load_jsonl_with_meta(path: Path) -> tuple[np.ndarray, list[str], dict[str, Any] | None]:
    records: list[dict[str, Any]] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_no, raw in enumerate(handle, start=1):
            line = raw.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError as exc:
                raise ValueError(f"Invalid JSON at line {line_no} in {path}: {exc}") from exc
            if not isinstance(obj, dict):
                raise ValueError(f"Line {line_no} in {path} is not a JSON object")
            records.append(obj)

    if not records:
        raise ValueError(f"{path} contains no JSON objects")

    meta = _load_meta_sidecar(path)
    data, columns = _records_to_structured_array(records, meta)

    if not any(field in columns for field in BASE_TIME_FIELDS):
        raise ValueError(f"{path} is missing a timestamp field")
    plottable = [c for c in columns if c.lower() not in BASE_IGNORE_FIELDS]
    if not plottable:
        raise ValueError(f"{path} does not contain any data channels to plot")

    return data, columns, meta


def _load_log_with_meta(path: Path) -> tuple[np.ndarray, list[str], dict[str, Any] | None]:
    suffix = path.suffix.lower()
    if suffix == ".csv":
        return _load_csv_with_meta(path)
    if suffix == ".jsonl":
        return _load_jsonl_with_meta(path)
    raise ValueError(f"Unsupported log file type: {path.suffix}")


def load_csv(path: Path) -> tuple[np.ndarray, list[str]]:
    """Load a CSV file with a header row into a structured NumPy array."""
    data = np.genfromtxt(path, delimiter=",", names=True)

    # When there's only a single row, genfromtxt may return a scalar; coerce.
    if data.size == 0:
        raise ValueError(f"File {path} contains no data rows")
    if data.ndim == 0:
        data = data.reshape(1)

    names = list(data.dtype.names or [])
    if not names:
        raise ValueError(f"File {path} has no header / column names")

    return data, names


def infer_sensor_type(columns: Sequence[str], meta: dict[str, Any] | None = None) -> str:
    """
    Heuristically infer sensor type from the available columns/metadata.

    Returns "mpu6050" if it finds typical MPU6050 columns, otherwise "generic".
    """
    if meta:
        declared = meta.get("sensor_type")
        if isinstance(declared, str) and declared.strip():
            return declared.strip().lower()

    lower = {c.lower() for c in columns}

    if {"sensor_id", "ax", "ay", "az", "gx", "gy", "gz"} <= lower:
        return "mpu6050"

    # Anything else is treated as generic.
    return "generic"


def build_time_axis(
    data: np.ndarray, columns: Sequence[str], meta: dict[str, Any] | None = None
) -> tuple[np.ndarray, str]:
    """Return (t, x_label)."""
    if "t_rel_s" in columns:
        t = data["t_rel_s"]
        return t, "Time [s]"
    if "t_s" in columns:
        t = data["t_s"]
        return t, "Time [s]"
    if "timestamp_ns" in columns:
        ns = data["timestamp_ns"]
        t0 = ns[0]
        t = (ns - t0) * 1e-9
        return t, "Time [s since start]"
    if "timestamp" in columns:
        ts = data["timestamp"]
        t0 = ts[0]
        t = ts - t0
        return t, "Time [relative units]"
    if meta:
        rate = meta.get("device_rate_hz")
        if isinstance(rate, (int, float)) and rate > 0:
            n = data.shape[0]
            t = np.arange(n, dtype=float) / float(rate)
            return t, f"Time [s @ {rate:g} Hz]"
    n = data.shape[0]
    t = np.arange(n, dtype=float)
    return t, "Sample index"


def _meta_sampling_rate(meta: dict[str, Any] | None) -> float | None:
    if not meta:
        return None
    for key in ("device_rate_hz", "requested_rate_hz", "clamped_rate_hz"):
        value = meta.get(key)
        if isinstance(value, (int, float)) and value > 0:
            return float(value)
    return None


def _preferred_data_columns(columns: Sequence[str], meta: dict[str, Any] | None) -> list[str]:
    cols = [c for c in columns if c]
    lookup = {c.lower(): c for c in cols}
    if meta:
        meta_channels = meta.get("channels")
        ordered: list[str] = []
        if isinstance(meta_channels, list):
            for entry in meta_channels:
                if not isinstance(entry, str):
                    continue
                resolved = lookup.get(entry.lower())
                if resolved and resolved not in ordered:
                    ordered.append(resolved)
        if ordered:
            return ordered

        header = meta.get("header")
        if isinstance(header, list):
            ordered = []
            for entry in header:
                if not isinstance(entry, str):
                    continue
                key = entry.lower()
                if key in BASE_IGNORE_FIELDS:
                    continue
                resolved = lookup.get(key)
                if resolved and resolved not in ordered:
                    ordered.append(resolved)
            if ordered:
                return ordered

    filtered = [c for c in cols if c.lower() not in BASE_IGNORE_FIELDS]
    if filtered:
        return filtered
    return list(cols)


def pick_data_columns(
    sensor_type: str, columns: Sequence[str], meta: dict[str, Any] | None = None
) -> tuple[list[str], list[str]]:
    """Return (acc_columns, gyro_columns) based on the header."""
    cols = [c for c in columns if c]
    lower_map = {c.lower(): c for c in cols}
    ordered = _preferred_data_columns(cols, meta)

    if sensor_type == "mpu6050":
        acc_cols: list[str] = []
        gyro_cols: list[str] = []
        for desired in ("ax", "ay", "az"):
            actual = lower_map.get(desired)
            if actual:
                acc_cols.append(actual)
        for desired in ("gx", "gy", "gz"):
            actual = lower_map.get(desired)
            if actual:
                gyro_cols.append(actual)
        if not acc_cols and ordered:
            acc_cols = ordered
        return acc_cols, gyro_cols

    if ordered:
        return ordered, []
    return cols, []


def setup_figure(
    path: Path,
    sensor_type: str,
    data: np.ndarray,
    columns: Sequence[str],
    meta: dict[str, Any] | None = None,
):
    """Create figure/axes/lines and return (fig, axes, lines, time_vector)."""
    t, x_label = build_time_axis(data, columns, meta)
    acc_cols, gyro_cols = pick_data_columns(sensor_type, columns, meta)

    if not acc_cols and not gyro_cols:
        raise ValueError(f"No plottable data columns found in {path}")

    n_axes = 2 if gyro_cols else 1
    fig, axes = plt.subplots(n_axes, 1, sharex=True)
    if not isinstance(axes, (list, tuple, np.ndarray)):
        axes = [axes]

    lines: dict[str, any] = {}

    # Acceleration subplot(s)
    if acc_cols:
        ax0 = axes[0]
        for col in acc_cols:
            y = data[col]
            (line,) = ax0.plot(t, y, label=col)
            lines[col] = line
        if sensor_type == "mpu6050":
            ax0.set_ylabel("Acceleration [m/sÂ²]")
        else:
            ax0.set_ylabel("Value")
        ax0.legend(loc="upper right")
        rate = _meta_sampling_rate(meta)
        rate_str = f" @ {rate:g} Hz" if rate else ""
        ax0.set_title(f"{sensor_type}{rate_str} â€” {path.name}")

    # Gyro subplot
    if gyro_cols:
        ax1 = axes[1]
        for col in gyro_cols:
            y = data[col]
            (line,) = ax1.plot(t, y, label=col)
            lines[col] = line
        ax1.set_ylabel("Angular rate [deg/s]")
        ax1.legend(loc="upper right")

    axes[-1].set_xlabel(x_label)
    fig.tight_layout()

    return fig, axes, lines, t


def build_plot_for_file(path: Path, sensor_type: str = "auto"):
    """Return a Matplotlib Figure configured for the given log file."""

    data, columns, meta = _load_log_with_meta(path)
    if sensor_type == "auto":
        sensor_type = infer_sensor_type(columns, meta)

    fig, axes, lines, _t = setup_figure(path, sensor_type, data, columns, meta)
    return fig, axes, lines


class Plotter:
    """Prepare decimated/smoothed data for visualisation.

    This class is used by the offline viewer to downsample long time-series
    logs while preserving their overall shape and extrema.
    """

    def __init__(self, sensor_type: str = "auto") -> None:
        self.sensor_type = sensor_type

    def build_figure(
        self,
        path: Path,
        sensor_type: Optional[str] = None,
    ):
        """
        Return the ``(fig, axes, lines)`` tuple for ``path``.

        ``sensor_type`` can override the default detected value for a single
        call.
        """
        resolved = sensor_type or self.sensor_type
        return build_plot_for_file(path, sensor_type=resolved)

    def replay(self, path: Path, sensor_type: Optional[str] = None) -> None:
        """Render ``path`` once in Matplotlib."""
        resolved = sensor_type or self.sensor_type
        plot_replay(path, resolved)

    def follow(
        self,
        path: Path,
        interval_s: float = 0.5,
        sensor_type: Optional[str] = None,
    ) -> None:
        """Continuously reload ``path`` and update the plot."""
        resolved = sensor_type or self.sensor_type
        plot_follow(path, resolved, interval_s)


# --------------------------------------------------------------------------- # plotting modes
def plot_replay(path: Path, sensor_type: str) -> None:
    fig, _axes, _lines = build_plot_for_file(path, sensor_type)
    fig.canvas.manager.set_window_title(f"SensePi replay â€” {path.name}")
    plt.show()


def plot_follow(path: Path, sensor_type: str, interval_s: float) -> None:
    data, columns, meta = _load_log_with_meta(path)
    if sensor_type == "auto":
        sensor_type = infer_sensor_type(columns, meta)

    fig, axes, lines, _t = setup_figure(path, sensor_type, data, columns, meta)

    def _update(_frame):
        try:
            new_data, new_columns, new_meta = _load_log_with_meta(path)
        except Exception:
            # If the file temporarily disappears or is being written to, just
            # skip this frame.
            return list(lines.values())

        t_new, _x_label = build_time_axis(new_data, new_columns, new_meta)

        for name, line in lines.items():
            if name not in new_columns:
                continue
            y = new_data[name]
            line.set_data(t_new, y)

        for ax in axes:
            ax.relim()
            ax.autoscale_view()

        return list(lines.values())

    fig.canvas.manager.set_window_title(f"SensePi live â€” {path.name}")
    FuncAnimation(fig, _update, interval=interval_s * 1000.0, blit=False)
    plt.show()


# --------------------------------------------------------------------------- # CLI
def main(argv: Sequence[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Simple Matplotlib-based plotter for SensePi CSV/JSONL logs."
    )
    parser.add_argument(
        "-f",
        "--file",
        type=str,
        help=(
            "Path to a log file (.csv or .jsonl). If omitted, the newest log "
            "found under data/raw, logs, or the project root is used."
        ),
    )
    parser.add_argument(
        "-s",
        "--sensor",
        type=str,
        choices=["auto", "mpu6050", "generic"],
        default="auto",
        help="Sensor type for plotting (default: auto-detect from columns).",
    )
    parser.add_argument(
        "-m",
        "--mode",
        type=str,
        choices=["follow", "replay"],
        default="follow",
        help=(
            "Plot mode: 'replay' for a static plot, 'follow' to periodically "
            "reload the file for a live view (default)."
        ),
    )
    parser.add_argument(
        "-i",
        "--interval",
        type=float,
        default=0.5,
        help="Update interval in seconds when using --mode follow (default: 0.5).",
    )

    args = parser.parse_args(argv)

    if args.file:
        csv_path = Path(args.file).expanduser().resolve()
        if not csv_path.exists():
            parser.error(f"Log file not found: {csv_path}")
    else:
        csv_path = find_latest_log(
            [
                REPO_ROOT / "data" / "raw",
                REPO_ROOT / "logs",
                REPO_ROOT,
            ]
        )
        if csv_path is None:
            parser.error(
                "No log files found in data/raw, logs, or the project root.\n"
                "Specify a file explicitly with --file."
            )
        print(f"[INFO] Using latest log: {csv_path}")

    try:
        if args.mode == "replay":
            plot_replay(csv_path, args.sensor)
        else:
            plot_follow(csv_path, args.sensor, args.interval)
    except KeyboardInterrupt:
        # Allow clean exit on Ctrl+C
        return 0

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

------------------------------ END OF FILE ------------------------------

============================= ssh_client.py
# File: ssh_client.py (ext: .py
# Dir : 
# Size: 309 bytes
# Time: 30/11/2025 16:51
============================= ssh_client.py
from __future__ import annotations

"""
Legacy shim module.

This file exists for backward compatibility only.
New code should import from :mod:`sensepi.remote.ssh_client` instead.
"""

from sensepi.remote.ssh_client import SSHClient, SSHConfig, Host

__all__ = ["SSHClient", "SSHConfig", "Host"]

------------------------------ END OF FILE ------------------------------

============================= tests\conftest.py
# File: conftest.py (ext: .py
# Dir : tests\
# Size: 168 bytes
# Time: 30/11/2025 16:51
============================= tests\conftest.py
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
src = ROOT / "src"
if str(src) not in sys.path:
    sys.path.insert(0, str(src))

------------------------------ END OF FILE ------------------------------

============================= tests\test_rate_controller.py
# File: test_rate_controller.py (ext: .py
# Dir : tests\
# Size: 325 bytes
# Time: 30/11/2025 16:51
============================= tests\test_rate_controller.py
from sensepi.analysis.rate import RateController


def test_rate_controller_estimates_rate_for_regular_samples() -> None:
    rc = RateController(window_size=100)
    t = 0.0
    for _ in range(100):
        rc.add_sample_time(t)
        t += 0.01  # 100 Hz
    est = rc.estimated_hz
    assert 90.0 < est < 110.0

------------------------------ END OF FILE ------------------------------

============================= tests\test_stream_reader.py
# File: test_stream_reader.py (ext: .py
# Dir : tests\
# Size: 1989 bytes
# Time: 30/11/2025 16:51
============================= tests\test_stream_reader.py
from __future__ import annotations

import io
import json
import time

from sensepi.core.stream_reader import (
    ChannelBufferStore,
    DEFAULT_RINGBUFFER_CAPACITY,
    reader_loop,
    start_reader,
)


def _build_line(sensor_id: int, timestamp: float, **channels: float) -> str:
    payload = {"sensor_id": sensor_id, "t_s": timestamp}
    payload.update(channels)
    return json.dumps(payload)


def test_reader_loop_populates_channel_buffers() -> None:
    store = ChannelBufferStore(capacity=4)
    lines = [
        _build_line(1, 0.001, ax=0.1, ay=0.2),
        _build_line(1, 0.002, ax=0.3, ay=0.4),
    ]
    reader_loop(lines, store)

    ax_buffer = store.get("1", "ax")
    assert ax_buffer is not None
    assert ax_buffer.snapshot() == [(0.001, 0.1), (0.002, 0.3)]

    ay_buffer = store.get("1", "ay")
    assert ay_buffer is not None
    assert ay_buffer.snapshot() == [(0.001, 0.2), (0.002, 0.4)]


def test_reader_loop_ignores_invalid_records() -> None:
    store = ChannelBufferStore(capacity=2)
    lines = [
        "not-json",
        json.dumps({"sensor_id": 1, "ax": 1.0}),  # missing timestamp
        json.dumps({"t_s": 0.1, "ax": 1.0}),  # missing sensor_id
        json.dumps({"sensor_id": 1, "t_s": 0.2, "ax": "not-a-number"}),
        _build_line(2, 0.3, ax=2.5),
    ]
    reader_loop(lines, store)

    assert store.get("2", "ax").snapshot() == [(0.3, 2.5)]


def test_start_reader_background_thread() -> None:
    buffer = io.StringIO("\n".join([_build_line(3, 0.1, ax=1.5)]) + "\n")
    handle = start_reader(buffer, capacity=DEFAULT_RINGBUFFER_CAPACITY)

    # Allow background thread to process the single line
    timeout = time.time() + 1.0
    while time.time() < timeout:
        buf = handle.buffers.get("3", "ax")
        if buf and len(buf) == 1:
            break
        time.sleep(0.01)

    handle.stop(join=True, timeout=1.0)

    buf = handle.buffers.get("3", "ax")
    assert buf is not None
    assert buf.snapshot() == [(0.1, 1.5)]

------------------------------ END OF FILE ------------------------------

